N=10
for ((i=1; i<=N; i++))
do
horizon=96
maxconcurrent=4
gpu_fraction=$(echo "scale=2; 1/$maxconcurrent" | bc)  # Calculate GPU fraction with 2 decimal places
start_time=$(date +%s)  # Get the current time in seconds
python3 tune_softs.py \
    --model SOFTS \
    --data ETTh2 \
    --root_path ./dataset/ETT-small/ \
    --data_path ETTh2.csv \
    --features M \
    --seq_len 96 \
    --label_len 48 \
    --pred_len $horizon \
    --enc_in 7 \
    --dec_in 7 \
    --c_out 7 \
    --embed timeF \
    --activation gelu \
    --train_epochs 8 \
    --patience 3 \
    --loss MSE \
    --num_workers 1 \
    --gpu 0 \
    --tune_search_algorithm random_search \
    --tune_trial_scheduler fifo \
    --tune_storage_path ./checkpoints/hptunning/random_search/ \
    --tune_experiment_name ETTh2_96_${horizon}_test_seed${i} \
    --tune_objective best_valid_loss \
    --tune_num_samples 10 \
    --tune_max_trial_time_s 70 \
    --tune_time_budget_s 14400 \
    --tune_max_concurrent $maxconcurrent \
    --tune_gpu_resources $gpu_fraction \
    --tune_cpu_resources 1 \
    --tune_default_config "{
        \"batch_size\": 32, \
        \"learning_rate\": 0.0003, \
        \"d_model\": 128, \
        \"alpha_d_ff\": 1, \
        \"d_core\": 64, \
        \"e_layers\": 2, \
        \"dropout\": 0.0, \
        \"lradj\": \"cosine\"
    }" \
    --tune_param_space "{
        \"batch_size\": [\"choice\", [8, 16, 32, 64, 128]], \
        \"learning_rate\": [\"loguniform\", [0.00005, 0.005]], \
        \"d_model\": [\"choice\", [32, 64, 128, 256, 512]], \
        \"alpha_d_ff\": [\"choice\", [1, 2, 3, 4]], \
        \"d_core\": [\"choice\", [32, 64, 128, 256, 512]], \
        \"e_layers\": [\"choice\", [1, 2, 3, 4]], \
        \"dropout\": [\"loguniform\", [0.0008, 0.012]], \
        \"lradj\": [\"choice\", [\"cosine\", \"type1\"]]
    }" \
    --seed $i;
end_time=$(date +%s)  # Get the current time in seconds
elapsed_time=$((end_time - start_time))  # Calculate the elapsed time
echo ""
echo ""
echo "Time taken ($maxconcurrent parallel trials): $elapsed_time seconds"
echo ""
echo ""
done2024-08-26 13:50:42,312	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:50:42,665	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:50:42,670	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:50:42,678	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:50:45,880	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2862632)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1/trial-6ba48_00003_3_alpha_d_ff=4,batch_size=128,d_core=64,d_model=256,dropout=0.0018,e_layers=2,learning_rate=0.0006,lradj=type1_2024-08-26_13-50-42/checkpoint_000000)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed1   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator    â”‚
â”‚ Scheduler                        FIFOScheduler            â”‚
â”‚ Number of trials                 10                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-50-41_880654_2860382/artifacts/2024-08-26_13-50-42/ETTh2_96_96_test_seed1/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 13:50:42. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-6ba48_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-6ba48_00001   PENDING              32       0.00398013          128              1         32            4   0.00186137   type1   â”‚
â”‚ trial-6ba48_00002   PENDING             128       0.00226139          128              2        128            3   0.00086199   type1   â”‚
â”‚ trial-6ba48_00003   PENDING             128       0.000596014         256              4         64            2   0.00181834   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-6ba48_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00086 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00226 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-6ba48_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-6ba48_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00182 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                       0.0006 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-6ba48_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00186 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00398 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2862631)[0m configuration
[36m(_train_fn pid=2862631)[0m {'batch_size': 128, 'learning_rate': 0.002261388566060138, 'd_model': 128, 'd_core': 128, 'e_layers': 3, 'dropout': 0.0008619895863825495, 'lradj': 'type1', 'd_ff': 256}
[36m(_train_fn pid=2862631)[0m Use GPU: cuda:0
[36m(_train_fn pid=2862629)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2862631)[0m train 8449
[36m(_train_fn pid=2862631)[0m val 2785
[36m(_train_fn pid=2862631)[0m start_epoch 0
[36m(_train_fn pid=2862631)[0m max_epoch 8
[36m(_train_fn pid=2862632)[0m Updating learning rate to 0.0005960142327626242
[36m(_train_fn pid=2862632)[0m saving checkpoint...
[36m(_train_fn pid=2862632)[0m Validation loss decreased (inf --> 0.2305).  Saving model state dict ...
[36m(_train_fn pid=2862632)[0m Epoch: 1 cost time: 0.9779133796691895
[36m(_train_fn pid=2862632)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2305476 Best vali loss: 0.2305476
[36m(_train_fn pid=2862629)[0m 	iters: 100, epoch: 1 | loss: 0.3755640
2024-08-26 13:50:46,039	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:46,728	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:46,974	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:47,555	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:47,785	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:47,895	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:48,342	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:48,817	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:49,146	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:49,218	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:49,520	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:49,929	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:50,141	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:50,781	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:51,331	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:51,366	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2862629)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1/trial-6ba48_00000_0_2024-08-26_13-50-42/checkpoint_000002)[32m [repeated 15x across cluster][0m
2024-08-26 13:50:52,643	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:53,110	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2862629)[0m 	speed: 0.0147s/iter; left time: 29.6969s
[36m(_train_fn pid=2862632)[0m Validation loss decreased (0.2305 --> 0.2202).  Saving model state dict ...
[36m(_train_fn pid=2862632)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2862629)[0m Epoch: 1, Steps: 265 | Train Loss: 0.5140803 Vali Loss: 0.2234485 Best vali loss: 0.2234485
[36m(_train_fn pid=2862632)[0m EarlyStopping counter: 2 out of 3

Trial trial-6ba48_00003 completed after 5 iterations at 2024-08-26 13:50:49. Total running time: 6s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             0.80091 â”‚
â”‚ time_total_s                                 4.87035 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22019 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.22161 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2862632)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2862632)[0m Early stopping
[36m(_train_fn pid=2862629)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2862632)[0m {'batch_size': 128, 'learning_rate': 0.0005960142327626242, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0018183386728865612, 'lradj': 'type1', 'd_ff': 1024}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2862630)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2862629)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2862629)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2862629)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2862629)[0m max_epoch 8[32m [repeated 3x across cluster][0m

Trial trial-6ba48_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00163 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00009 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2863924)[0m {'batch_size': 8, 'learning_rate': 9.269437890715644e-05, 'd_model': 128, 'd_core': 128, 'e_layers': 4, 'dropout': 0.0016277751487466937, 'lradj': 'cosine', 'd_ff': 256}

Trial trial-6ba48_00002 completed after 7 iterations at 2024-08-26 13:50:50. Total running time: 8s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             0.63761 â”‚
â”‚ time_total_s                                  6.5165 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.22644 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.23852 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2862631)[0m Updating learning rate to 3.533419634468966e-05[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2862631)[0m saving checkpoint...[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2862630)[0m Validation loss decreased (inf --> 0.2737).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2862631)[0m Epoch: 7 cost time: 0.5095269680023193[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2862631)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2385230 Best vali loss: 0.2264435[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2862629)[0m 	iters: 200, epoch: 3 | loss: 0.4513496[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2862629)[0m 	speed: 0.0045s/iter; left time: 6.2932s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2862630)[0m Validation loss decreased (0.2737 --> 0.2656).  Saving model state dict ...[32m [repeated 4x across cluster][0m

Trial trial-6ba48_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.01082 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00018 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-6ba48_00000 completed after 4 iterations at 2024-08-26 13:50:52. Total running time: 9s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.27447 â”‚
â”‚ time_total_s                                 8.37421 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22345 â”‚
â”‚ train_loss                                   0.41885 â”‚
â”‚ valid_loss                                   0.22641 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2862629)[0m EarlyStopping counter: 3 out of 3[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2862630)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3844815 Vali Loss: 0.2716449 Best vali loss: 0.2655903[32m [repeated 6x across cluster][0m

Trial trial-6ba48_00006 started with configuration:
2024-08-26 13:50:54,993	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:56,840	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2863924)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1/trial-6ba48_00004_4_alpha_d_ff=2,batch_size=8,d_core=128,d_model=128,dropout=0.0016,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_13-50-49/checkpoint_000000)[32m [repeated 5x across cluster][0m
2024-08-26 13:50:57,295	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:50:57,721	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:01,097	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:02,384	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:02,408	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2863924)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1/trial-6ba48_00004_4_alpha_d_ff=2,batch_size=8,d_core=128,d_model=128,dropout=0.0016,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_13-50-49/checkpoint_000001)[32m [repeated 4x across cluster][0m
2024-08-26 13:51:02,453	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:02,861	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:04,070	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:05,770	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:07,511	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2865051)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1/trial-6ba48_00007_7_alpha_d_ff=2,batch_size=32,d_core=256,d_model=128,dropout=0.0042,e_layers=2,learning_rate=0.0001,lradj=type1_2024-08-26_13-50-57/checkpoint_000004)[32m [repeated 6x across cluster][0m
2024-08-26 13:51:07,942	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:08,892	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:09,045	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:10,514	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:10,530	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:12,144	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.01106 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                       0.0006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2862629)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2864592)[0m configuration[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2864297)[0m {'batch_size': 8, 'learning_rate': 0.0001818810783039845, 'd_model': 256, 'd_core': 128, 'e_layers': 1, 'dropout': 0.01081649631520423, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2864592)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2864592)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2864592)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2864592)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2864592)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2864592)[0m {'batch_size': 8, 'learning_rate': 0.0006045381538566557, 'd_model': 256, 'd_core': 64, 'e_layers': 3, 'dropout': 0.011061417572575135, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2862630)[0m Updating learning rate to 0.000497516409976749[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2862630)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2862630)[0m Epoch: 4 cost time: 1.6041302680969238[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2864592)[0m 	iters: 200, epoch: 1 | loss: 0.2966067[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=2864592)[0m 	speed: 0.0074s/iter; left time: 60.8541s[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=2863924)[0m Validation loss decreased (inf --> 0.2189).  Saving model state dict ...

Trial trial-6ba48_00001 completed after 5 iterations at 2024-08-26 13:50:57. Total running time: 14s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                              2.3004 â”‚
â”‚ time_total_s                                13.00299 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.26559 â”‚
â”‚ train_loss                                   0.46802 â”‚
â”‚ valid_loss                                   0.27155 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2862630)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2864297)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.5534754 Vali Loss: 0.2219104 Best vali loss: 0.2219104[32m [repeated 4x across cluster][0m

Trial trial-6ba48_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00421 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00009 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2865051)[0m {'batch_size': 32, 'learning_rate': 8.525134298760106e-05, 'd_model': 128, 'd_core': 256, 'e_layers': 2, 'dropout': 0.004207632598489661, 'lradj': 'type1', 'd_ff': 256}
[36m(_train_fn pid=2862630)[0m Early stopping
[36m(_train_fn pid=2865051)[0m configuration
[36m(_train_fn pid=2865051)[0m Use GPU: cuda:0
[36m(_train_fn pid=2865051)[0m train 8449
[36m(_train_fn pid=2865051)[0m val 2785
[36m(_train_fn pid=2865051)[0m start_epoch 0
[36m(_train_fn pid=2865051)[0m max_epoch 8
[36m(_train_fn pid=2865051)[0m Updating learning rate to 8.525134298760106e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2865051)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2865051)[0m Epoch: 1 cost time: 1.5724663734436035[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2863924)[0m 	iters: 1000, epoch: 2 | loss: 0.1828706[32m [repeated 31x across cluster][0m
[36m(_train_fn pid=2863924)[0m 	speed: 0.0049s/iter; left time: 31.1402s[32m [repeated 31x across cluster][0m
[36m(_train_fn pid=2865051)[0m Validation loss decreased (inf --> 0.2271).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2863924)[0m Validation loss decreased (0.2189 --> 0.2168).  Saving model state dict ...
[36m(_train_fn pid=2864297)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.2574789 Vali Loss: 0.2181547 Best vali loss: 0.2181547[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2865051)[0m Updating learning rate to 1.0656417873450133e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2865051)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2865051)[0m Epoch: 4 cost time: 1.3968727588653564[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2863924)[0m 	iters: 900, epoch: 3 | loss: 0.1980269[32m [repeated 30x across cluster][0m
[36m(_train_fn pid=2863924)[0m 	speed: 0.0047s/iter; left time: 25.4337s[32m [repeated 30x across cluster][0m
[36m(_train_fn pid=2864592)[0m Validation loss decreased (inf --> 0.2612).  Saving model state dict ...
[36m(_train_fn pid=2865051)[0m Validation loss decreased (0.2173 --> 0.2172).  Saving model state dict ...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2863924)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2863924)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4567909 Vali Loss: 0.2206970 Best vali loss: 0.2168004[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2865051)[0m Updating learning rate to 1.3320522341812666e-06[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2865051)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2865051)[0m Epoch: 7 cost time: 1.2228410243988037[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2864297)[0m 	iters: 700, epoch: 4 | loss: 0.2439205[32m [repeated 31x across cluster][0m
[36m(_train_fn pid=2864297)[0m 	speed: 0.0047s/iter; left time: 21.6322s[32m [repeated 31x across cluster][0m

Trial trial-6ba48_00007 completed after 8 iterations at 2024-08-26 13:51:12. Total running time: 29s
2024-08-26 13:51:13,334	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2863924)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1/trial-6ba48_00004_4_alpha_d_ff=2,batch_size=8,d_core=128,d_model=128,dropout=0.0016,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_13-50-49/checkpoint_000003)[32m [repeated 7x across cluster][0m
2024-08-26 13:51:13,786	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:16,607	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.62669 â”‚
â”‚ time_total_s                                13.27005 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21714 â”‚
â”‚ train_loss                                   0.41349 â”‚
â”‚ valid_loss                                   0.21714 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2865051)[0m Validation loss decreased (0.2172 --> 0.2171).  Saving model state dict ...[32m [repeated 3x across cluster][0m

Trial status: 5 TERMINATED | 3 RUNNING | 1 PENDING
Current time: 2024-08-26 13:51:12. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 6ba48_00004 with best_valid_loss=0.21680037830847598 and params={'batch_size': 8, 'learning_rate': 9.269437890715644e-05, 'd_model': 128, 'd_core': 128, 'e_layers': 4, 'dropout': 0.0016277751487466937, 'lradj': 'cosine', 'd_ff': 256}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-6ba48_00004   RUNNING                 8       9.26944e-05         128              2        128            4   0.00162778   cosine         3           17.2761        0.456791       0.220697            0.2168   â”‚
â”‚ trial-6ba48_00005   RUNNING                 8       0.000181881         256              2        128            1   0.0108165    type1          3           16.545         0.35738        0.225045            0.218155 â”‚
â”‚ trial-6ba48_00006   RUNNING                 8       0.000604538         256              4         64            3   0.0110614    cosine         2           16.3571        0.548837       0.239578            0.239578 â”‚
â”‚ trial-6ba48_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            8.37421       0.418853       0.226411            0.223448 â”‚
â”‚ trial-6ba48_00001   TERMINATED             32       0.00398013          128              1         32            4   0.00186137   type1          5           13.003         0.468024       0.271554            0.26559  â”‚
â”‚ trial-6ba48_00002   TERMINATED            128       0.00226139          128              2        128            3   0.00086199   type1          7            6.5165      nan              0.238523            0.226443 â”‚
â”‚ trial-6ba48_00003   TERMINATED            128       0.000596014         256              4         64            2   0.00181834   type1          5            4.87035     nan              0.221611            0.220187 â”‚
â”‚ trial-6ba48_00007   TERMINATED             32       8.52513e-05         128              2        256            2   0.00420763   type1          8           13.2701        0.413494       0.217142            0.217142 â”‚
â”‚ trial-6ba48_00008   PENDING                64       6.00006e-05         512              3        128            4   0.00094715   cosine                                                                                â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2863924)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m

Trial trial-6ba48_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00095 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2866240)[0m configuration
[36m(_train_fn pid=2866240)[0m {'batch_size': 64, 'learning_rate': 6.0000622707083504e-05, 'd_model': 512, 'd_core': 128, 'e_layers': 4, 'dropout': 0.0009471500402056636, 'lradj': 'cosine', 'd_ff': 1536}
[36m(_train_fn pid=2866240)[0m Use GPU: cuda:0
[36m(_train_fn pid=2863924)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3607210 Vali Loss: 0.2195928 Best vali loss: 0.2168004[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2866240)[0m train 8449
[36m(_train_fn pid=2866240)[0m val 2785
[36m(_train_fn pid=2866240)[0m start_epoch 0
[36m(_train_fn pid=2866240)[0m max_epoch 8
[36m(_train_fn pid=2866240)[0m Validation loss decreased (inf --> 0.2160).  Saving model state dict ...
[36m(_train_fn pid=2866240)[0m Updating learning rate to 5.771698498204495e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2866240)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2866240)[0m Epoch: 1 cost time: 2.1235475540161133[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2863924)[0m 	iters: 800, epoch: 5 | loss: 0.6877096[32m [repeated 26x across cluster][0m
[36m(_train_fn pid=2863924)[0m 	speed: 0.0046s/iter; left time: 15.7424s[32m [repeated 26x across cluster][0m
2024-08-26 13:51:18,734	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:18,771	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2863924)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1/trial-6ba48_00004_4_alpha_d_ff=2,batch_size=8,d_core=128,d_model=128,dropout=0.0016,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_13-50-49/checkpoint_000004)[32m [repeated 3x across cluster][0m
2024-08-26 13:51:20,786	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:21,118	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:23,202	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:23,529	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:24,350	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2866661)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1/trial-6ba48_00009_9_alpha_d_ff=1,batch_size=32,d_core=128,d_model=256,dropout=0.0078,e_layers=4,learning_rate=0.0025,lradj=cosine_2024-08-26_13-51-18/checkpoint_000000)[32m [repeated 6x across cluster][0m
2024-08-26 13:51:26,280	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:28,269	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:29,691	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2864592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1/trial-6ba48_00006_6_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0111,e_layers=3,learning_rate=0.0006,lradj=cosine_2024-08-26_13-50-52/checkpoint_000003)[32m [repeated 3x across cluster][0m
2024-08-26 13:51:30,239	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:32,262	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:34,292	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:36,259	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2864297)[0m EarlyStopping counter: 2 out of 3

Trial trial-6ba48_00004 completed after 5 iterations at 2024-08-26 13:51:18. Total running time: 36s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.39803 â”‚
â”‚ time_total_s                                 28.0623 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                               0.2168 â”‚
â”‚ train_loss                                   0.33492 â”‚
â”‚ valid_loss                                    0.2262 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2863924)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2863924)[0m Early stopping
[36m(_train_fn pid=2863924)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.3349178 Vali Loss: 0.2262032 Best vali loss: 0.2168004[32m [repeated 3x across cluster][0m

Trial trial-6ba48_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00778 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00254 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2866661)[0m configuration
[36m(_train_fn pid=2866661)[0m {'batch_size': 32, 'learning_rate': 0.0025365047579379886, 'd_model': 256, 'd_core': 128, 'e_layers': 4, 'dropout': 0.007777988604331208, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2866661)[0m Use GPU: cuda:0
[36m(_train_fn pid=2866661)[0m train 8449
[36m(_train_fn pid=2866661)[0m val 2785
[36m(_train_fn pid=2866661)[0m start_epoch 0
[36m(_train_fn pid=2866661)[0m max_epoch 8
[36m(_train_fn pid=2864592)[0m Updating learning rate to 0.0004179424447850877[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2864592)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2864592)[0m Epoch: 3 cost time: 8.280810594558716[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2866240)[0m 	iters: 100, epoch: 4 | loss: 0.4179069[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2866240)[0m 	speed: 0.0225s/iter; left time: 12.7412s[32m [repeated 12x across cluster][0m

Trial trial-6ba48_00008 completed after 4 iterations at 2024-08-26 13:51:23. Total running time: 40s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.41407 â”‚
â”‚ time_total_s                                 9.50578 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21603 â”‚
â”‚ train_loss                                   0.41791 â”‚
â”‚ valid_loss                                   0.22101 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-6ba48_00005 completed after 5 iterations at 2024-08-26 13:51:23. Total running time: 40s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             9.74023 â”‚
â”‚ time_total_s                                31.17702 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21815 â”‚
â”‚ train_loss                                   0.38119 â”‚
â”‚ valid_loss                                   0.22234 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2864297)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2864297)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2864297)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.3811942 Vali Loss: 0.2223420 Best vali loss: 0.2181547[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2866661)[0m Validation loss decreased (inf --> 0.2953).  Saving model state dict ...
[36m(_train_fn pid=2866661)[0m Validation loss decreased (0.2953 --> 0.2716).  Saving model state dict ...
[36m(_train_fn pid=2866661)[0m Updating learning rate to 0.002165042236393941[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2866661)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2866661)[0m Epoch: 2 cost time: 1.6581642627716064[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2866661)[0m 	iters: 200, epoch: 3 | loss: 0.3752965[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2866661)[0m 	speed: 0.0063s/iter; left time: 8.8116s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2866661)[0m Validation loss decreased (0.2716 --> 0.2706).  Saving model state dict ...
[36m(_train_fn pid=2866661)[0m Epoch: 3, Steps: 265 | Train Loss: 0.5494466 Vali Loss: 0.2705769 Best vali loss: 0.2705769[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2866661)[0m Validation loss decreased (0.2706 --> 0.2677).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2866661)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2866661)[0m Updating learning rate to 0.000782913205479949[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2866661)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2866661)[0m Epoch: 5 cost time: 1.7606046199798584[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2866661)[0m 	iters: 100, epoch: 6 | loss: 0.6723373[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2866661)[0m 	speed: 0.0137s/iter; left time: 9.5522s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2866661)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2866661)[0m Epoch: 6, Steps: 265 | Train Loss: 0.4599819 Vali Loss: 0.2715560 Best vali loss: 0.2676893[32m [repeated 4x across cluster][0m

Trial trial-6ba48_00009 completed after 7 iterations at 2024-08-26 13:51:36. Total running time: 53s
[36m(_train_fn pid=2866661)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1/trial-6ba48_00009_9_alpha_d_ff=1,batch_size=32,d_core=128,d_model=256,dropout=0.0078,e_layers=4,learning_rate=0.0025,lradj=cosine_2024-08-26_13-51-18/checkpoint_000006)[32m [repeated 4x across cluster][0m
2024-08-26 13:51:36,493	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:41,208	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:46,034	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2864592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1/trial-6ba48_00006_6_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0111,e_layers=3,learning_rate=0.0006,lradj=cosine_2024-08-26_13-50-52/checkpoint_000006)[32m [repeated 3x across cluster][0m
2024-08-26 13:51:50,593	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             1.96435 â”‚
â”‚ time_total_s                                15.97289 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26769 â”‚
â”‚ train_loss                                   0.74663 â”‚
â”‚ valid_loss                                   0.27012 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2866661)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2866661)[0m Early stopping
[36m(_train_fn pid=2864592)[0m Validation loss decreased (0.2331 --> 0.2310).  Saving model state dict ...
[36m(_train_fn pid=2864592)[0m Updating learning rate to 0.000186595709071568[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2864592)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2864592)[0m Epoch: 5 cost time: 6.0227601528167725[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2864592)[0m 	iters: 400, epoch: 6 | loss: 0.4412242[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2864592)[0m 	speed: 0.0039s/iter; left time: 10.7163s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2864592)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.4075883 Vali Loss: 0.2310034 Best vali loss: 0.2310034[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2864592)[0m Validation loss decreased (0.2310 --> 0.2286).  Saving model state dict ...

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2024-08-26 13:51:42. Total running time: 1min 0s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 6ba48_00008 with best_valid_loss=0.21602602403082583 and params={'batch_size': 64, 'learning_rate': 6.0000622707083504e-05, 'd_model': 512, 'd_core': 128, 'e_layers': 4, 'dropout': 0.0009471500402056636, 'lradj': 'cosine', 'd_ff': 1536}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-6ba48_00006   RUNNING                 8       0.000604538         256              4         64            3   0.0110614    cosine         6           47.0217        0.27525        0.228558            0.228558 â”‚
â”‚ trial-6ba48_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            8.37421       0.418853       0.226411            0.223448 â”‚
â”‚ trial-6ba48_00001   TERMINATED             32       0.00398013          128              1         32            4   0.00186137   type1          5           13.003         0.468024       0.271554            0.26559  â”‚
â”‚ trial-6ba48_00002   TERMINATED            128       0.00226139          128              2        128            3   0.00086199   type1          7            6.5165      nan              0.238523            0.226443 â”‚
â”‚ trial-6ba48_00003   TERMINATED            128       0.000596014         256              4         64            2   0.00181834   type1          5            4.87035     nan              0.221611            0.220187 â”‚
â”‚ trial-6ba48_00004   TERMINATED              8       9.26944e-05         128              2        128            4   0.00162778   cosine         5           28.0623        0.334918       0.226203            0.2168   â”‚
â”‚ trial-6ba48_00005   TERMINATED              8       0.000181881         256              2        128            1   0.0108165    type1          5           31.177         0.381194       0.222342            0.218155 â”‚
â”‚ trial-6ba48_00007   TERMINATED             32       8.52513e-05         128              2        256            2   0.00420763   type1          8           13.2701        0.413494       0.217142            0.217142 â”‚
â”‚ trial-6ba48_00008   TERMINATED             64       6.00006e-05         512              3        128            4   0.00094715   cosine         4            9.50578       0.417907       0.221009            0.216026 â”‚
â”‚ trial-6ba48_00009   TERMINATED             32       0.0025365           256              1        128            4   0.00777799   cosine         7           15.9729        0.746631       0.270118            0.267689 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2864592)[0m Updating learning rate to 8.853256288930904e-05
[36m(_train_fn pid=2864592)[0m saving checkpoint...
[36m(_train_fn pid=2864592)[0m Epoch: 6 cost time: 4.125858306884766
[36m(_train_fn pid=2864592)[0m 	iters: 500, epoch: 7 | loss: 0.6103756[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2864592)[0m 	speed: 0.0039s/iter; left time: 6.2344s[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2864592)[0m Epoch: 6, Steps: 1057 | Train Loss: 0.2752496 Vali Loss: 0.2285576 Best vali loss: 0.2285576
[36m(_train_fn pid=2864592)[0m Updating learning rate to 2.3008863443166145e-05
[36m(_train_fn pid=2864592)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2864592)[0m saving checkpoint...
[36m(_train_fn pid=2864592)[0m Epoch: 7 cost time: 4.272890567779541
[36m(_train_fn pid=2864592)[0m Epoch: 7, Steps: 1057 | Train Loss: 0.4910234 Vali Loss: 0.2325175 Best vali loss: 0.2285576
[36m(_train_fn pid=2864592)[0m 	iters: 600, epoch: 8 | loss: 0.8208829[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2864592)[0m 	speed: 0.0037s/iter; left time: 1.7152s[32m [repeated 11x across cluster][0m

2024-08-26 13:51:50,602	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1' in 0.0084s.
Trial trial-6ba48_00006 completed after 8 iterations at 2024-08-26 13:51:50. Total running time: 1min 7s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6ba48_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             4.55554 â”‚
â”‚ time_total_s                                56.39881 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22856 â”‚
â”‚ train_loss                                   0.37864 â”‚
â”‚ valid_loss                                     0.231 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:51:50. Total running time: 1min 7s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 6ba48_00008 with best_valid_loss=0.21602602403082583 and params={'batch_size': 64, 'learning_rate': 6.0000622707083504e-05, 'd_model': 512, 'd_core': 128, 'e_layers': 4, 'dropout': 0.0009471500402056636, 'lradj': 'cosine', 'd_ff': 1536}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-6ba48_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            8.37421       0.418853       0.226411            0.223448 â”‚
â”‚ trial-6ba48_00001   TERMINATED             32       0.00398013          128              1         32            4   0.00186137   type1          5           13.003         0.468024       0.271554            0.26559  â”‚
â”‚ trial-6ba48_00002   TERMINATED            128       0.00226139          128              2        128            3   0.00086199   type1          7            6.5165      nan              0.238523            0.226443 â”‚
â”‚ trial-6ba48_00003   TERMINATED            128       0.000596014         256              4         64            2   0.00181834   type1          5            4.87035     nan              0.221611            0.220187 â”‚
â”‚ trial-6ba48_00004   TERMINATED              8       9.26944e-05         128              2        128            4   0.00162778   cosine         5           28.0623        0.334918       0.226203            0.2168   â”‚
â”‚ trial-6ba48_00005   TERMINATED              8       0.000181881         256              2        128            1   0.0108165    type1          5           31.177         0.381194       0.222342            0.218155 â”‚
â”‚ trial-6ba48_00006   TERMINATED              8       0.000604538         256              4         64            3   0.0110614    cosine         8           56.3988        0.378639       0.231004            0.228558 â”‚
â”‚ trial-6ba48_00007   TERMINATED             32       8.52513e-05         128              2        256            2   0.00420763   type1          8           13.2701        0.413494       0.217142            0.217142 â”‚
â”‚ trial-6ba48_00008   TERMINATED             64       6.00006e-05         512              3        128            4   0.00094715   cosine         4            9.50578       0.417907       0.221009            0.216026 â”‚
â”‚ trial-6ba48_00009   TERMINATED             32       0.0025365           256              1        128            4   0.00777799   cosine         7           15.9729        0.746631       0.270118            0.267689 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 64, 'learning_rate': 6.0000622707083504e-05, 'd_model': 512, 'd_core': 128, 'e_layers': 4, 'dropout': 0.0009471500402056636, 'lradj': 'cosine', 'd_ff': 1536}
[36m(_train_fn pid=2864592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1/trial-6ba48_00006_6_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0111,e_layers=3,learning_rate=0.0006,lradj=cosine_2024-08-26_13-50-52/checkpoint_000007)
[36m(_train_fn pid=2864592)[0m Updating learning rate to 0.0
[36m(_train_fn pid=2864592)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2864592)[0m saving checkpoint...
[36m(_train_fn pid=2864592)[0m Epoch: 8 cost time: 4.0063745975494385
[36m(_train_fn pid=2864592)[0m Epoch: 8, Steps: 1057 | Train Loss: 0.3786395 Vali Loss: 0.2310037 Best vali loss: 0.2285576
[36m(_train_fn pid=2864592)[0m 	iters: 1000, epoch: 8 | loss: 0.2151358[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2864592)[0m 	speed: 0.0035s/iter; left time: 0.2038s[32m [repeated 4x across cluster][0m


Time taken (4 parallel trials): 72 seconds


2024-08-26 13:51:54,690	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:51:55,061	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:51:55,066	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:51:55,073	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:51:58,216	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2870179)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed2/trial-96c86_00001_1_alpha_d_ff=2,batch_size=128,d_core=512,d_model=64,dropout=0.0041,e_layers=2,learning_rate=0.0002,lradj=cosine_2024-08-26_13-51-55/checkpoint_000000)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed2   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator    â”‚
â”‚ Scheduler                        FIFOScheduler            â”‚
â”‚ Number of trials                 10                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed2
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-51-54_258050_2867952/artifacts/2024-08-26_13-51-55/ETTh2_96_96_test_seed2/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 13:51:55. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-96c86_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-96c86_00001   PENDING             128       0.000197675          64              2        512            2   0.00406315   cosine  â”‚
â”‚ trial-96c86_00002   PENDING             128       0.000118788         256              4         32            3   0.0047456    cosine  â”‚
â”‚ trial-96c86_00003   PENDING              16       9.97917e-05         128              3        128            3   0.00251376   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-96c86_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00475 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00012 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-96c86_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00406 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                       0.0002 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-96c86_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2870179)[0m configuration
[36m(_train_fn pid=2870179)[0m {'batch_size': 128, 'learning_rate': 0.00019767524747015385, 'd_model': 64, 'd_core': 512, 'e_layers': 2, 'dropout': 0.0040631510720679585, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2870179)[0m Use GPU: cuda:0

Trial trial-96c86_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00251 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                       0.0001 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2870180)[0m train 8449
[36m(_train_fn pid=2870181)[0m {'batch_size': 16, 'learning_rate': 9.979172523907365e-05, 'd_model': 128, 'd_core': 128, 'e_layers': 3, 'dropout': 0.002513757305101763, 'lradj': 'type1', 'd_ff': 384}
[36m(_train_fn pid=2870179)[0m val 2785
[36m(_train_fn pid=2870180)[0m start_epoch 0
[36m(_train_fn pid=2870180)[0m max_epoch 8
[36m(_train_fn pid=2870179)[0m Updating learning rate to 0.00019015168134596625
[36m(_train_fn pid=2870179)[0m saving checkpoint...
[36m(_train_fn pid=2870179)[0m Validation loss decreased (inf --> 0.2499).  Saving model state dict ...
[36m(_train_fn pid=2870179)[0m Epoch: 1 cost time: 0.9534518718719482
[36m(_train_fn pid=2870179)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2499411 Best vali loss: 0.2499411
[36m(_train_fn pid=2870178)[0m 	iters: 100, epoch: 1 | loss: 0.4930977
2024-08-26 13:51:58,412	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:58,920	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:59,530	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:51:59,649	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:00,246	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:00,450	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:00,572	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:01,194	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:01,653	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:01,913	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:02,728	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:02,737	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:02,888	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:03,251	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:04,041	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2870181)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed2/trial-96c86_00003_3_alpha_d_ff=3,batch_size=16,d_core=128,d_model=128,dropout=0.0025,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_13-51-55/checkpoint_000000)[32m [repeated 15x across cluster][0m
2024-08-26 13:52:04,244	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:05,471	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:06,970	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:06,999	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2870178)[0m 	speed: 0.0150s/iter; left time: 30.2766s
[36m(_train_fn pid=2870179)[0m Validation loss decreased (0.2499 --> 0.2337).  Saving model state dict ...
[36m(_train_fn pid=2870178)[0m Epoch: 1, Steps: 265 | Train Loss: 0.3751404 Vali Loss: 0.2217328 Best vali loss: 0.2217328
[36m(_train_fn pid=2870180)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2870180)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2870181)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2870178)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2870181)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2870181)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2870181)[0m val 2785[32m [repeated 3x across cluster][0m

Trial trial-96c86_00002 completed after 5 iterations at 2024-08-26 13:52:02. Total running time: 7s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                              1.0715 â”‚
â”‚ time_total_s                                 6.08084 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21557 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.21566 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2870181)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2870181)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2870180)[0m Early stopping
[36m(_train_fn pid=2870178)[0m Epoch: 2, Steps: 265 | Train Loss: 0.3592269 Vali Loss: 0.2183387 Best vali loss: 0.2183387

Trial trial-96c86_00001 completed after 8 iterations at 2024-08-26 13:52:03. Total running time: 8s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.52072 â”‚
â”‚ time_total_s                                 6.56735 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22073 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.22073 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2870179)[0m Updating learning rate to 0.0[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2870179)[0m saving checkpoint...[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2870178)[0m Validation loss decreased (inf --> 0.2217).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2870179)[0m Epoch: 8 cost time: 0.38779735565185547[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2870179)[0m Epoch: 8, Steps: 67 | Train Loss: nan Vali Loss: 0.2207278 Best vali loss: 0.2207278[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2870178)[0m 	iters: 200, epoch: 3 | loss: 0.3788062[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2870178)[0m 	speed: 0.0039s/iter; left time: 5.4490s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2870179)[0m Validation loss decreased (0.2209 --> 0.2207).  Saving model state dict ...[32m [repeated 7x across cluster][0m

Trial trial-96c86_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00133 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                       0.0043 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-96c86_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00189 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00053 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2871808)[0m {'batch_size': 16, 'learning_rate': 0.0005261407644632557, 'd_model': 64, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0018934743358690579, 'lradj': 'type1', 'd_ff': 192}
[36m(_train_fn pid=2870178)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3402422 Vali Loss: 0.2241174 Best vali loss: 0.2183387[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2870178)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m

Trial trial-96c86_00000 completed after 5 iterations at 2024-08-26 13:52:06. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.52577 â”‚
â”‚ time_total_s                                10.31904 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21834 â”‚
â”‚ train_loss                                   0.50716 â”‚
â”‚ valid_loss                                   0.22529 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
2024-08-26 13:52:07,627	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:09,105	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2871675)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed2/trial-96c86_00004_4_alpha_d_ff=4,batch_size=16,d_core=256,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0043,lradj=cosine_2024-08-26_13-52-02/checkpoint_000001)[32m [repeated 6x across cluster][0m
2024-08-26 13:52:10,496	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:10,541	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:11,014	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:11,786	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:12,995	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:13,056	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:13,156	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:14,320	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2872316)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed2/trial-96c86_00006_6_alpha_d_ff=3,batch_size=128,d_core=256,d_model=512,dropout=0.0011,e_layers=3,learning_rate=0.0004,lradj=cosine_2024-08-26_13-52-07/checkpoint_000003)[32m [repeated 8x across cluster][0m
2024-08-26 13:52:15,616	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:16,162	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:17,047	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:17,056	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2871808)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2871675)[0m {'batch_size': 16, 'learning_rate': 0.004303699199059842, 'd_model': 256, 'd_core': 256, 'e_layers': 2, 'dropout': 0.0013283609419647653, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2871808)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2871808)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2871808)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2871808)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2871808)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2870178)[0m Early stopping

Trial trial-96c86_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00106 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00044 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2872316)[0m {'batch_size': 128, 'learning_rate': 0.0004373103692112933, 'd_model': 512, 'd_core': 256, 'e_layers': 3, 'dropout': 0.0010618000764035455, 'lradj': 'cosine', 'd_ff': 1536}
[36m(_train_fn pid=2870181)[0m Updating learning rate to 4.9895862619536825e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2870181)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2871675)[0m Validation loss decreased (inf --> 0.2678).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2870181)[0m Epoch: 2 cost time: 3.0968589782714844[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2870181)[0m 	iters: 200, epoch: 3 | loss: 0.3085536[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2870181)[0m 	speed: 0.0051s/iter; left time: 15.3146s[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2870181)[0m Validation loss decreased (0.2188 --> 0.2159).  Saving model state dict ...
[36m(_train_fn pid=2871675)[0m Epoch: 2, Steps: 529 | Train Loss: 0.4405652 Vali Loss: 0.2680157 Best vali loss: 0.2678433[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2872316)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2334439 Best vali loss: 0.2334439
[36m(_train_fn pid=2871675)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2872316)[0m Validation loss decreased (0.2334 --> 0.2291).  Saving model state dict ...
[36m(_train_fn pid=2872316)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2290602 Best vali loss: 0.2290602
[36m(_train_fn pid=2872316)[0m configuration
[36m(_train_fn pid=2872316)[0m Use GPU: cuda:0
[36m(_train_fn pid=2872316)[0m train 8449
[36m(_train_fn pid=2872316)[0m val 2785

Trial trial-96c86_00004 completed after 4 iterations at 2024-08-26 13:52:12. Total running time: 17s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.97895 â”‚
â”‚ time_total_s                                 8.62345 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.26784 â”‚
â”‚ train_loss                                   0.62393 â”‚
â”‚ valid_loss                                   0.27054 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2871675)[0m Early stopping
[36m(_train_fn pid=2872316)[0m Validation loss decreased (0.2291 --> 0.2215).  Saving model state dict ...
[36m(_train_fn pid=2872316)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2215409 Best vali loss: 0.2215409
[36m(_train_fn pid=2872316)[0m start_epoch 0
[36m(_train_fn pid=2872316)[0m max_epoch 8
[36m(_train_fn pid=2872316)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2385906 Best vali loss: 0.2215409
[36m(_train_fn pid=2872316)[0m Updating learning rate to 0.00021865518460564666[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2872316)[0m saving checkpoint...[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2871808)[0m Validation loss decreased (inf --> 0.2216).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2872316)[0m Epoch: 4 cost time: 1.0701184272766113[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2871808)[0m 	iters: 200, epoch: 2 | loss: 0.2357936[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2871808)[0m 	speed: 0.0141s/iter; left time: 49.2939s[32m [repeated 16x across cluster][0m

Trial trial-96c86_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00458 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00294 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2873003)[0m configuration
[36m(_train_fn pid=2873003)[0m {'batch_size': 128, 'learning_rate': 0.0029367660552572427, 'd_model': 64, 'd_core': 256, 'e_layers': 2, 'dropout': 0.004581480582188235, 'lradj': 'type1', 'd_ff': 192}
[36m(_train_fn pid=2873003)[0m Use GPU: cuda:0
[36m(_train_fn pid=2873003)[0m train 8449
[36m(_train_fn pid=2873003)[0m val 2785
[36m(_train_fn pid=2873003)[0m start_epoch 0
[36m(_train_fn pid=2873003)[0m max_epoch 8
[36m(_train_fn pid=2872316)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2301559 Best vali loss: 0.2215409
[36m(_train_fn pid=2870181)[0m Epoch: 3, Steps: 529 | Train Loss: 0.3451168 Vali Loss: 0.2166695 Best vali loss: 0.2159394[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2873003)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2225717 Best vali loss: 0.2225717
[36m(_train_fn pid=2872316)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m

Trial trial-96c86_00006 completed after 6 iterations at 2024-08-26 13:52:17. Total running time: 21s
2024-08-26 13:52:17,565	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:18,096	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:18,518	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:19,126	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:20,463	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2873522)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed2/trial-96c86_00008_8_alpha_d_ff=4,batch_size=32,d_core=128,d_model=128,dropout=0.0026,e_layers=2,learning_rate=0.0005,lradj=type1_2024-08-26_13-52-17/checkpoint_000000)[32m [repeated 9x across cluster][0m
2024-08-26 13:52:21,470	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:22,088	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:22,534	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:23,555	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:23,772	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:23,867	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:24,139	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:24,655	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.43151 â”‚
â”‚ time_total_s                                 8.48042 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22154 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.23603 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-96c86_00007 completed after 4 iterations at 2024-08-26 13:52:18. Total running time: 23s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.52915 â”‚
â”‚ time_total_s                                 3.57317 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22257 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.22712 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2873003)[0m Early stopping[32m [repeated 2x across cluster][0m

Trial trial-96c86_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00262 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00054 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-96c86_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00522 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00084 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2873732)[0m {'batch_size': 32, 'learning_rate': 0.0008441193991653212, 'd_model': 512, 'd_core': 512, 'e_layers': 2, 'dropout': 0.005218605991821963, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=2870181)[0m Updating learning rate to 1.2473965654884206e-05[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2870181)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2873003)[0m Validation loss decreased (inf --> 0.2226).  Saving model state dict ...
[36m(_train_fn pid=2870181)[0m Epoch: 4 cost time: 5.46631932258606[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2871808)[0m 	iters: 100, epoch: 3 | loss: 0.7078356[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2871808)[0m 	speed: 0.0139s/iter; left time: 42.8123s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2873732)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2873522)[0m {'batch_size': 32, 'learning_rate': 0.0005396676589713644, 'd_model': 128, 'd_core': 128, 'e_layers': 2, 'dropout': 0.0026208182654582626, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2873732)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2873522)[0m train 8449
[36m(_train_fn pid=2873732)[0m train 8449
[36m(_train_fn pid=2873732)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2873522)[0m start_epoch 0
[36m(_train_fn pid=2873522)[0m max_epoch 8
[36m(_train_fn pid=2873732)[0m start_epoch 0
[36m(_train_fn pid=2873732)[0m max_epoch 8
[36m(_train_fn pid=2873522)[0m Validation loss decreased (inf --> 0.2311).  Saving model state dict ...
[36m(_train_fn pid=2873522)[0m Epoch: 1, Steps: 265 | Train Loss: 0.6746758 Vali Loss: 0.2310564 Best vali loss: 0.2310564[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2873003)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2271164 Best vali loss: 0.2225717[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2870181)[0m EarlyStopping counter: 2 out of 3[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2873522)[0m Validation loss decreased (0.2311 --> 0.2275).  Saving model state dict ...
[36m(_train_fn pid=2873522)[0m Validation loss decreased (0.2275 --> 0.2238).  Saving model state dict ...
[36m(_train_fn pid=2873522)[0m Validation loss decreased (0.2238 --> 0.2188).  Saving model state dict ...

Trial trial-96c86_00003 completed after 5 iterations at 2024-08-26 13:52:24. Total running time: 29s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                              5.0097 â”‚
â”‚ time_total_s                                27.39165 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21594 â”‚
â”‚ train_loss                                   0.42491 â”‚
â”‚ valid_loss                                   0.21726 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2870181)[0m Early stopping
[36m(_train_fn pid=2873522)[0m Updating learning rate to 3.3729228685710276e-05[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2873522)[0m saving checkpoint...[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2873522)[0m Epoch: 5 cost time: 0.9049320220947266[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2873732)[0m 	iters: 100, epoch: 3 | loss: 0.3803572[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=2873732)[0m 	speed: 0.0110s/iter; left time: 16.4406s[32m [repeated 24x across cluster][0m

2024-08-26 13:52:25,253	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:25,810	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2873522)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed2/trial-96c86_00008_8_alpha_d_ff=4,batch_size=32,d_core=128,d_model=128,dropout=0.0026,e_layers=2,learning_rate=0.0005,lradj=type1_2024-08-26_13-52-17/checkpoint_000005)[32m [repeated 10x across cluster][0m
2024-08-26 13:52:26,724	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:26,885	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:28,133	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:28,840	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:29,331	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
Trial status: 7 TERMINATED | 3 RUNNING
Current time: 2024-08-26 13:52:25. Total running time: 30s
Logical resource usage: 3.0/32 CPUs, 0.75/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 96c86_00002 with best_valid_loss=0.2155671870901084 and params={'batch_size': 128, 'learning_rate': 0.00011878788533227877, 'd_model': 256, 'd_core': 32, 'e_layers': 3, 'dropout': 0.004745600872570679, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-96c86_00005   RUNNING                16       0.000526141          64              3        512            4   0.00189347   type1          3           19.0949        0.449195       0.226465            0.221595 â”‚
â”‚ trial-96c86_00008   RUNNING                32       0.000539668         128              4        128            2   0.00262082   type1          5            6.06731       0.350934       0.222461            0.218783 â”‚
â”‚ trial-96c86_00009   RUNNING                32       0.000844119         512              1        512            2   0.00521861   cosine         3            5.61716       0.461622       0.23406             0.229809 â”‚
â”‚ trial-96c86_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5           10.319         0.507165       0.225285            0.218339 â”‚
â”‚ trial-96c86_00001   TERMINATED            128       0.000197675          64              2        512            2   0.00406315   cosine         8            6.56735     nan              0.220728            0.220728 â”‚
â”‚ trial-96c86_00002   TERMINATED            128       0.000118788         256              4         32            3   0.0047456    cosine         5            6.08084     nan              0.215663            0.215567 â”‚
â”‚ trial-96c86_00003   TERMINATED             16       9.97917e-05         128              3        128            3   0.00251376   type1          5           27.3917        0.424906       0.217255            0.215939 â”‚
â”‚ trial-96c86_00004   TERMINATED             16       0.0043037           256              4        256            2   0.00132836   cosine         4            8.62345       0.623926       0.270544            0.267843 â”‚
â”‚ trial-96c86_00006   TERMINATED            128       0.00043731          512              3        256            3   0.0010618    cosine         6            8.48042     nan              0.236032            0.221541 â”‚
â”‚ trial-96c86_00007   TERMINATED            128       0.00293677           64              3        256            2   0.00458148   type1          4            3.57317     nan              0.227116            0.222572 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2873732)[0m Validation loss decreased (inf --> 0.2323).  Saving model state dict ...
[36m(_train_fn pid=2873522)[0m Epoch: 6, Steps: 265 | Train Loss: 0.3228628 Vali Loss: 0.2222705 Best vali loss: 0.2187835[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2873522)[0m EarlyStopping counter: 2 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2873732)[0m Validation loss decreased (0.2323 --> 0.2298).  Saving model state dict ...
[36m(_train_fn pid=2873732)[0m Validation loss decreased (0.2298 --> 0.2271).  Saving model state dict ...

Trial trial-96c86_00008 completed after 7 iterations at 2024-08-26 13:52:26. Total running time: 31s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             1.07319 â”‚
â”‚ time_total_s                                 8.29235 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.21878 â”‚
â”‚ train_loss                                   0.21977 â”‚
â”‚ valid_loss                                   0.22394 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-96c86_00005 completed after 4 iterations at 2024-08-26 13:52:28. Total running time: 33s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             4.97124 â”‚
â”‚ time_total_s                                24.06617 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                               0.2216 â”‚
â”‚ train_loss                                   0.39321 â”‚
â”‚ valid_loss                                   0.23077 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2871808)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2873732)[0m Updating learning rate to 0.00012361842394220424[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2873732)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2873732)[0m Epoch: 6 cost time: 1.0239901542663574[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2873732)[0m 	iters: 200, epoch: 7 | loss: 0.2309719[32m [repeated 18x across cluster][0m
2024-08-26 13:52:30,403	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:30,412	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed2' in 0.0084s.
[36m(_train_fn pid=2873732)[0m 	speed: 0.0032s/iter; left time: 1.0740s[32m [repeated 18x across cluster][0m

Trial trial-96c86_00009 completed after 7 iterations at 2024-08-26 13:52:30. Total running time: 35s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-96c86_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             1.06966 â”‚
â”‚ time_total_s                                10.74977 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                               0.2271 â”‚
â”‚ train_loss                                   0.22636 â”‚
â”‚ valid_loss                                   0.24426 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:52:30. Total running time: 35s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 96c86_00002 with best_valid_loss=0.2155671870901084 and params={'batch_size': 128, 'learning_rate': 0.00011878788533227877, 'd_model': 256, 'd_core': 32, 'e_layers': 3, 'dropout': 0.004745600872570679, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-96c86_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5           10.319         0.507165       0.225285            0.218339 â”‚
â”‚ trial-96c86_00001   TERMINATED            128       0.000197675          64              2        512            2   0.00406315   cosine         8            6.56735     nan              0.220728            0.220728 â”‚
â”‚ trial-96c86_00002   TERMINATED            128       0.000118788         256              4         32            3   0.0047456    cosine         5            6.08084     nan              0.215663            0.215567 â”‚
â”‚ trial-96c86_00003   TERMINATED             16       9.97917e-05         128              3        128            3   0.00251376   type1          5           27.3917        0.424906       0.217255            0.215939 â”‚
â”‚ trial-96c86_00004   TERMINATED             16       0.0043037           256              4        256            2   0.00132836   cosine         4            8.62345       0.623926       0.270544            0.267843 â”‚
â”‚ trial-96c86_00005   TERMINATED             16       0.000526141          64              3        512            4   0.00189347   type1          4           24.0662        0.393215       0.230772            0.221595 â”‚
â”‚ trial-96c86_00006   TERMINATED            128       0.00043731          512              3        256            3   0.0010618    cosine         6            8.48042     nan              0.236032            0.221541 â”‚
â”‚ trial-96c86_00007   TERMINATED            128       0.00293677           64              3        256            2   0.00458148   type1          4            3.57317     nan              0.227116            0.222572 â”‚
â”‚ trial-96c86_00008   TERMINATED             32       0.000539668         128              4        128            2   0.00262082   type1          7            8.29235       0.219766       0.223938            0.218783 â”‚
â”‚ trial-96c86_00009   TERMINATED             32       0.000844119         512              1        512            2   0.00521861   cosine         7           10.7498        0.22636        0.24426             0.227104 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 128, 'learning_rate': 0.00011878788533227877, 'd_model': 256, 'd_core': 32, 'e_layers': 3, 'dropout': 0.004745600872570679, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2873732)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed2/trial-96c86_00009_9_alpha_d_ff=1,batch_size=32,d_core=512,d_model=512,dropout=0.0052,e_layers=2,learning_rate=0.0008,lradj=cosine_2024-08-26_13-52-18/checkpoint_000006)[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2873732)[0m Epoch: 7, Steps: 265 | Train Loss: 0.2263599 Vali Loss: 0.2442603 Best vali loss: 0.2271040[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2873732)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2873732)[0m Early stopping
[36m(_train_fn pid=2873732)[0m Updating learning rate to 3.2127381640378004e-05
[36m(_train_fn pid=2873732)[0m saving checkpoint...
[36m(_train_fn pid=2873732)[0m Epoch: 7 cost time: 0.9020869731903076


Time taken (4 parallel trials): 40 seconds


2024-08-26 13:52:34,425	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:52:34,792	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:52:34,798	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:52:34,805	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:52:37,955	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed3   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator    â”‚
â”‚ Scheduler                        FIFOScheduler            â”‚
â”‚ Number of trials                 10                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed3
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-52-33_992520_2875151/artifacts/2024-08-26_13-52-34/ETTh2_96_96_test_seed3/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 13:52:34. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-ae776_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-ae776_00001   PENDING             128       0.000148796          32              1        512            4   0.00103227   type1   â”‚
â”‚ trial-ae776_00002   PENDING              16       0.000454018         128              2         32            3   0.00108838   type1   â”‚
â”‚ trial-ae776_00003   PENDING              32       0.00054007           64              2        128            3   0.00590003   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ae776_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ae776_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00109 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00045 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ae776_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00103 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00015 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ae776_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                             0.0059 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00054 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2877376)[0m configuration
[36m(_train_fn pid=2877376)[0m {'batch_size': 16, 'learning_rate': 0.0004540175080048915, 'd_model': 128, 'd_core': 32, 'e_layers': 3, 'dropout': 0.0010883752469816998, 'lradj': 'type1', 'd_ff': 256}
[36m(_train_fn pid=2877376)[0m Use GPU: cuda:0
[36m(_train_fn pid=2877374)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2877376)[0m train 8449
[36m(_train_fn pid=2877376)[0m val 2785
[36m(_train_fn pid=2877376)[0m start_epoch 0
[36m(_train_fn pid=2877376)[0m max_epoch 8
[36m(_train_fn pid=2877374)[0m 	iters: 100, epoch: 1 | loss: 0.2426268
[36m(_train_fn pid=2877374)[0m 	speed: 0.0122s/iter; left time: 24.5821s
[36m(_train_fn pid=2877375)[0m Updating learning rate to 0.0001487959181348439
[36m(_train_fn pid=2877375)[0m saving checkpoint...
[36m(_train_fn pid=2877375)[0m Validation loss decreased (inf --> 0.2753).  Saving model state dict ...
[36m(_train_fn pid=2877375)[0m Epoch: 1 cost time: 0.9856235980987549
[36m(_train_fn pid=2877375)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2753013 Best vali loss: 0.2753013[36m(_train_fn pid=2877375)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed3/trial-ae776_00001_1_alpha_d_ff=1,batch_size=128,d_core=512,d_model=32,dropout=0.0010,e_layers=4,learning_rate=0.0001,lradj=type1_2024-08-26_13-52-34/checkpoint_000000)
2024-08-26 13:52:38,795	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2877375)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed3/trial-ae776_00001_1_alpha_d_ff=1,batch_size=128,d_core=512,d_model=32,dropout=0.0010,e_layers=4,learning_rate=0.0001,lradj=type1_2024-08-26_13-52-34/checkpoint_000001)
2024-08-26 13:52:39,019	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:39,485	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:39,603	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:40,422	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:40,894	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:41,241	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:41,936	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:42,077	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:42,761	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:42,808	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:42,906	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:43,750	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2877375)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed3/trial-ae776_00001_1_alpha_d_ff=1,batch_size=128,d_core=512,d_model=32,dropout=0.0010,e_layers=4,learning_rate=0.0001,lradj=type1_2024-08-26_13-52-34/checkpoint_000007)[32m [repeated 12x across cluster][0m
2024-08-26 13:52:44,184	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:44,498	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:45,820	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.

[36m(_train_fn pid=2877375)[0m Updating learning rate to 7.439795906742196e-05
[36m(_train_fn pid=2877375)[0m saving checkpoint...
[36m(_train_fn pid=2877375)[0m Validation loss decreased (0.2753 --> 0.2553).  Saving model state dict ...
[36m(_train_fn pid=2877375)[0m Epoch: 2 cost time: 0.6983802318572998
[36m(_train_fn pid=2877375)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2553463 Best vali loss: 0.2553463
[36m(_train_fn pid=2877374)[0m Epoch: 1, Steps: 265 | Train Loss: 0.3811444 Vali Loss: 0.2191478 Best vali loss: 0.2191478
[36m(_train_fn pid=2877375)[0m Validation loss decreased (0.2553 --> 0.2506).  Saving model state dict ...
[36m(_train_fn pid=2877375)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2506164 Best vali loss: 0.2506164
[36m(_train_fn pid=2877375)[0m Validation loss decreased (0.2506 --> 0.2490).  Saving model state dict ...
[36m(_train_fn pid=2877375)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2489821 Best vali loss: 0.2489821
[36m(_train_fn pid=2877374)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2877375)[0m Validation loss decreased (0.2490 --> 0.2484).  Saving model state dict ...
[36m(_train_fn pid=2877375)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2483658 Best vali loss: 0.2483658
[36m(_train_fn pid=2877374)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2877375)[0m {'batch_size': 128, 'learning_rate': 0.0001487959181348439, 'd_model': 32, 'd_core': 512, 'e_layers': 4, 'dropout': 0.001032271317679711, 'lradj': 'type1', 'd_ff': 32}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2877377)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2877375)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2877374)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2877374)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2877374)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2877375)[0m Validation loss decreased (0.2484 --> 0.2478).  Saving model state dict ...
[36m(_train_fn pid=2877375)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2477930 Best vali loss: 0.2477930
[36m(_train_fn pid=2877375)[0m Validation loss decreased (0.2478 --> 0.2475).  Saving model state dict ...
[36m(_train_fn pid=2877375)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2475075 Best vali loss: 0.2475075
[36m(_train_fn pid=2877377)[0m 	iters: 100, epoch: 3 | loss: 0.3644589[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2877377)[0m 	speed: 0.0159s/iter; left time: 23.7219s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2877375)[0m Updating learning rate to 2.324936220856936e-06[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2877375)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2877376)[0m Validation loss decreased (inf --> 0.2201).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2877375)[0m Epoch: 7 cost time: 0.6823751926422119[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2877375)[0m Validation loss decreased (0.2475 --> 0.2474).  Saving model state dict ...

Trial trial-ae776_00001 completed after 8 iterations at 2024-08-26 13:52:43. Total running time: 8s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.84133 â”‚
â”‚ time_total_s                                 7.33919 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.24736 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.24736 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2877375)[0m Epoch: 8, Steps: 67 | Train Loss: nan Vali Loss: 0.2473635 Best vali loss: 0.2473635
[36m(_train_fn pid=2877377)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3702085 Vali Loss: 0.2249094 Best vali loss: 0.2205320[32m [repeated 6x across cluster][0m

Trial trial-ae776_00000 completed after 4 iterations at 2024-08-26 13:52:44. Total running time: 9s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.73453 â”‚
â”‚ time_total_s                                 8.13716 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21915 â”‚
â”‚ train_loss                                   0.57542 â”‚
â”‚ valid_loss                                   0.22758 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2877374)[0m Early stopping

Trial trial-ae776_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00177 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00019 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ae776_00003 completed after 4 iterations at 2024-08-26 13:52:45. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.63332 â”‚
â”‚ time_total_s                                 9.40522 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22053 â”‚
â”‚ train_loss                                   0.48488 â”‚
â”‚ valid_loss                                   0.22372 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2877377)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m

2024-08-26 13:52:46,451	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:47,169	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:47,557	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:47,699	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:47,985	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:48,428	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:48,848	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2879240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed3/trial-ae776_00006_6_alpha_d_ff=1,batch_size=64,d_core=256,d_model=128,dropout=0.0054,e_layers=2,learning_rate=0.0004,lradj=cosine_2024-08-26_13-52-45/checkpoint_000000)[32m [repeated 10x across cluster][0m
2024-08-26 13:52:49,321	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:49,443	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:49,617	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:50,408	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:50,930	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:51,268	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:52,116	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
Trial trial-ae776_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00895 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00442 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2879091)[0m {'batch_size': 128, 'learning_rate': 0.004424768501490731, 'd_model': 32, 'd_core': 64, 'e_layers': 1, 'dropout': 0.008950011277392856, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2877376)[0m Validation loss decreased (0.2201 --> 0.2193).  Saving model state dict ...
[36m(_train_fn pid=2879091)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2878920)[0m {'batch_size': 64, 'learning_rate': 0.0001850854724353575, 'd_model': 512, 'd_core': 256, 'e_layers': 3, 'dropout': 0.001767487297248069, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2879091)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2879091)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2879091)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2879091)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2879091)[0m max_epoch 8[32m [repeated 2x across cluster][0m

Trial trial-ae776_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00543 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00044 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2877376)[0m 	iters: 300, epoch: 3 | loss: 0.1627533[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2877376)[0m 	speed: 0.0043s/iter; left time: 12.4734s[32m [repeated 14x across cluster][0m

Trial trial-ae776_00005 completed after 4 iterations at 2024-08-26 13:52:48. Total running time: 13s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.44038 â”‚
â”‚ time_total_s                                 2.34014 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22424 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.22738 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2879091)[0m Updating learning rate to 0.0022123842507453654[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2879091)[0m saving checkpoint...[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2878920)[0m Validation loss decreased (inf --> 0.2150).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2879091)[0m Epoch: 4 cost time: 0.30896735191345215[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2879091)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2273806 Best vali loss: 0.2242403[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2878920)[0m Epoch: 2, Steps: 133 | Train Loss: 0.7510113 Vali Loss: 0.2163197 Best vali loss: 0.2150119[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2879091)[0m Early stopping[32m [repeated 2x across cluster][0m

Trial trial-ae776_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                             0.0014 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00008 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2879818)[0m {'batch_size': 32, 'learning_rate': 7.597582616965e-05, 'd_model': 64, 'd_core': 256, 'e_layers': 3, 'dropout': 0.0014020571248991246, 'lradj': 'type1', 'd_ff': 192}
[36m(_train_fn pid=2878920)[0m EarlyStopping counter: 2 out of 3[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2879240)[0m {'batch_size': 64, 'learning_rate': 0.0004381162227366631, 'd_model': 128, 'd_core': 256, 'e_layers': 2, 'dropout': 0.005426820494453241, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2879240)[0m Validation loss decreased (0.2232 --> 0.2201).  Saving model state dict ...
[36m(_train_fn pid=2879818)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2879818)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2879818)[0m train 8449[32m [repeated 2x across cluster][0m

Trial trial-ae776_00006 completed after 5 iterations at 2024-08-26 13:52:52. Total running time: 17s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             0.84524 â”‚
â”‚ time_total_s                                  4.7303 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22013 â”‚
â”‚ train_loss                                   0.27717 â”‚
â”‚ valid_loss                                   0.22927 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2879818)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2879818)[0m start_epoch 0[32m [repeated 2x across cluster][0m
2024-08-26 13:52:52,548	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:52,654	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:53,048	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:54,328	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2879818)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed3/trial-ae776_00007_7_alpha_d_ff=3,batch_size=32,d_core=256,d_model=64,dropout=0.0014,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_13-52-48/checkpoint_000001)[32m [repeated 11x across cluster][0m
2024-08-26 13:52:55,353	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:56,379	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:57,141	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:57,364	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:58,376	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:58,441	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:58,852	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:52:59,220	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2879818)[0m max_epoch 8[32m [repeated 2x across cluster][0m

Trial trial-ae776_00004 completed after 4 iterations at 2024-08-26 13:52:52. Total running time: 17s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                              1.6153 â”‚
â”‚ time_total_s                                 7.21861 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21501 â”‚
â”‚ train_loss                                   0.22715 â”‚
â”‚ valid_loss                                   0.21736 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2877376)[0m 	iters: 500, epoch: 4 | loss: 0.3705913[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2877376)[0m 	speed: 0.0051s/iter; left time: 10.9866s[32m [repeated 17x across cluster][0m

Trial trial-ae776_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00145 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                       0.0002 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2877376)[0m Updating learning rate to 5.6752188500611436e-05[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2877376)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2879818)[0m Validation loss decreased (inf --> 0.2444).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2877376)[0m Epoch: 4 cost time: 3.139730930328369[32m [repeated 11x across cluster][0m

Trial trial-ae776_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00255 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00103 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2879818)[0m Validation loss decreased (0.2444 --> 0.2288).  Saving model state dict ...
[36m(_train_fn pid=2879818)[0m Epoch: 2, Steps: 265 | Train Loss: 0.3413977 Vali Loss: 0.2288018 Best vali loss: 0.2288018[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2878920)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2880612)[0m {'batch_size': 16, 'learning_rate': 0.0010334142029794543, 'd_model': 512, 'd_core': 256, 'e_layers': 1, 'dropout': 0.0025534704338397757, 'lradj': 'type1', 'd_ff': 512}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2877376)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2880612)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2880612)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2880612)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2879818)[0m Validation loss decreased (0.2288 --> 0.2245).  Saving model state dict ...
[36m(_train_fn pid=2880612)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2880612)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2880612)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2880612)[0m 	iters: 500, epoch: 1 | loss: 0.3977031[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2880612)[0m 	speed: 0.0068s/iter; left time: 25.4449s[32m [repeated 19x across cluster][0m

Trial trial-ae776_00008 completed after 4 iterations at 2024-08-26 13:52:58. Total running time: 23s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.00943 â”‚
â”‚ time_total_s                                 4.67688 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21778 â”‚
â”‚ train_loss                                   0.25463 â”‚
â”‚ valid_loss                                   0.22004 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ae776_00002 completed after 5 iterations at 2024-08-26 13:52:58. Total running time: 23s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.38989 â”‚
â”‚ time_total_s                                 22.0646 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21926 â”‚
â”‚ train_loss                                   0.27428 â”‚
â”‚ valid_loss                                   0.22373 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2880612)[0m Updating learning rate to 0.0010334142029794543[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2880612)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2880612)[0m Validation loss decreased (inf --> 0.2301).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2877376)[0m Epoch: 5 cost time: 4.302131652832031[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2879818)[0m Validation loss decreased (0.2245 --> 0.2232).  Saving model state dict ...
2024-08-26 13:53:00,751	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2880612)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed3/trial-ae776_00009_9_alpha_d_ff=1,batch_size=16,d_core=256,d_model=512,dropout=0.0026,e_layers=1,learning_rate=0.0010,lradj=type1_2024-08-26_13-52-52/checkpoint_000001)[32m [repeated 9x across cluster][0m
2024-08-26 13:53:00,885	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:02,564	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:02,743	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:04,239	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:04,724	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:05,883	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2879818)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed3/trial-ae776_00007_7_alpha_d_ff=3,batch_size=32,d_core=256,d_model=64,dropout=0.0014,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_13-52-48/checkpoint_000007)[32m [repeated 6x across cluster][0m
2024-08-26 13:53:06,424	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2879818)[0m Epoch: 4, Steps: 265 | Train Loss: 0.3889370 Vali Loss: 0.2232349 Best vali loss: 0.2232349[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2877376)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2880612)[0m Validation loss decreased (0.2301 --> 0.2290).  Saving model state dict ...
[36m(_train_fn pid=2880482)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2880612)[0m 	iters: 200, epoch: 4 | loss: 0.1537196[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2880612)[0m 	speed: 0.0031s/iter; left time: 7.6545s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2880612)[0m Updating learning rate to 0.00025835355074486357[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2880612)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2880612)[0m Epoch: 3 cost time: 1.630244255065918[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2879818)[0m Epoch: 7, Steps: 265 | Train Loss: 0.3082609 Vali Loss: 0.2220804 Best vali loss: 0.2220804[32m [repeated 5x across cluster][0m

Trial status: 8 TERMINATED | 2 RUNNING
Current time: 2024-08-26 13:53:04. Total running time: 30s
Logical resource usage: 2.0/32 CPUs, 0.5/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: ae776_00004 with best_valid_loss=0.21501186272828327 and params={'batch_size': 64, 'learning_rate': 0.0001850854724353575, 'd_model': 512, 'd_core': 256, 'e_layers': 3, 'dropout': 0.001767487297248069, 'lradj': 'type1', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-ae776_00007   RUNNING                32       7.59758e-05          64              3        256            3   0.00140206   type1          7           14.2223        0.308261       0.22208             0.22208  â”‚
â”‚ trial-ae776_00009   RUNNING                16       0.00103341          512              1        256            1   0.00255347   type1          4           10.6359        0.240219       0.237827            0.228994 â”‚
â”‚ trial-ae776_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            8.13716       0.57542        0.227578            0.219148 â”‚
â”‚ trial-ae776_00001   TERMINATED            128       0.000148796          32              1        512            4   0.00103227   type1          8            7.33919     nan              0.247363            0.247363 â”‚
â”‚ trial-ae776_00002   TERMINATED             16       0.000454018         128              2         32            3   0.00108838   type1          5           22.0646        0.27428        0.223726            0.219258 â”‚
â”‚ trial-ae776_00003   TERMINATED             32       0.00054007           64              2        128            3   0.00590003   type1          4            9.40522       0.484876       0.223725            0.220532 â”‚
â”‚ trial-ae776_00004   TERMINATED             64       0.000185085         512              2        256            3   0.00176749   type1          4            7.21861       0.227154       0.217357            0.215012 â”‚
â”‚ trial-ae776_00005   TERMINATED            128       0.00442477           32              4         64            1   0.00895001   cosine         4            2.34014     nan              0.227381            0.22424  â”‚
â”‚ trial-ae776_00006   TERMINATED             64       0.000438116         128              1        256            2   0.00542682   cosine         5            4.7303        0.277168       0.229266            0.220133 â”‚
â”‚ trial-ae776_00008   TERMINATED             64       0.000197377         256              2        256            2   0.0014465    type1          4            4.67688       0.254626       0.220039            0.21778  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ae776_00007 completed after 8 iterations at 2024-08-26 13:53:05. Total running time: 31s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.64128 â”‚
â”‚ time_total_s                                15.86355 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22202 â”‚
â”‚ train_loss                                   0.30061 â”‚
â”‚ valid_loss                                   0.22202 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2879818)[0m Validation loss decreased (0.2221 --> 0.2220).  Saving model state dict ...[32m [repeated 4x across cluster][0m

Trial trial-ae776_00009 completed after 5 iterations at 2024-08-26 13:53:06. Total running time: 31s
2024-08-26 13:53:06,433	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed3' in 0.0080s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ae776_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.69694 â”‚
â”‚ time_total_s                                 12.3328 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22899 â”‚
â”‚ train_loss                                   0.31746 â”‚
â”‚ valid_loss                                   0.25199 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:53:06. Total running time: 31s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: ae776_00004 with best_valid_loss=0.21501186272828327 and params={'batch_size': 64, 'learning_rate': 0.0001850854724353575, 'd_model': 512, 'd_core': 256, 'e_layers': 3, 'dropout': 0.001767487297248069, 'lradj': 'type1', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-ae776_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            8.13716       0.57542        0.227578            0.219148 â”‚
â”‚ trial-ae776_00001   TERMINATED            128       0.000148796          32              1        512            4   0.00103227   type1          8            7.33919     nan              0.247363            0.247363 â”‚
â”‚ trial-ae776_00002   TERMINATED             16       0.000454018         128              2         32            3   0.00108838   type1          5           22.0646        0.27428        0.223726            0.219258 â”‚
â”‚ trial-ae776_00003   TERMINATED             32       0.00054007           64              2        128            3   0.00590003   type1          4            9.40522       0.484876       0.223725            0.220532 â”‚
â”‚ trial-ae776_00004   TERMINATED             64       0.000185085         512              2        256            3   0.00176749   type1          4            7.21861       0.227154       0.217357            0.215012 â”‚
â”‚ trial-ae776_00005   TERMINATED            128       0.00442477           32              4         64            1   0.00895001   cosine         4            2.34014     nan              0.227381            0.22424  â”‚
â”‚ trial-ae776_00006   TERMINATED             64       0.000438116         128              1        256            2   0.00542682   cosine         5            4.7303        0.277168       0.229266            0.220133 â”‚
â”‚ trial-ae776_00007   TERMINATED             32       7.59758e-05          64              3        256            3   0.00140206   type1          8           15.8635        0.300607       0.222016            0.222016 â”‚
â”‚ trial-ae776_00008   TERMINATED             64       0.000197377         256              2        256            2   0.0014465    type1          4            4.67688       0.254626       0.220039            0.21778  â”‚
â”‚ trial-ae776_00009   TERMINATED             16       0.00103341          512              1        256            1   0.00255347   type1          5           12.3328        0.317455       0.25199             0.228994 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 64, 'learning_rate': 0.0001850854724353575, 'd_model': 512, 'd_core': 256, 'e_layers': 3, 'dropout': 0.001767487297248069, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2880612)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed3/trial-ae776_00009_9_alpha_d_ff=1,batch_size=16,d_core=256,d_model=512,dropout=0.0026,e_layers=1,learning_rate=0.0010,lradj=type1_2024-08-26_13-52-52/checkpoint_000004)
[36m(_train_fn pid=2880612)[0m Early stopping
[36m(_train_fn pid=2880612)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2880612)[0m 	iters: 500, epoch: 5 | loss: 0.5215759[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2880612)[0m 	speed: 0.0022s/iter; left time: 3.5902s[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2880612)[0m Updating learning rate to 6.458838768621589e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2880612)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2880612)[0m Epoch: 5 cost time: 1.4515721797943115[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2880612)[0m Epoch: 5, Steps: 529 | Train Loss: 0.3174551 Vali Loss: 0.2519900 Best vali loss: 0.2289945[32m [repeated 3x across cluster][0m


Time taken (4 parallel trials): 36 seconds


2024-08-26 13:53:10,351	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:53:10,724	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:53:10,730	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:53:10,737	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:53:16,045	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed4   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator    â”‚
â”‚ Scheduler                        FIFOScheduler            â”‚
â”‚ Number of trials                 10                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-53-09_910385_2882048/artifacts/2024-08-26_13-53-10/ETTh2_96_96_test_seed4/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 13:53:10. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-c3dff_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-c3dff_00001   PENDING              64       0.000526775         512              4        512            4   0.00414377   cosine  â”‚
â”‚ trial-c3dff_00002   PENDING              16       0.00200804           64              3         32            3   0.00348991   type1   â”‚
â”‚ trial-c3dff_00003   PENDING              16       0.000450067         512              4        128            1   0.0114959    type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c3dff_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c3dff_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00414 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00053 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c3dff_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                             0.0115 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00045 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2884277)[0m configuration
[36m(_train_fn pid=2884277)[0m {'batch_size': 16, 'learning_rate': 0.0004500668771232701, 'd_model': 512, 'd_core': 128, 'e_layers': 1, 'dropout': 0.011495919864972797, 'lradj': 'type1', 'd_ff': 2048}
[36m(_train_fn pid=2884277)[0m Use GPU: cuda:0
[36m(_train_fn pid=2884274)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}

Trial trial-c3dff_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00349 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00201 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2884274)[0m train 8449
[36m(_train_fn pid=2884276)[0m val 2785
[36m(_train_fn pid=2884276)[0m start_epoch 0
[36m(_train_fn pid=2884276)[0m max_epoch 8
[36m(_train_fn pid=2884274)[0m 	iters: 100, epoch: 1 | loss: 0.7173259
[36m(_train_fn pid=2884274)[0m 	speed: 0.0149s/iter; left time: 30.0432s
[36m(_train_fn pid=2884275)[0m Updating learning rate to 0.000506725766919993
[36m(_train_fn pid=2884275)[0m saving checkpoint...
[36m(_train_fn pid=2884275)[0m Validation loss decreased (inf --> 0.2470).  Saving model state dict ...
[36m(_train_fn pid=2884274)[0m Epoch: 1 cost time: 2.780914545059204
[36m(_train_fn pid=2884275)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4/trial-c3dff_00001_1_alpha_d_ff=4,batch_size=64,d_core=512,d_model=512,dropout=0.0041,e_layers=4,learning_rate=0.0005,lradj=cosine_2024-08-26_13-53-10/checkpoint_000000)
2024-08-26 13:53:18,926	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:18,930	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:18,931	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:21,919	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:21,952	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2884275)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4/trial-c3dff_00001_1_alpha_d_ff=4,batch_size=64,d_core=512,d_model=512,dropout=0.0041,e_layers=4,learning_rate=0.0005,lradj=cosine_2024-08-26_13-53-10/checkpoint_000002)[32m [repeated 5x across cluster][0m
2024-08-26 13:53:23,062	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:24,727	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:24,851	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:24,912	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:27,644	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2884275)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4/trial-c3dff_00001_1_alpha_d_ff=4,batch_size=64,d_core=512,d_model=512,dropout=0.0041,e_layers=4,learning_rate=0.0005,lradj=cosine_2024-08-26_13-53-10/checkpoint_000004)[32m [repeated 6x across cluster][0m
2024-08-26 13:53:27,991	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:30,464	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:30,719	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:30,749	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:31,805	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:32,227	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:33,206	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:33,219	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2884277)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4/trial-c3dff_00003_3_alpha_d_ff=4,batch_size=16,d_core=128,d_model=512,dropout=0.0115,e_layers=1,learning_rate=0.0005,lradj=type1_2024-08-26_13-53-10/checkpoint_000003)[32m [repeated 7x across cluster][0m
2024-08-26 13:53:33,701	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2884274)[0m Epoch: 1, Steps: 265 | Train Loss: 0.5860755 Vali Loss: 0.2195585 Best vali loss: 0.2195585
[36m(_train_fn pid=2884276)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2884276)[0m {'batch_size': 16, 'learning_rate': 0.0020080402711157644, 'd_model': 64, 'd_core': 32, 'e_layers': 3, 'dropout': 0.003489913910428876, 'lradj': 'type1', 'd_ff': 192}
[36m(_train_fn pid=2884276)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2884275)[0m {'batch_size': 64, 'learning_rate': 0.0005267749444358936, 'd_model': 512, 'd_core': 512, 'e_layers': 4, 'dropout': 0.004143772196920276, 'lradj': 'cosine', 'd_ff': 2048}
[36m(_train_fn pid=2884275)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2884275)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2884275)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2884275)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2884274)[0m Validation loss decreased (0.2196 --> 0.2191).  Saving model state dict ...
[36m(_train_fn pid=2884276)[0m 	iters: 400, epoch: 1 | loss: 0.2426122[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2884276)[0m 	speed: 0.0112s/iter; left time: 42.9306s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2884275)[0m Updating learning rate to 0.00044963053990284033[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2884275)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2884277)[0m Validation loss decreased (inf --> 0.2294).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2884275)[0m Epoch: 2 cost time: 2.47563099861145[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2884275)[0m Epoch: 2, Steps: 133 | Train Loss: 0.3523355 Vali Loss: 0.2356418 Best vali loss: 0.2356418[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2884274)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2884275)[0m Validation loss decreased (0.2356 --> 0.2219).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2884276)[0m 	iters: 100, epoch: 2 | loss: 0.5599285[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2884276)[0m 	speed: 0.0353s/iter; left time: 127.2153s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2884274)[0m Updating learning rate to 0.00015[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2884274)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2884276)[0m Validation loss decreased (inf --> 0.2263).  Saving model state dict ...
[36m(_train_fn pid=2884274)[0m Epoch: 4 cost time: 2.496593952178955[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2884274)[0m Epoch: 4, Steps: 265 | Train Loss: 0.3011411 Vali Loss: 0.2156811 Best vali loss: 0.2156811[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2884275)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2884274)[0m Validation loss decreased (0.2191 --> 0.2157).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2884276)[0m 	iters: 400, epoch: 2 | loss: 0.2491507[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2884276)[0m 	speed: 0.0188s/iter; left time: 62.1441s[32m [repeated 13x across cluster][0m

Trial trial-c3dff_00001 completed after 6 iterations at 2024-08-26 13:53:30. Total running time: 19s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             2.81726 â”‚
â”‚ time_total_s                                18.12643 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22194 â”‚
â”‚ train_loss                                    0.2023 â”‚
â”‚ valid_loss                                   0.24267 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2884275)[0m Early stopping
[36m(_train_fn pid=2884274)[0m Updating learning rate to 4.3933982822017876e-05[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2884274)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2884274)[0m Epoch: 6 cost time: 2.4696550369262695[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2884274)[0m Epoch: 6, Steps: 265 | Train Loss: 0.4027096 Vali Loss: 0.2239814 Best vali loss: 0.2156811[32m [repeated 5x across cluster][0m

Trial trial-c3dff_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00416 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00433 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2885893)[0m configuration
[36m(_train_fn pid=2885893)[0m {'batch_size': 128, 'learning_rate': 0.004333469487295887, 'd_model': 64, 'd_core': 512, 'e_layers': 3, 'dropout': 0.004160591252268344, 'lradj': 'cosine', 'd_ff': 192}
[36m(_train_fn pid=2885893)[0m Use GPU: cuda:0
[36m(_train_fn pid=2885893)[0m train 8449
[36m(_train_fn pid=2885893)[0m val 2785

Trial trial-c3dff_00000 completed after 7 iterations at 2024-08-26 13:53:32. Total running time: 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             1.50609 â”‚
â”‚ time_total_s                                19.89918 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.21568 â”‚
â”‚ train_loss                                   0.21353 â”‚
â”‚ valid_loss                                   0.22359 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2885893)[0m start_epoch 0
[36m(_train_fn pid=2885893)[0m max_epoch 8
[36m(_train_fn pid=2884274)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2885893)[0m Validation loss decreased (inf --> 0.2368).  Saving model state dict ...
[36m(_train_fn pid=2885893)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2368014 Best vali loss: 0.2368014

2024-08-26 13:53:34,174	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:34,634	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:35,143	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:35,628	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:35,778	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:35,804	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:36,105	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:36,833	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:39,460	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2886188)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4/trial-c3dff_00005_5_alpha_d_ff=1,batch_size=16,d_core=256,d_model=256,dropout=0.0031,e_layers=1,learning_rate=0.0038,lradj=cosine_2024-08-26_13-53-32/checkpoint_000001)[32m [repeated 11x across cluster][0m
2024-08-26 13:53:39,840	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
Trial trial-c3dff_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00308 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00384 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2885893)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2360875 Best vali loss: 0.2360875
[36m(_train_fn pid=2885893)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2381739 Best vali loss: 0.2360875
[36m(_train_fn pid=2885893)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2336862 Best vali loss: 0.2336862
[36m(_train_fn pid=2885893)[0m Validation loss decreased (0.2361 --> 0.2337).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2884277)[0m 	iters: 500, epoch: 5 | loss: 0.1753742[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2884277)[0m 	speed: 0.0043s/iter; left time: 6.9843s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2885893)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2444818 Best vali loss: 0.2336862
[36m(_train_fn pid=2885893)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2359388 Best vali loss: 0.2336862
[36m(_train_fn pid=2884274)[0m Early stopping

Trial trial-c3dff_00003 completed after 5 iterations at 2024-08-26 13:53:35. Total running time: 25s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             2.58291 â”‚
â”‚ time_total_s                                23.46288 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21988 â”‚
â”‚ train_loss                                   0.18528 â”‚
â”‚ valid_loss                                   0.23575 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2884277)[0m Early stopping

Trial trial-c3dff_00004 completed after 7 iterations at 2024-08-26 13:53:36. Total running time: 25s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             0.47426 â”‚
â”‚ time_total_s                                 4.05424 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.23369 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.24062 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2885893)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2406153 Best vali loss: 0.2336862
[36m(_train_fn pid=2885893)[0m Updating learning rate to 0.00016493286161051876[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2885893)[0m saving checkpoint...[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2885893)[0m Epoch: 7 cost time: 0.36969804763793945[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2884277)[0m Epoch: 5, Steps: 529 | Train Loss: 0.1852751 Vali Loss: 0.2357522 Best vali loss: 0.2198776[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2886188)[0m configuration
[36m(_train_fn pid=2886188)[0m {'batch_size': 16, 'learning_rate': 0.003842176953014058, 'd_model': 256, 'd_core': 256, 'e_layers': 1, 'dropout': 0.003080546140771698, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2886188)[0m Use GPU: cuda:0

Trial trial-c3dff_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00147 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                       0.0005 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2886188)[0m train 8449
[36m(_train_fn pid=2886188)[0m val 2785
[36m(_train_fn pid=2886959)[0m configuration
[36m(_train_fn pid=2886959)[0m {'batch_size': 32, 'learning_rate': 0.0005005210915705279, 'd_model': 128, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0014664433036895423, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2886959)[0m Use GPU: cuda:0
[36m(_train_fn pid=2886959)[0m train 8449
[36m(_train_fn pid=2886959)[0m val 2785

Trial trial-c3dff_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00165 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00096 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2887024)[0m {'batch_size': 8, 'learning_rate': 0.000957881647008655, 'd_model': 128, 'd_core': 512, 'e_layers': 3, 'dropout': 0.0016522908437256163, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2886188)[0m start_epoch 0
[36m(_train_fn pid=2886188)[0m max_epoch 8
[36m(_train_fn pid=2886959)[0m start_epoch 0
[36m(_train_fn pid=2886959)[0m max_epoch 8
[36m(_train_fn pid=2885893)[0m EarlyStopping counter: 3 out of 3[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2886188)[0m Validation loss decreased (inf --> 0.2296).  Saving model state dict ...
[36m(_train_fn pid=2884276)[0m Validation loss decreased (0.2257 --> 0.2211).  Saving model state dict ...
2024-08-26 13:53:40,883	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:43,565	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:43,597	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:45,424	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2884276)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4/trial-c3dff_00002_2_alpha_d_ff=3,batch_size=16,d_core=32,d_model=64,dropout=0.0035,e_layers=3,learning_rate=0.0020,lradj=type1_2024-08-26_13-53-10/checkpoint_000004)[32m [repeated 5x across cluster][0m
2024-08-26 13:53:46,336	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:47,524	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:48,929	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:49,185	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:50,862	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2884276)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4/trial-c3dff_00002_2_alpha_d_ff=3,batch_size=16,d_core=32,d_model=64,dropout=0.0035,e_layers=3,learning_rate=0.0020,lradj=type1_2024-08-26_13-53-10/checkpoint_000005)[32m [repeated 5x across cluster][0m
2024-08-26 13:53:51,158	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2886188)[0m 	iters: 200, epoch: 3 | loss: 0.3407074[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2886188)[0m 	speed: 0.0057s/iter; left time: 16.9490s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2886959)[0m Validation loss decreased (inf --> 0.2332).  Saving model state dict ...

Trial status: 4 TERMINATED | 4 RUNNING
Current time: 2024-08-26 13:53:40. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: c3dff_00000 with best_valid_loss=0.2156810847604296 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-c3dff_00002   RUNNING                16       0.00200804           64              3         32            3   0.00348991   type1          4           27.4642        0.480444       0.22603             0.221103 â”‚
â”‚ trial-c3dff_00005   RUNNING                16       0.00384218          256              1        256            1   0.00308055   cosine         2            5.70385       0.484048       0.252546            0.229614 â”‚
â”‚ trial-c3dff_00006   RUNNING                32       0.000500521         128              2        512            4   0.00146644   cosine         1            3.49958       0.25858        0.233217            0.233217 â”‚
â”‚ trial-c3dff_00007   RUNNING                 8       0.000957882         128              4        512            3   0.00165229   type1                                                                                 â”‚
â”‚ trial-c3dff_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         7           19.8992        0.213533       0.223586            0.215681 â”‚
â”‚ trial-c3dff_00001   TERMINATED             64       0.000526775         512              4        512            4   0.00414377   cosine         6           18.1264        0.202304       0.242669            0.221941 â”‚
â”‚ trial-c3dff_00003   TERMINATED             16       0.000450067         512              4        128            1   0.0114959    type1          5           23.4629        0.185275       0.235752            0.219878 â”‚
â”‚ trial-c3dff_00004   TERMINATED            128       0.00433347           64              3        512            3   0.00416059   cosine         7            4.05424     nan              0.240615            0.233686 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2885893)[0m Early stopping
[36m(_train_fn pid=2886959)[0m Updating learning rate to 0.0004814711418313731[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2886959)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2886959)[0m Epoch: 1 cost time: 2.7124404907226562[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2886959)[0m Epoch: 1, Steps: 265 | Train Loss: 0.2585802 Vali Loss: 0.2332170 Best vali loss: 0.2332170[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2887024)[0m configuration
[36m(_train_fn pid=2887024)[0m Use GPU: cuda:0
[36m(_train_fn pid=2887024)[0m train 8449
[36m(_train_fn pid=2887024)[0m val 2785
[36m(_train_fn pid=2887024)[0m start_epoch 0
[36m(_train_fn pid=2887024)[0m max_epoch 8
[36m(_train_fn pid=2884276)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2886188)[0m Validation loss decreased (0.2296 --> 0.2290).  Saving model state dict ...
[36m(_train_fn pid=2886188)[0m 	iters: 400, epoch: 4 | loss: 0.3379504[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2886188)[0m 	speed: 0.0057s/iter; left time: 12.8958s[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2886959)[0m Updating learning rate to 0.0003460311104319295[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2886959)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2886959)[0m Epoch: 3 cost time: 2.368974447250366[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2886959)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3794613 Vali Loss: 0.2349907 Best vali loss: 0.2328914[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2886188)[0m EarlyStopping counter: 1 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2886959)[0m Validation loss decreased (0.2332 --> 0.2329).  Saving model state dict ...
[36m(_train_fn pid=2886959)[0m Validation loss decreased (0.2329 --> 0.2234).  Saving model state dict ...
[36m(_train_fn pid=2887024)[0m Validation loss decreased (inf --> 0.2565).  Saving model state dict ...

Trial trial-c3dff_00002 completed after 6 iterations at 2024-08-26 13:53:50. Total running time: 40s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             5.43553 â”‚
â”‚ time_total_s                                38.48179 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                               0.2211 â”‚
â”‚ train_loss                                   0.31085 â”‚
â”‚ valid_loss                                   0.23048 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2884276)[0m Early stopping
[36m(_train_fn pid=2887024)[0m 	iters: 200, epoch: 2 | loss: 0.3845322[32m [repeated 19x across cluster][0m
2024-08-26 13:53:51,505	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:53,662	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:53,923	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:54,136	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:54,219	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:54,756	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:55,280	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:55,815	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:55,999	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2886959)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4/trial-c3dff_00006_6_alpha_d_ff=2,batch_size=32,d_core=512,d_model=128,dropout=0.0015,e_layers=4,learning_rate=0.0005,lradj=cosine_2024-08-26_13-53-35/checkpoint_000006)[32m [repeated 10x across cluster][0m
2024-08-26 13:53:57,041	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:57,312	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:57,704	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2887024)[0m 	speed: 0.0081s/iter; left time: 58.1809s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2886959)[0m Updating learning rate to 0.00015448998113859846[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2886959)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2886959)[0m Epoch: 5 cost time: 2.2632174491882324[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2886959)[0m Epoch: 5, Steps: 265 | Train Loss: 0.2922436 Vali Loss: 0.2374500 Best vali loss: 0.2234140[32m [repeated 6x across cluster][0m

Trial trial-c3dff_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00144 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00048 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2888146)[0m configuration
[36m(_train_fn pid=2888146)[0m {'batch_size': 128, 'learning_rate': 0.0004813052218989188, 'd_model': 512, 'd_core': 256, 'e_layers': 1, 'dropout': 0.0014397208676278574, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2888146)[0m Use GPU: cuda:0
[36m(_train_fn pid=2888146)[0m train 8449
[36m(_train_fn pid=2888146)[0m val 2785
[36m(_train_fn pid=2888146)[0m start_epoch 0
[36m(_train_fn pid=2888146)[0m max_epoch 8
[36m(_train_fn pid=2888146)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2155145 Best vali loss: 0.2155145
[36m(_train_fn pid=2886959)[0m EarlyStopping counter: 1 out of 3[32m [repeated 3x across cluster][0m

Trial trial-c3dff_00005 completed after 6 iterations at 2024-08-26 13:53:54. Total running time: 43s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             2.97582 â”‚
â”‚ time_total_s                                20.36872 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                                0.229 â”‚
â”‚ train_loss                                   0.35997 â”‚
â”‚ valid_loss                                   0.23485 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2888146)[0m Validation loss decreased (0.2155 --> 0.2153).  Saving model state dict ...
[36m(_train_fn pid=2888146)[0m Validation loss decreased (inf --> 0.2155).  Saving model state dict ...
[36m(_train_fn pid=2888146)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2152825 Best vali loss: 0.2152825
[36m(_train_fn pid=2888146)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2201677 Best vali loss: 0.2152825
[36m(_train_fn pid=2888146)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2290014 Best vali loss: 0.2152825

Trial trial-c3dff_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00213 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00174 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c3dff_00008 completed after 5 iterations at 2024-08-26 13:53:55. Total running time: 45s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             0.53144 â”‚
â”‚ time_total_s                                 3.39885 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21528 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.22345 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2888146)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2234498 Best vali loss: 0.2152825

Trial trial-c3dff_00006 completed after 7 iterations at 2024-08-26 13:53:55. Total running time: 45s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             2.07221 â”‚
â”‚ time_total_s                                18.59751 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.22341 â”‚
â”‚ train_loss                                   0.34211 â”‚
â”‚ valid_loss                                     0.243 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2886959)[0m Early stopping[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2888604)[0m Validation loss decreased (inf --> 0.2285).  Saving model state dict ...
[36m(_train_fn pid=2887024)[0m 	iters: 1000, epoch: 2 | loss: 1.1516600[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2887024)[0m 	speed: 0.0052s/iter; left time: 33.0517s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2888604)[0m Updating learning rate to 0.0014840948648057546[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2888604)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2888604)[0m Epoch: 2 cost time: 0.5302557945251465[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2887024)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.4412240 Vali Loss: 0.2558287 Best vali loss: 0.2558287[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2888604)[0m configuration
2024-08-26 13:53:58,427	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:53:59,141	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:02,421	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2887024)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4/trial-c3dff_00007_7_alpha_d_ff=4,batch_size=8,d_core=512,d_model=128,dropout=0.0017,e_layers=3,learning_rate=0.0010,lradj=type1_2024-08-26_13-53-36/checkpoint_000002)[32m [repeated 6x across cluster][0m
2024-08-26 13:54:07,171	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:12,016	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2887024)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4/trial-c3dff_00007_7_alpha_d_ff=4,batch_size=8,d_core=512,d_model=128,dropout=0.0017,e_layers=3,learning_rate=0.0010,lradj=type1_2024-08-26_13-53-36/checkpoint_000004)[32m [repeated 2x across cluster][0m
2024-08-26 13:54:16,446	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:21,394	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2887024)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4/trial-c3dff_00007_7_alpha_d_ff=4,batch_size=8,d_core=512,d_model=128,dropout=0.0017,e_layers=3,learning_rate=0.0010,lradj=type1_2024-08-26_13-53-36/checkpoint_000006)[32m [repeated 2x across cluster][0m
2024-08-26 13:54:25,839	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:25,849	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4' in 0.0083s.
[36m(_train_fn pid=2888604)[0m {'batch_size': 128, 'learning_rate': 0.0017387252879098923, 'd_model': 256, 'd_core': 32, 'e_layers': 3, 'dropout': 0.002131167164963301, 'lradj': 'cosine', 'd_ff': 768}
[36m(_train_fn pid=2888604)[0m Use GPU: cuda:0
[36m(_train_fn pid=2888604)[0m train 8449
[36m(_train_fn pid=2888604)[0m val 2785
[36m(_train_fn pid=2888604)[0m start_epoch 0
[36m(_train_fn pid=2888604)[0m max_epoch 8

Trial trial-c3dff_00009 completed after 4 iterations at 2024-08-26 13:53:59. Total running time: 48s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.71222 â”‚
â”‚ time_total_s                                 3.45333 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22847 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.24583 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2888604)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2458265 Best vali loss: 0.2284669[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2888604)[0m EarlyStopping counter: 3 out of 3[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2887024)[0m Validation loss decreased (0.2565 --> 0.2558).  Saving model state dict ...
[36m(_train_fn pid=2888604)[0m Early stopping
[36m(_train_fn pid=2887024)[0m Validation loss decreased (0.2558 --> 0.2410).  Saving model state dict ...
[36m(_train_fn pid=2887024)[0m 	iters: 1000, epoch: 3 | loss: 0.2660814[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2887024)[0m 	speed: 0.0035s/iter; left time: 18.4747s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2887024)[0m Updating learning rate to 0.00023947041175216376[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2887024)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2887024)[0m Epoch: 3 cost time: 4.5480451583862305[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2887024)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.3589572 Vali Loss: 0.2410493 Best vali loss: 0.2410493
[36m(_train_fn pid=2887024)[0m Validation loss decreased (0.2410 --> 0.2296).  Saving model state dict ...
[36m(_train_fn pid=2887024)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.2417537 Vali Loss: 0.2296027 Best vali loss: 0.2296027
[36m(_train_fn pid=2887024)[0m 	iters: 100, epoch: 5 | loss: 0.2805670[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2887024)[0m 	speed: 0.0123s/iter; left time: 50.7382s[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2887024)[0m Updating learning rate to 0.00011973520587608188
[36m(_train_fn pid=2887024)[0m saving checkpoint...
[36m(_train_fn pid=2887024)[0m Epoch: 4 cost time: 4.198911666870117

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2024-08-26 13:54:10. Total running time: 1min 0s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: c3dff_00008 with best_valid_loss=0.21528254512503503 and params={'batch_size': 128, 'learning_rate': 0.0004813052218989188, 'd_model': 512, 'd_core': 256, 'e_layers': 1, 'dropout': 0.0014397208676278574, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-c3dff_00007   RUNNING                 8       0.000957882         128              4        512            3   0.00165229   type1          4           29.563         0.241754       0.229603            0.229603 â”‚
â”‚ trial-c3dff_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         7           19.8992        0.213533       0.223586            0.215681 â”‚
â”‚ trial-c3dff_00001   TERMINATED             64       0.000526775         512              4        512            4   0.00414377   cosine         6           18.1264        0.202304       0.242669            0.221941 â”‚
â”‚ trial-c3dff_00002   TERMINATED             16       0.00200804           64              3         32            3   0.00348991   type1          6           38.4818        0.310851       0.230479            0.221103 â”‚
â”‚ trial-c3dff_00003   TERMINATED             16       0.000450067         512              4        128            1   0.0114959    type1          5           23.4629        0.185275       0.235752            0.219878 â”‚
â”‚ trial-c3dff_00004   TERMINATED            128       0.00433347           64              3        512            3   0.00416059   cosine         7            4.05424     nan              0.240615            0.233686 â”‚
â”‚ trial-c3dff_00005   TERMINATED             16       0.00384218          256              1        256            1   0.00308055   cosine         6           20.3687        0.359975       0.234855            0.229    â”‚
â”‚ trial-c3dff_00006   TERMINATED             32       0.000500521         128              2        512            4   0.00146644   cosine         7           18.5975        0.342108       0.243002            0.223414 â”‚
â”‚ trial-c3dff_00008   TERMINATED            128       0.000481305         512              2        256            1   0.00143972   cosine         5            3.39885     nan              0.22345             0.215283 â”‚
â”‚ trial-c3dff_00009   TERMINATED            128       0.00173873          256              3         32            3   0.00213117   cosine         4            3.45333     nan              0.245827            0.228467 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2887024)[0m Updating learning rate to 5.986760293804094e-05
[36m(_train_fn pid=2887024)[0m saving checkpoint...
[36m(_train_fn pid=2887024)[0m Validation loss decreased (0.2296 --> 0.2295).  Saving model state dict ...
[36m(_train_fn pid=2887024)[0m Epoch: 5 cost time: 4.259182929992676
[36m(_train_fn pid=2887024)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.3539052 Vali Loss: 0.2295409 Best vali loss: 0.2295409
[36m(_train_fn pid=2887024)[0m 	iters: 200, epoch: 6 | loss: 0.2226199[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2887024)[0m 	speed: 0.0036s/iter; left time: 10.6251s[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2887024)[0m Updating learning rate to 2.993380146902047e-05
[36m(_train_fn pid=2887024)[0m saving checkpoint...
[36m(_train_fn pid=2887024)[0m Validation loss decreased (0.2295 --> 0.2294).  Saving model state dict ...
[36m(_train_fn pid=2887024)[0m Epoch: 6 cost time: 3.851024866104126
[36m(_train_fn pid=2887024)[0m Epoch: 6, Steps: 1057 | Train Loss: 0.3821088 Vali Loss: 0.2294370 Best vali loss: 0.2294370
[36m(_train_fn pid=2887024)[0m 	iters: 400, epoch: 7 | loss: 0.5590488[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2887024)[0m 	speed: 0.0041s/iter; left time: 7.0820s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2887024)[0m Updating learning rate to 1.4966900734510235e-05
[36m(_train_fn pid=2887024)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2887024)[0m saving checkpoint...
[36m(_train_fn pid=2887024)[0m Epoch: 7 cost time: 4.378757476806641
[36m(_train_fn pid=2887024)[0m Epoch: 7, Steps: 1057 | Train Loss: 0.4026280 Vali Loss: 0.2301383 Best vali loss: 0.2294370
[36m(_train_fn pid=2887024)[0m 	iters: 500, epoch: 8 | loss: 0.2105439[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2887024)[0m 	speed: 0.0037s/iter; left time: 2.0514s[32m [repeated 11x across cluster][0m

Trial trial-c3dff_00007 completed after 8 iterations at 2024-08-26 13:54:25. Total running time: 1min 15s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c3dff_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                              4.4422 â”‚
â”‚ time_total_s                                48.21943 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22944 â”‚
â”‚ train_loss                                   0.37047 â”‚
â”‚ valid_loss                                    0.2297 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:54:25. Total running time: 1min 15s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: c3dff_00008 with best_valid_loss=0.21528254512503503 and params={'batch_size': 128, 'learning_rate': 0.0004813052218989188, 'd_model': 512, 'd_core': 256, 'e_layers': 1, 'dropout': 0.0014397208676278574, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-c3dff_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         7           19.8992        0.213533       0.223586            0.215681 â”‚
â”‚ trial-c3dff_00001   TERMINATED             64       0.000526775         512              4        512            4   0.00414377   cosine         6           18.1264        0.202304       0.242669            0.221941 â”‚
â”‚ trial-c3dff_00002   TERMINATED             16       0.00200804           64              3         32            3   0.00348991   type1          6           38.4818        0.310851       0.230479            0.221103 â”‚
â”‚ trial-c3dff_00003   TERMINATED             16       0.000450067         512              4        128            1   0.0114959    type1          5           23.4629        0.185275       0.235752            0.219878 â”‚
â”‚ trial-c3dff_00004   TERMINATED            128       0.00433347           64              3        512            3   0.00416059   cosine         7            4.05424     nan              0.240615            0.233686 â”‚
â”‚ trial-c3dff_00005   TERMINATED             16       0.00384218          256              1        256            1   0.00308055   cosine         6           20.3687        0.359975       0.234855            0.229    â”‚
â”‚ trial-c3dff_00006   TERMINATED             32       0.000500521         128              2        512            4   0.00146644   cosine         7           18.5975        0.342108       0.243002            0.223414 â”‚
â”‚ trial-c3dff_00007   TERMINATED              8       0.000957882         128              4        512            3   0.00165229   type1          8           48.2194        0.370468       0.229701            0.229437 â”‚
â”‚ trial-c3dff_00008   TERMINATED            128       0.000481305         512              2        256            1   0.00143972   cosine         5            3.39885     nan              0.22345             0.215283 â”‚
â”‚ trial-c3dff_00009   TERMINATED            128       0.00173873          256              3         32            3   0.00213117   cosine         4            3.45333     nan              0.245827            0.228467 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 128, 'learning_rate': 0.0004813052218989188, 'd_model': 512, 'd_core': 256, 'e_layers': 1, 'dropout': 0.0014397208676278574, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2887024)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed4/trial-c3dff_00007_7_alpha_d_ff=4,batch_size=8,d_core=512,d_model=128,dropout=0.0017,e_layers=3,learning_rate=0.0010,lradj=type1_2024-08-26_13-53-36/checkpoint_000007)
[36m(_train_fn pid=2887024)[0m Updating learning rate to 7.4834503672551175e-06
[36m(_train_fn pid=2887024)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2887024)[0m saving checkpoint...
[36m(_train_fn pid=2887024)[0m Epoch: 8 cost time: 3.8640880584716797
[36m(_train_fn pid=2887024)[0m Epoch: 8, Steps: 1057 | Train Loss: 0.3704678 Vali Loss: 0.2297014 Best vali loss: 0.2294370
[36m(_train_fn pid=2887024)[0m 	iters: 1000, epoch: 8 | loss: 0.2771733[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2887024)[0m 	speed: 0.0037s/iter; left time: 0.2127s[32m [repeated 5x across cluster][0m


Time taken (4 parallel trials): 80 seconds


2024-08-26 13:54:29,831	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:54:30,197	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:54:30,202	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:54:30,210	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:54:34,209	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed5   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator    â”‚
â”‚ Scheduler                        FIFOScheduler            â”‚
â”‚ Number of trials                 10                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed5
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-54-29_402991_2889820/artifacts/2024-08-26_13-54-30/ETTh2_96_96_test_seed5/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 13:54:30. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-f3419_00000   PENDING              32       0.0003              128              1         64            2   0             cosine  â”‚
â”‚ trial-f3419_00001   PENDING              64       0.00206467          512              2        128            3   0.000925801   cosine  â”‚
â”‚ trial-f3419_00002   PENDING              16       0.000328032          64              1         32            1   0.0119733     cosine  â”‚
â”‚ trial-f3419_00003   PENDING               8       0.000147228         256              2        128            2   0.0090958     type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-f3419_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.01197 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00033 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-f3419_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-f3419_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                             0.0091 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00015 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-f3419_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00093 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00206 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2892043)[0m configuration
[36m(_train_fn pid=2892043)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2892043)[0m Use GPU: cuda:0
[36m(_train_fn pid=2892046)[0m {'batch_size': 8, 'learning_rate': 0.00014722799836150368, 'd_model': 256, 'd_core': 128, 'e_layers': 2, 'dropout': 0.009095797337927954, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2892045)[0m train 8449
[36m(_train_fn pid=2892043)[0m val 2785
[36m(_train_fn pid=2892043)[0m start_epoch 0
[36m(_train_fn pid=2892043)[0m max_epoch 8
[36m(_train_fn pid=2892043)[0m 	iters: 100, epoch: 1 | loss: 0.6239124
[36m(_train_fn pid=2892043)[0m 	speed: 0.0131s/iter; left time: 26.5444s
[36m(_train_fn pid=2892044)[0m Updating learning rate to 0.0019860924592791985
[36m(_train_fn pid=2892044)[0m saving checkpoint...
[36m(_train_fn pid=2892044)[0m Validation loss decreased (inf --> 0.2677).  Saving model state dict ...
[36m(_train_fn pid=2892044)[0m Epoch: 1 cost time: 1.6610169410705566
[36m(_train_fn pid=2892044)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed5/trial-f3419_00001_1_alpha_d_ff=2,batch_size=64,d_core=128,d_model=512,dropout=0.0009,e_layers=3,learning_rate=0.0021,lradj=cosine_2024-08-26_13-54-30/checkpoint_000000)
2024-08-26 13:54:35,055	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:35,736	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:37,281	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:37,681	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:38,047	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:38,784	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:39,654	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2892043)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed5/trial-f3419_00000_0_2024-08-26_13-54-30/checkpoint_000002)[32m [repeated 7x across cluster][0m
2024-08-26 13:54:41,068	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:41,107	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:41,555	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:42,456	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:43,072	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:43,668	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2892044)[0m Epoch: 1, Steps: 133 | Train Loss: 0.4703184 Vali Loss: 0.2676811 Best vali loss: 0.2676811
[36m(_train_fn pid=2892044)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2892045)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2892045)[0m {'batch_size': 16, 'learning_rate': 0.00032803215358538, 'd_model': 64, 'd_core': 32, 'e_layers': 1, 'dropout': 0.011973256384061087, 'lradj': 'cosine', 'd_ff': 64}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2892045)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2892046)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2892045)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2892044)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2892046)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2892046)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2892043)[0m Validation loss decreased (0.2198 --> 0.2151).  Saving model state dict ...
[36m(_train_fn pid=2892043)[0m 	iters: 100, epoch: 3 | loss: 0.1945136[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2892043)[0m 	speed: 0.0183s/iter; left time: 27.2257s[32m [repeated 19x across cluster][0m

Trial trial-f3419_00001 completed after 4 iterations at 2024-08-26 13:54:38. Total running time: 8s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.50066 â”‚
â”‚ time_total_s                                 6.94982 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.26768 â”‚
â”‚ train_loss                                   0.73652 â”‚
â”‚ valid_loss                                   0.27215 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2892044)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2892044)[0m Early stopping
[36m(_train_fn pid=2892044)[0m Updating learning rate to 0.0010323372257548286[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2892044)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2892045)[0m Validation loss decreased (inf --> 0.2200).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2892044)[0m Epoch: 4 cost time: 1.2767555713653564[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2892044)[0m Epoch: 4, Steps: 133 | Train Loss: 0.7365151 Vali Loss: 0.2721490 Best vali loss: 0.2676811[32m [repeated 6x across cluster][0m

Trial trial-f3419_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00094 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                       0.0003 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2893086)[0m {'batch_size': 64, 'learning_rate': 0.000304634673374229, 'd_model': 512, 'd_core': 128, 'e_layers': 3, 'dropout': 0.000943190735461981, 'lradj': 'type1', 'd_ff': 1536}
[36m(_train_fn pid=2892043)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2892043)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2893086)[0m configuration
[36m(_train_fn pid=2893086)[0m Use GPU: cuda:0
[36m(_train_fn pid=2893086)[0m train 8449
[36m(_train_fn pid=2893086)[0m val 2785
[36m(_train_fn pid=2893086)[0m start_epoch 0
[36m(_train_fn pid=2893086)[0m max_epoch 8

Trial trial-f3419_00000 completed after 5 iterations at 2024-08-26 13:54:42. Total running time: 12s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.38475 â”‚
â”‚ time_total_s                                10.64274 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21512 â”‚
â”‚ train_loss                                    0.3634 â”‚
â”‚ valid_loss                                   0.22267 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2892043)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2892045)[0m Validation loss decreased (0.2200 --> 0.2173).  Saving model state dict ...
[36m(_train_fn pid=2892045)[0m Validation loss decreased (0.2173 --> 0.2158).  Saving model state dict ...
[36m(_train_fn pid=2892046)[0m 	iters: 500, epoch: 2 | loss: 0.2720087[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2892046)[0m 	speed: 0.0035s/iter; left time: 23.8791s[32m [repeated 25x across cluster][0m

Trial trial-f3419_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00503 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00017 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2892043)[0m Early stopping
[36m(_train_fn pid=2893535)[0m configuration
[36m(_train_fn pid=2893535)[0m {'batch_size': 128, 'learning_rate': 0.00017453125573251818, 'd_model': 128, 'd_core': 512, 'e_layers': 1, 'dropout': 0.005033492794961964, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2893535)[0m Use GPU: cuda:0
[36m(_train_fn pid=2893535)[0m train 8449
[36m(_train_fn pid=2893535)[0m val 2785
[36m(_train_fn pid=2892045)[0m Updating learning rate to 0.00022678231202277277[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2892045)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
2024-08-26 13:54:45,059	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2893086)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed5/trial-f3419_00004_4_alpha_d_ff=3,batch_size=64,d_core=128,d_model=512,dropout=0.0009,e_layers=3,learning_rate=0.0003,lradj=type1_2024-08-26_13-54-38/checkpoint_000001)[32m [repeated 7x across cluster][0m
2024-08-26 13:54:45,118	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:45,538	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:45,942	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:46,135	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:46,331	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:46,704	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:46,990	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:47,053	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:47,100	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:47,497	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:47,886	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:48,502	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:49,025	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:51,203	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2893086)[0m Validation loss decreased (inf --> 0.2176).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2892045)[0m Epoch: 3 cost time: 2.114067792892456[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2892045)[0m Epoch: 3, Steps: 529 | Train Loss: 0.4655858 Vali Loss: 0.2157954 Best vali loss: 0.2157954[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2893535)[0m start_epoch 0
[36m(_train_fn pid=2893535)[0m max_epoch 8
[36m(_train_fn pid=2893535)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2364557 Best vali loss: 0.2364557
[36m(_train_fn pid=2893535)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2234141 Best vali loss: 0.2234141
[36m(_train_fn pid=2893535)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2189664 Best vali loss: 0.2189664
[36m(_train_fn pid=2892045)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2893535)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2175962 Best vali loss: 0.2175962
[36m(_train_fn pid=2893535)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2172894 Best vali loss: 0.2172894
[36m(_train_fn pid=2893535)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2173375 Best vali loss: 0.2172894
[36m(_train_fn pid=2893535)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2173829 Best vali loss: 0.2172894

Trial trial-f3419_00005 completed after 8 iterations at 2024-08-26 13:54:47. Total running time: 17s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                              0.3863 â”‚
â”‚ time_total_s                                 3.83626 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21729 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.21747 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2893535)[0m Epoch: 8, Steps: 67 | Train Loss: nan Vali Loss: 0.2174669 Best vali loss: 0.2172894
[36m(_train_fn pid=2893535)[0m Early stopping
[36m(_train_fn pid=2893535)[0m Validation loss decreased (0.2176 --> 0.2173).  Saving model state dict ...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2892046)[0m 	iters: 400, epoch: 3 | loss: 0.1210273[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2892046)[0m 	speed: 0.0036s/iter; left time: 21.2850s[32m [repeated 22x across cluster][0m

Trial trial-f3419_00004 completed after 4 iterations at 2024-08-26 13:54:49. Total running time: 18s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.96856 â”‚
â”‚ time_total_s                                  8.6904 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21762 â”‚
â”‚ train_loss                                   0.21504 â”‚
â”‚ valid_loss                                   0.22769 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-f3419_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00084 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00014 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2893086)[0m Updating learning rate to 3.807933417177863e-05[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2893086)[0m saving checkpoint...[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2893535)[0m Validation loss decreased (inf --> 0.2365).  Saving model state dict ...
[36m(_train_fn pid=2893086)[0m Epoch: 4 cost time: 1.7779600620269775[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2893086)[0m Epoch: 4, Steps: 133 | Train Loss: 0.2150412 Vali Loss: 0.2276941 Best vali loss: 0.2176245[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2894643)[0m configuration
[36m(_train_fn pid=2894643)[0m {'batch_size': 8, 'learning_rate': 0.00014243183013406004, 'd_model': 512, 'd_core': 512, 'e_layers': 4, 'dropout': 0.000841139300251799, 'lradj': 'type1', 'd_ff': 1536}
[36m(_train_fn pid=2894643)[0m Use GPU: cuda:0
[36m(_train_fn pid=2894643)[0m train 8449
[36m(_train_fn pid=2894643)[0m val 2785
[36m(_train_fn pid=2894643)[0m start_epoch 0
[36m(_train_fn pid=2894643)[0m max_epoch 8

Trial trial-f3419_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00139 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00005 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2894812)[0m {'batch_size': 64, 'learning_rate': 5.0277001832294136e-05, 'd_model': 256, 'd_core': 128, 'e_layers': 2, 'dropout': 0.0013871833611300358, 'lradj': 'cosine', 'd_ff': 256}

Trial trial-f3419_00002 completed after 6 iterations at 2024-08-26 13:54:51. Total running time: 20s
[36m(_train_fn pid=2892045)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed5/trial-f3419_00002_2_alpha_d_ff=1,batch_size=16,d_core=32,d_model=64,dropout=0.0120,e_layers=1,learning_rate=0.0003,lradj=cosine_2024-08-26_13-54-30/checkpoint_000005)[32m [repeated 14x across cluster][0m
2024-08-26 13:54:52,260	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:53,197	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:53,663	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:54,150	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:54,176	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:54,949	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:55,160	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:55,818	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:56,170	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:56,667	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2895014)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed5/trial-f3419_00008_8_alpha_d_ff=1,batch_size=32,d_core=64,d_model=64,dropout=0.0053,e_layers=1,learning_rate=0.0020,lradj=cosine_2024-08-26_13-54-51/checkpoint_000003)[32m [repeated 10x across cluster][0m
2024-08-26 13:54:57,179	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:58,153	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:54:59,170	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             2.69834 â”‚
â”‚ time_total_s                                19.41012 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                               0.2158 â”‚
â”‚ train_loss                                   0.38517 â”‚
â”‚ valid_loss                                    0.2168 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2892045)[0m EarlyStopping counter: 3 out of 3[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2894812)[0m Validation loss decreased (inf --> 0.2346).  Saving model state dict ...

Trial trial-f3419_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                             0.0053 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00205 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2892045)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2894812)[0m Validation loss decreased (0.2346 --> 0.2236).  Saving model state dict ...
[36m(_train_fn pid=2894812)[0m 	iters: 100, epoch: 3 | loss: 0.5575556[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2894812)[0m 	speed: 0.0095s/iter; left time: 6.6692s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2894812)[0m Validation loss decreased (0.2236 --> 0.2193).  Saving model state dict ...
[36m(_train_fn pid=2894812)[0m Updating learning rate to 3.4758588731251184e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2894812)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2895014)[0m Epoch: 1 cost time: 0.8678421974182129[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2895014)[0m Epoch: 1, Steps: 265 | Train Loss: 0.3853632 Vali Loss: 0.2201025 Best vali loss: 0.2201025[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2895014)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2895014)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2895014)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2895014)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2895014)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2895014)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2894812)[0m Validation loss decreased (0.2193 --> 0.2188).  Saving model state dict ...
[36m(_train_fn pid=2895014)[0m {'batch_size': 32, 'learning_rate': 0.002048333493161473, 'd_model': 64, 'd_core': 64, 'e_layers': 1, 'dropout': 0.0053040038807050304, 'lradj': 'cosine', 'd_ff': 64}
[36m(_train_fn pid=2894812)[0m Validation loss decreased (0.2188 --> 0.2171).  Saving model state dict ...
[36m(_train_fn pid=2895014)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m

Trial trial-f3419_00008 completed after 4 iterations at 2024-08-26 13:54:56. Total running time: 26s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.84693 â”‚
â”‚ time_total_s                                 3.94698 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                               0.2201 â”‚
â”‚ train_loss                                   0.33215 â”‚
â”‚ valid_loss                                   0.22954 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2895014)[0m Validation loss decreased (inf --> 0.2201).  Saving model state dict ...
[36m(_train_fn pid=2895014)[0m Early stopping

Trial trial-f3419_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00698 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00198 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2894812)[0m Validation loss decreased (0.2171 --> 0.2170).  Saving model state dict ...
[36m(_train_fn pid=2895842)[0m {'batch_size': 32, 'learning_rate': 0.0019808381250815974, 'd_model': 128, 'd_core': 64, 'e_layers': 1, 'dropout': 0.006979451695785885, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=2894643)[0m 	iters: 900, epoch: 1 | loss: 0.9608303[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=2894643)[0m 	speed: 0.0102s/iter; left time: 76.9246s[32m [repeated 24x across cluster][0m

Trial trial-f3419_00007 completed after 8 iterations at 2024-08-26 13:54:59. Total running time: 28s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.01481 â”‚
â”‚ time_total_s                                 8.60267 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21693 â”‚
â”‚ train_loss                                   0.45529 â”‚
â”‚ valid_loss                                   0.21693 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2894812)[0m Validation loss decreased (0.2170 --> 0.2169).  Saving model state dict ...
2024-08-26 13:55:00,452	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:01,697	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:01,816	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2894643)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed5/trial-f3419_00006_6_alpha_d_ff=3,batch_size=8,d_core=512,d_model=512,dropout=0.0008,e_layers=4,learning_rate=0.0001,lradj=type1_2024-08-26_13-54-47/checkpoint_000000)[32m [repeated 6x across cluster][0m
2024-08-26 13:55:02,614	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:03,029	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:04,273	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:05,607	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2894812)[0m Updating learning rate to 0.0[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2894812)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2894812)[0m Epoch: 8 cost time: 0.8077056407928467[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2894812)[0m Epoch: 8, Steps: 133 | Train Loss: 0.4552863 Vali Loss: 0.2169320 Best vali loss: 0.2169320[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2895842)[0m configuration
[36m(_train_fn pid=2895842)[0m Use GPU: cuda:0
[36m(_train_fn pid=2895842)[0m train 8449
[36m(_train_fn pid=2895842)[0m val 2785
[36m(_train_fn pid=2895842)[0m start_epoch 0
[36m(_train_fn pid=2895842)[0m max_epoch 8

Trial status: 7 TERMINATED | 3 RUNNING
Current time: 2024-08-26 13:55:00. Total running time: 30s
Logical resource usage: 3.0/32 CPUs, 0.75/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: f3419_00000 with best_valid_loss=0.21512357929664624 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-f3419_00003   RUNNING                 8       0.000147228         256              2        128            2   0.0090958     type1          3           21.841         0.238611       0.228175            0.216829 â”‚
â”‚ trial-f3419_00006   RUNNING                 8       0.000142432         512              3        512            4   0.000841139   type1                                                                                 â”‚
â”‚ trial-f3419_00009   RUNNING                32       0.00198084          128              4         64            1   0.00697945    cosine                                                                                â”‚
â”‚ trial-f3419_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5           10.6427        0.363401       0.222672            0.215124 â”‚
â”‚ trial-f3419_00001   TERMINATED             64       0.00206467          512              2        128            3   0.000925801   cosine         4            6.94982       0.736515       0.272149            0.267681 â”‚
â”‚ trial-f3419_00002   TERMINATED             16       0.000328032          64              1         32            1   0.0119733     cosine         6           19.4101        0.385172       0.216805            0.215795 â”‚
â”‚ trial-f3419_00004   TERMINATED             64       0.000304635         512              3        128            3   0.000943191   type1          4            8.6904        0.215041       0.227694            0.217625 â”‚
â”‚ trial-f3419_00005   TERMINATED            128       0.000174531         128              2        512            1   0.00503349    cosine         8            3.83626     nan              0.217467            0.217289 â”‚
â”‚ trial-f3419_00007   TERMINATED             64       5.0277e-05          256              1        128            2   0.00138718    cosine         8            8.60267       0.455286       0.216932            0.216932 â”‚
â”‚ trial-f3419_00008   TERMINATED             32       0.00204833           64              1         64            1   0.005304      cosine         4            3.94698       0.332149       0.229542            0.220103 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2895842)[0m Validation loss decreased (inf --> 0.2270).  Saving model state dict ...
[36m(_train_fn pid=2894812)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m

Trial trial-f3419_00003 completed after 4 iterations at 2024-08-26 13:55:02. Total running time: 32s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             8.94792 â”‚
â”‚ time_total_s                                30.78892 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21683 â”‚
â”‚ train_loss                                   0.23087 â”‚
â”‚ valid_loss                                   0.22693 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2892046)[0m Early stopping
[36m(_train_fn pid=2894643)[0m 	iters: 300, epoch: 2 | loss: 0.1936596[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2894643)[0m 	speed: 0.0069s/iter; left time: 48.6483s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2895842)[0m Validation loss decreased (0.2270 --> 0.2247).  Saving model state dict ...
[36m(_train_fn pid=2895842)[0m Updating learning rate to 0.0009904190625407987[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2895842)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2895842)[0m Epoch: 4 cost time: 0.9742496013641357[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2895842)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4123106 Vali Loss: 0.2375728 Best vali loss: 0.2247350[32m [repeated 6x across cluster][0m

Trial trial-f3419_00009 completed after 5 iterations at 2024-08-26 13:55:05. Total running time: 35s
2024-08-26 13:55:08,850	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2894643)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed5/trial-f3419_00006_6_alpha_d_ff=3,batch_size=8,d_core=512,d_model=512,dropout=0.0008,e_layers=4,learning_rate=0.0001,lradj=type1_2024-08-26_13-54-47/checkpoint_000001)[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2894643)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed5/trial-f3419_00006_6_alpha_d_ff=3,batch_size=8,d_core=512,d_model=512,dropout=0.0008,e_layers=4,learning_rate=0.0001,lradj=type1_2024-08-26_13-54-47/checkpoint_000002)
[36m(_train_fn pid=2894643)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed5/trial-f3419_00006_6_alpha_d_ff=3,batch_size=8,d_core=512,d_model=512,dropout=0.0008,e_layers=4,learning_rate=0.0001,lradj=type1_2024-08-26_13-54-47/checkpoint_000003)
2024-08-26 13:55:26,329	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed5' in 0.0084s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.33194 â”‚
â”‚ time_total_s                                 7.41395 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22474 â”‚
â”‚ train_loss                                   0.30707 â”‚
â”‚ valid_loss                                   0.24262 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2894643)[0m Validation loss decreased (inf --> 0.2236).  Saving model state dict ...
[36m(_train_fn pid=2895842)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2895842)[0m Early stopping
[36m(_train_fn pid=2894643)[0m Validation loss decreased (0.2236 --> 0.2226).  Saving model state dict ...
[36m(_train_fn pid=2894643)[0m 	iters: 100, epoch: 3 | loss: 0.4879127[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2894643)[0m 	speed: 0.0156s/iter; left time: 97.1438s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2894643)[0m Updating learning rate to 7.121591506703002e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2894643)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2894643)[0m Epoch: 2 cost time: 6.3225016593933105[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2894643)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.4030760 Vali Loss: 0.2226388 Best vali loss: 0.2226388[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2894643)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2894643)[0m 	iters: 1000, epoch: 3 | loss: 0.4723976[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2894643)[0m 	speed: 0.0051s/iter; left time: 27.3331s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2894643)[0m Updating learning rate to 3.560795753351501e-05
[36m(_train_fn pid=2894643)[0m saving checkpoint...
[36m(_train_fn pid=2894643)[0m Epoch: 3 cost time: 5.395996809005737
[36m(_train_fn pid=2894643)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.3841406 Vali Loss: 0.2323061 Best vali loss: 0.2226388
[36m(_train_fn pid=2894643)[0m Updating learning rate to 1.7803978766757505e-05
[36m(_train_fn pid=2894643)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2894643)[0m saving checkpoint...
[36m(_train_fn pid=2894643)[0m Epoch: 4 cost time: 4.979302883148193
[36m(_train_fn pid=2894643)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.2228606 Vali Loss: 0.2397893 Best vali loss: 0.2226388
[36m(_train_fn pid=2894643)[0m 	iters: 1000, epoch: 4 | loss: 0.2886281[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2894643)[0m 	speed: 0.0046s/iter; left time: 19.5078s[32m [repeated 10x across cluster][0m

Trial trial-f3419_00006 completed after 5 iterations at 2024-08-26 13:55:26. Total running time: 56s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3419_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.82258 â”‚
â”‚ time_total_s                                36.88083 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22264 â”‚
â”‚ train_loss                                   0.29544 â”‚
â”‚ valid_loss                                   0.24152 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:55:26. Total running time: 56s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: f3419_00000 with best_valid_loss=0.21512357929664624 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-f3419_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5           10.6427        0.363401       0.222672            0.215124 â”‚
â”‚ trial-f3419_00001   TERMINATED             64       0.00206467          512              2        128            3   0.000925801   cosine         4            6.94982       0.736515       0.272149            0.267681 â”‚
â”‚ trial-f3419_00002   TERMINATED             16       0.000328032          64              1         32            1   0.0119733     cosine         6           19.4101        0.385172       0.216805            0.215795 â”‚
â”‚ trial-f3419_00003   TERMINATED              8       0.000147228         256              2        128            2   0.0090958     type1          4           30.7889        0.230871       0.226926            0.216829 â”‚
â”‚ trial-f3419_00004   TERMINATED             64       0.000304635         512              3        128            3   0.000943191   type1          4            8.6904        0.215041       0.227694            0.217625 â”‚
â”‚ trial-f3419_00005   TERMINATED            128       0.000174531         128              2        512            1   0.00503349    cosine         8            3.83626     nan              0.217467            0.217289 â”‚
â”‚ trial-f3419_00006   TERMINATED              8       0.000142432         512              3        512            4   0.000841139   type1          5           36.8808        0.295439       0.241519            0.222639 â”‚
â”‚ trial-f3419_00007   TERMINATED             64       5.0277e-05          256              1        128            2   0.00138718    cosine         8            8.60267       0.455286       0.216932            0.216932 â”‚
â”‚ trial-f3419_00008   TERMINATED             32       0.00204833           64              1         64            1   0.005304      cosine         4            3.94698       0.332149       0.229542            0.220103 â”‚
â”‚ trial-f3419_00009   TERMINATED             32       0.00198084          128              4         64            1   0.00697945    cosine         5            7.41395       0.307073       0.242624            0.224735 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2894643)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed5/trial-f3419_00006_6_alpha_d_ff=3,batch_size=8,d_core=512,d_model=512,dropout=0.0008,e_layers=4,learning_rate=0.0001,lradj=type1_2024-08-26_13-54-47/checkpoint_000004)
[36m(_train_fn pid=2894643)[0m Updating learning rate to 8.901989383378752e-06
[36m(_train_fn pid=2894643)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2894643)[0m saving checkpoint...
[36m(_train_fn pid=2894643)[0m Epoch: 5 cost time: 5.195436477661133
[36m(_train_fn pid=2894643)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.2954391 Vali Loss: 0.2415193 Best vali loss: 0.2226388
[36m(_train_fn pid=2894643)[0m Early stopping
[36m(_train_fn pid=2894643)[0m 	iters: 1000, epoch: 5 | loss: 0.0897544[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2894643)[0m 	speed: 0.0050s/iter; left time: 16.1543s[32m [repeated 10x across cluster][0m


Time taken (4 parallel trials): 60 seconds


2024-08-26 13:55:30,304	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:55:30,672	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:55:30,677	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:55:30,684	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:55:34,613	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2899182)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6/trial-174cf_00003_3_alpha_d_ff=4,batch_size=64,d_core=32,d_model=32,dropout=0.0075,e_layers=4,learning_rate=0.0025,lradj=type1_2024-08-26_13-55-30/checkpoint_000000)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed6   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator    â”‚
â”‚ Scheduler                        FIFOScheduler            â”‚
â”‚ Number of trials                 10                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-55-29_875211_2896958/artifacts/2024-08-26_13-55-30/ETTh2_96_96_test_seed6/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 13:55:30. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-174cf_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-174cf_00001   PENDING              32       0.000242947         128              4         64            3   0.0115989    cosine  â”‚
â”‚ trial-174cf_00002   PENDING               8       0.00111588          256              4         64            3   0.00111613   type1   â”‚
â”‚ trial-174cf_00003   PENDING              64       0.00250815           32              4         32            4   0.0075114    type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-174cf_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                             0.0116 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00024 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-174cf_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-174cf_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00112 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00112 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2899179)[0m configuration
[36m(_train_fn pid=2899179)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2899179)[0m Use GPU: cuda:0
[36m(_train_fn pid=2899181)[0m {'batch_size': 8, 'learning_rate': 0.0011158809547558613, 'd_model': 256, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0011161349875484135, 'lradj': 'type1', 'd_ff': 1024}

Trial trial-174cf_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00751 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00251 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2899179)[0m train 8449
[36m(_train_fn pid=2899179)[0m val 2785
[36m(_train_fn pid=2899179)[0m start_epoch 0
[36m(_train_fn pid=2899179)[0m max_epoch 8
[36m(_train_fn pid=2899179)[0m 	iters: 100, epoch: 1 | loss: 0.8677460
[36m(_train_fn pid=2899179)[0m 	speed: 0.0121s/iter; left time: 24.5480s
[36m(_train_fn pid=2899182)[0m Updating learning rate to 0.0025081453337423725
[36m(_train_fn pid=2899182)[0m saving checkpoint...
[36m(_train_fn pid=2899182)[0m Validation loss decreased (inf --> 0.2366).  Saving model state dict ...
[36m(_train_fn pid=2899182)[0m Epoch: 1 cost time: 1.644986629486084
2024-08-26 13:55:34,982	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:35,470	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:36,121	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:36,965	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:37,633	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:37,913	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:38,960	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:39,152	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:40,461	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2899180)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6/trial-174cf_00001_1_alpha_d_ff=4,batch_size=32,d_core=64,d_model=128,dropout=0.0116,e_layers=3,learning_rate=0.0002,lradj=cosine_2024-08-26_13-55-30/checkpoint_000002)[32m [repeated 9x across cluster][0m
2024-08-26 13:55:40,638	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:40,899	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:42,174	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:42,713	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:42,794	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:43,604	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:44,410	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2899182)[0m Epoch: 1, Steps: 133 | Train Loss: 0.3100984 Vali Loss: 0.2366113 Best vali loss: 0.2366113
[36m(_train_fn pid=2899182)[0m Validation loss decreased (0.2366 --> 0.2353).  Saving model state dict ...
[36m(_train_fn pid=2899182)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2899180)[0m {'batch_size': 32, 'learning_rate': 0.00024294711376113956, 'd_model': 128, 'd_core': 64, 'e_layers': 3, 'dropout': 0.011598862731726244, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=2899182)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2899182)[0m {'batch_size': 64, 'learning_rate': 0.0025081453337423725, 'd_model': 32, 'd_core': 32, 'e_layers': 4, 'dropout': 0.007511402197503053, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2899182)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2899182)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2899182)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2899182)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2899180)[0m 	iters: 100, epoch: 3 | loss: 0.5336288[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2899180)[0m 	speed: 0.0169s/iter; left time: 25.2009s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2899179)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2899182)[0m Updating learning rate to 0.00031351816671779657[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2899182)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2899180)[0m Validation loss decreased (inf --> 0.2178).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2899182)[0m Epoch: 4 cost time: 1.3012125492095947[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2899182)[0m Epoch: 4, Steps: 133 | Train Loss: 0.2986045 Vali Loss: 0.2294514 Best vali loss: 0.2291050[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2899180)[0m Validation loss decreased (0.2178 --> 0.2160).  Saving model state dict ...[32m [repeated 3x across cluster][0m

Trial trial-174cf_00003 completed after 6 iterations at 2024-08-26 13:55:42. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                              1.5335 â”‚
â”‚ time_total_s                                 9.82457 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                               0.2291 â”‚
â”‚ train_loss                                   0.35877 â”‚
â”‚ valid_loss                                   0.22938 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2899182)[0m Early stopping

Trial trial-174cf_00000 completed after 5 iterations at 2024-08-26 13:55:42. Total running time: 12s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                              1.8916 â”‚
â”‚ time_total_s                                10.53293 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21638 â”‚
â”‚ train_loss                                   0.45291 â”‚
â”‚ valid_loss                                   0.22471 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-174cf_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00258 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2900663)[0m configuration
[36m(_train_fn pid=2900663)[0m {'batch_size': 64, 'learning_rate': 6.239315951508393e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0025807346600683527, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2900663)[0m Use GPU: cuda:0
[36m(_train_fn pid=2900663)[0m train 8449
[36m(_train_fn pid=2900663)[0m val 2785
[36m(_train_fn pid=2899180)[0m 	iters: 200, epoch: 5 | loss: 0.5545430[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2899180)[0m 	speed: 0.0051s/iter; left time: 4.4097s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2899179)[0m EarlyStopping counter: 3 out of 3[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2900663)[0m start_epoch 0
[36m(_train_fn pid=2900663)[0m max_epoch 8

Trial trial-174cf_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00626 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00009 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-174cf_00001 completed after 5 iterations at 2024-08-26 13:55:44. Total running time: 13s
2024-08-26 13:55:45,319	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:46,183	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2900663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6/trial-174cf_00004_4_alpha_d_ff=2,batch_size=64,d_core=64,d_model=256,dropout=0.0026,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_13-55-42/checkpoint_000001)[32m [repeated 9x across cluster][0m
2024-08-26 13:55:46,873	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:47,050	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:47,893	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:48,753	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:48,949	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:49,591	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:50,433	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:51,010	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:51,286	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2900663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6/trial-174cf_00004_4_alpha_d_ff=2,batch_size=64,d_core=64,d_model=256,dropout=0.0026,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_13-55-42/checkpoint_000007)[32m [repeated 9x across cluster][0m
2024-08-26 13:55:52,227	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:52,782	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.69407 â”‚
â”‚ time_total_s                                12.15208 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21601 â”‚
â”‚ train_loss                                   0.51197 â”‚
â”‚ valid_loss                                   0.22271 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2900870)[0m {'batch_size': 32, 'learning_rate': 8.780814488684396e-05, 'd_model': 256, 'd_core': 32, 'e_layers': 3, 'dropout': 0.006255931569349177, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2900663)[0m Updating learning rate to 6.239315951508393e-05[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2900663)[0m saving checkpoint...[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2900663)[0m Validation loss decreased (inf --> 0.2219).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2900663)[0m Epoch: 1 cost time: 0.968364953994751[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2900663)[0m Epoch: 1, Steps: 133 | Train Loss: 0.4030954 Vali Loss: 0.2219300 Best vali loss: 0.2219300[32m [repeated 9x across cluster][0m

Trial trial-174cf_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00263 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00008 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2900663)[0m Validation loss decreased (0.2219 --> 0.2179).  Saving model state dict ...
[36m(_train_fn pid=2900663)[0m Validation loss decreased (0.2179 --> 0.2173).  Saving model state dict ...
[36m(_train_fn pid=2899180)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2900663)[0m Validation loss decreased (0.2173 --> 0.2156).  Saving model state dict ...
[36m(_train_fn pid=2901082)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2901082)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2901082)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2901082)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2901082)[0m 	iters: 400, epoch: 1 | loss: 0.3784501[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2901082)[0m 	speed: 0.0051s/iter; left time: 41.4607s[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2900870)[0m EarlyStopping counter: 1 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2901082)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2901082)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2901082)[0m {'batch_size': 8, 'learning_rate': 8.179564124948514e-05, 'd_model': 128, 'd_core': 512, 'e_layers': 1, 'dropout': 0.002631283440464936, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2900663)[0m Validation loss decreased (0.2156 --> 0.2156).  Saving model state dict ...
[36m(_train_fn pid=2900663)[0m Validation loss decreased (0.2156 --> 0.2155).  Saving model state dict ...
[36m(_train_fn pid=2900663)[0m Updating learning rate to 9.748931174231864e-07[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2900663)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2900870)[0m Validation loss decreased (inf --> 0.2164).  Saving model state dict ...
[36m(_train_fn pid=2900663)[0m Epoch: 7 cost time: 0.6770915985107422[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2900663)[0m Epoch: 7, Steps: 133 | Train Loss: 0.2786132 Vali Loss: 0.2155158 Best vali loss: 0.2155158[32m [repeated 8x across cluster][0m

Trial trial-174cf_00004 completed after 8 iterations at 2024-08-26 13:55:51. Total running time: 20s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                              0.8494 â”‚
â”‚ time_total_s                                 7.52072 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21552 â”‚
â”‚ train_loss                                   0.39256 â”‚
â”‚ valid_loss                                   0.21554 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2899181)[0m Validation loss decreased (0.2743 --> 0.2703).  Saving model state dict ...

Trial trial-174cf_00005 completed after 4 iterations at 2024-08-26 13:55:52. Total running time: 22s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.76988 â”‚
â”‚ time_total_s                                 8.43915 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21643 â”‚
â”‚ train_loss                                   0.23257 â”‚
â”‚ valid_loss                                   0.21666 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-174cf_00007 started with configuration:
2024-08-26 13:55:53,269	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:55,852	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:56,694	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2902220)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6/trial-174cf_00008_8_alpha_d_ff=1,batch_size=64,d_core=64,d_model=128,dropout=0.0021,e_layers=4,learning_rate=0.0044,lradj=type1_2024-08-26_13-55-52/checkpoint_000001)[32m [repeated 5x across cluster][0m
2024-08-26 13:55:57,551	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:58,409	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:59,255	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:55:59,372	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:00,138	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:00,208	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00274 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00022 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2900870)[0m Early stopping
[36m(_train_fn pid=2902050)[0m {'batch_size': 8, 'learning_rate': 0.00021740474025212938, 'd_model': 128, 'd_core': 128, 'e_layers': 4, 'dropout': 0.0027364972500705273, 'lradj': 'type1', 'd_ff': 256}
[36m(_train_fn pid=2901082)[0m Validation loss decreased (inf --> 0.2207).  Saving model state dict ...
[36m(_train_fn pid=2902050)[0m configuration
[36m(_train_fn pid=2902050)[0m Use GPU: cuda:0
[36m(_train_fn pid=2902050)[0m train 8449
[36m(_train_fn pid=2902050)[0m val 2785
[36m(_train_fn pid=2899181)[0m 	iters: 300, epoch: 3 | loss: 0.2617508[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2899181)[0m 	speed: 0.0063s/iter; left time: 38.2701s[32m [repeated 19x across cluster][0m

Trial trial-174cf_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00213 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00436 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2902220)[0m configuration
[36m(_train_fn pid=2902220)[0m Use GPU: cuda:0
[36m(_train_fn pid=2900870)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2902050)[0m start_epoch 0
[36m(_train_fn pid=2902050)[0m max_epoch 8
[36m(_train_fn pid=2902220)[0m train 8449
[36m(_train_fn pid=2902220)[0m val 2785
[36m(_train_fn pid=2902220)[0m start_epoch 0
[36m(_train_fn pid=2902220)[0m max_epoch 8
[36m(_train_fn pid=2901082)[0m Updating learning rate to 7.86824800242602e-05[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2901082)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2901082)[0m Epoch: 1 cost time: 5.841032028198242[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2901082)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.4609514 Vali Loss: 0.2206960 Best vali loss: 0.2206960[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2902220)[0m Validation loss decreased (0.2460 --> 0.2439).  Saving model state dict ...
[36m(_train_fn pid=2902220)[0m Validation loss decreased (0.2439 --> 0.2303).  Saving model state dict ...
[36m(_train_fn pid=2902220)[0m {'batch_size': 64, 'learning_rate': 0.004364638795560977, 'd_model': 128, 'd_core': 64, 'e_layers': 4, 'dropout': 0.002131400693677419, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2902220)[0m Validation loss decreased (inf --> 0.2460).  Saving model state dict ...
[36m(_train_fn pid=2902050)[0m 	iters: 700, epoch: 1 | loss: 0.6270845[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=2902050)[0m 	speed: 0.0074s/iter; left time: 57.5976s[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=2902220)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m

Trial trial-174cf_00008 completed after 6 iterations at 2024-08-26 13:56:00. Total running time: 29s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             0.87975 â”‚
â”‚ time_total_s                                 5.82681 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.23034 â”‚
â”‚ train_loss                                   0.25795 â”‚
â”‚ valid_loss                                    0.2317 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2902220)[0m Early stopping
[36m(_train_fn pid=2902220)[0m Updating learning rate to 0.00013639496236128053[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2902220)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2902220)[0m Epoch: 6 cost time: 0.733234167098999[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2902220)[0m Epoch: 6, Steps: 133 | Train Loss: 0.2579461 Vali Loss: 0.2316984 Best vali loss: 0.2303430[32m [repeated 8x across cluster][0m

Trial status: 6 TERMINATED | 3 RUNNING | 1 PENDING
Current time: 2024-08-26 13:56:00. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 174cf_00004 with best_valid_loss=0.21551577203256653 and params={'batch_size': 64, 'learning_rate': 6.239315951508393e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0025807346600683527, 'lradj': 'type1', 'd_ff': 512}
2024-08-26 13:56:03,510	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2902964)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6/trial-174cf_00009_9_alpha_d_ff=2,batch_size=64,d_core=32,d_model=64,dropout=0.0075,e_layers=3,learning_rate=0.0029,lradj=type1_2024-08-26_13-56-00/checkpoint_000000)[32m [repeated 7x across cluster][0m
2024-08-26 13:56:04,171	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:04,627	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:05,732	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:05,884	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:07,003	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:08,196	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:09,312	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2902964)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6/trial-174cf_00009_9_alpha_d_ff=2,batch_size=64,d_core=32,d_model=64,dropout=0.0075,e_layers=3,learning_rate=0.0029,lradj=type1_2024-08-26_13-56-00/checkpoint_000005)[32m [repeated 7x across cluster][0m
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-174cf_00002   RUNNING                 8       0.00111588          256              4         64            3   0.00111613   type1          3           27.905         0.523197       0.272457            0.270302 â”‚
â”‚ trial-174cf_00006   RUNNING                 8       8.17956e-05         128              2        512            1   0.00263128   cosine         2           13.4234        0.498564       0.217214            0.217214 â”‚
â”‚ trial-174cf_00007   RUNNING                 8       0.000217405         128              2        128            4   0.0027365    type1                                                                                 â”‚
â”‚ trial-174cf_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5           10.5329        0.452909       0.22471             0.216383 â”‚
â”‚ trial-174cf_00001   TERMINATED             32       0.000242947         128              4         64            3   0.0115989    cosine         5           12.1521        0.511966       0.222715            0.216005 â”‚
â”‚ trial-174cf_00003   TERMINATED             64       0.00250815           32              4         32            4   0.0075114    type1          6            9.82457       0.358768       0.229378            0.229105 â”‚
â”‚ trial-174cf_00004   TERMINATED             64       6.23932e-05         256              2         64            3   0.00258073   type1          8            7.52072       0.392559       0.215536            0.215516 â”‚
â”‚ trial-174cf_00005   TERMINATED             32       8.78081e-05         256              4         32            3   0.00625593   cosine         4            8.43915       0.232573       0.216658            0.216434 â”‚
â”‚ trial-174cf_00008   TERMINATED             64       0.00436464          128              1         64            4   0.0021314    type1          6            5.82681       0.257946       0.231698            0.230343 â”‚
â”‚ trial-174cf_00009   PENDING                64       0.00289433           64              2         32            3   0.00752697   type1                                                                                 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-174cf_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00753 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00289 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2902964)[0m configuration
[36m(_train_fn pid=2902964)[0m {'batch_size': 64, 'learning_rate': 0.0028943323132122013, 'd_model': 64, 'd_core': 32, 'e_layers': 3, 'dropout': 0.007526971082674683, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2902964)[0m Use GPU: cuda:0
[36m(_train_fn pid=2902964)[0m train 8449
[36m(_train_fn pid=2902964)[0m val 2785
[36m(_train_fn pid=2902964)[0m start_epoch 0
[36m(_train_fn pid=2902964)[0m max_epoch 8
[36m(_train_fn pid=2901082)[0m Validation loss decreased (0.2207 --> 0.2172).  Saving model state dict ...
[36m(_train_fn pid=2902964)[0m Validation loss decreased (inf --> 0.2311).  Saving model state dict ...
[36m(_train_fn pid=2899181)[0m 	iters: 600, epoch: 4 | loss: 0.2743909[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2899181)[0m 	speed: 0.0079s/iter; left time: 37.0325s[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2902964)[0m EarlyStopping counter: 1 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2902964)[0m Validation loss decreased (0.2311 --> 0.2295).  Saving model state dict ...
[36m(_train_fn pid=2902964)[0m Updating learning rate to 0.0007235830783030503[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2902964)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2902964)[0m Epoch: 2 cost time: 0.9340360164642334[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2902964)[0m Epoch: 2, Steps: 133 | Train Loss: 0.2218755 Vali Loss: 0.2446050 Best vali loss: 0.2311185[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2902050)[0m Validation loss decreased (inf --> 0.2210).  Saving model state dict ...

Trial trial-174cf_00009 completed after 6 iterations at 2024-08-26 13:56:09. Total running time: 38s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.11256 â”‚
â”‚ time_total_s                                 7.64391 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22946 â”‚
â”‚ train_loss                                   0.26978 â”‚
â”‚ valid_loss                                    0.2313 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
2024-08-26 13:56:10,008	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:12,250	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:15,178	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2902050)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6/trial-174cf_00007_7_alpha_d_ff=2,batch_size=8,d_core=128,d_model=128,dropout=0.0027,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_13-55-51/checkpoint_000001)[32m [repeated 3x across cluster][0m
2024-08-26 13:56:17,969	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:17,987	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2902050)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6/trial-174cf_00007_7_alpha_d_ff=2,batch_size=8,d_core=128,d_model=128,dropout=0.0027,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_13-55-51/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 13:56:24,465	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2902964)[0m Early stopping
[36m(_train_fn pid=2901082)[0m 	iters: 700, epoch: 4 | loss: 1.0654323[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2901082)[0m 	speed: 0.0038s/iter; left time: 17.5166s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2902964)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2899181)[0m Validation loss decreased (0.2703 --> 0.2701).  Saving model state dict ...
[36m(_train_fn pid=2899181)[0m Updating learning rate to 0.00013948511934448266[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2899181)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2899181)[0m Epoch: 4 cost time: 8.325530767440796[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2899181)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.6064561 Vali Loss: 0.2700845 Best vali loss: 0.2700845[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2899181)[0m 	iters: 800, epoch: 5 | loss: 0.4786615[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2899181)[0m 	speed: 0.0061s/iter; left time: 20.9959s[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2902050)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2902050)[0m Updating learning rate to 0.00010870237012606469[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2902050)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2902050)[0m Epoch: 2 cost time: 9.533087968826294[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2902050)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.4529146 Vali Loss: 0.2210551 Best vali loss: 0.2209956[32m [repeated 2x across cluster][0m

Trial trial-174cf_00006 completed after 5 iterations at 2024-08-26 13:56:17. Total running time: 47s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.71653 â”‚
â”‚ time_total_s                                32.01218 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21721 â”‚
â”‚ train_loss                                   0.40347 â”‚
â”‚ valid_loss                                   0.21866 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2901082)[0m Early stopping
[36m(_train_fn pid=2899181)[0m 	iters: 400, epoch: 6 | loss: 0.5709863[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2899181)[0m 	speed: 0.0051s/iter; left time: 14.1690s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2901082)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2901082)[0m Updating learning rate to 2.5246902251814326e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2901082)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2901082)[0m Epoch: 5 cost time: 4.618155002593994[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2901082)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.4034658 Vali Loss: 0.2186576 Best vali loss: 0.2172135[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2902050)[0m Validation loss decreased (0.2210 --> 0.2206).  Saving model state dict ...
[36m(_train_fn pid=2899181)[0m 	iters: 200, epoch: 7 | loss: 0.2073707[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2899181)[0m 	speed: 0.0056s/iter; left time: 10.7785s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2899181)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2899181)[0m Updating learning rate to 3.4871279836120664e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2899181)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2899181)[0m Epoch: 6 cost time: 5.58657169342041[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2899181)[0m Epoch: 6, Steps: 1057 | Train Loss: 0.4497692 Vali Loss: 0.2718273 Best vali loss: 0.2700845[32m [repeated 2x across cluster][0m

Trial status: 8 TERMINATED | 2 RUNNING
Current time: 2024-08-26 13:56:30. Total running time: 1min 0s
Logical resource usage: 2.0/32 CPUs, 0.5/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 174cf_00004 with best_valid_loss=0.21551577203256653 and params={'batch_size': 64, 'learning_rate': 6.239315951508393e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0025807346600683527, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2899181)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6/trial-174cf_00002_2_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0011,e_layers=3,learning_rate=0.0011,lradj=type1_2024-08-26_13-55-30/checkpoint_000006)[32m [repeated 2x across cluster][0m
2024-08-26 13:56:31,356	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2902050)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6/trial-174cf_00007_7_alpha_d_ff=2,batch_size=8,d_core=128,d_model=128,dropout=0.0027,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_13-55-51/checkpoint_000004)[32m [repeated 2x across cluster][0m
2024-08-26 13:56:42,595	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6' in 0.0086s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-174cf_00002   RUNNING                 8       0.00111588          256              4         64            3   0.00111613   type1          6           52.1526        0.449769       0.271827            0.270085 â”‚
â”‚ trial-174cf_00007   RUNNING                 8       0.000217405         128              2        128            4   0.0027365    type1          3           31.0653        0.382382       0.220579            0.220579 â”‚
â”‚ trial-174cf_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5           10.5329        0.452909       0.22471             0.216383 â”‚
â”‚ trial-174cf_00001   TERMINATED             32       0.000242947         128              4         64            3   0.0115989    cosine         5           12.1521        0.511966       0.222715            0.216005 â”‚
â”‚ trial-174cf_00003   TERMINATED             64       0.00250815           32              4         32            4   0.0075114    type1          6            9.82457       0.358768       0.229378            0.229105 â”‚
â”‚ trial-174cf_00004   TERMINATED             64       6.23932e-05         256              2         64            3   0.00258073   type1          8            7.52072       0.392559       0.215536            0.215516 â”‚
â”‚ trial-174cf_00005   TERMINATED             32       8.78081e-05         256              4         32            3   0.00625593   cosine         4            8.43915       0.232573       0.216658            0.216434 â”‚
â”‚ trial-174cf_00006   TERMINATED              8       8.17956e-05         128              2        512            1   0.00263128   cosine         5           32.0122        0.403466       0.218658            0.217214 â”‚
â”‚ trial-174cf_00008   TERMINATED             64       0.00436464          128              1         64            4   0.0021314    type1          6            5.82681       0.257946       0.231698            0.230343 â”‚
â”‚ trial-174cf_00009   TERMINATED             64       0.00289433           64              2         32            3   0.00752697   type1          6            7.64391       0.269785       0.231301            0.229465 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-174cf_00002 completed after 7 iterations at 2024-08-26 13:56:30. Total running time: 1min 0s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             6.51582 â”‚
â”‚ time_total_s                                58.66846 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.27008 â”‚
â”‚ train_loss                                   0.47028 â”‚
â”‚ valid_loss                                   0.27128 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2899181)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2899181)[0m Early stopping
[36m(_train_fn pid=2902050)[0m 	iters: 1000, epoch: 4 | loss: 0.3198650[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2902050)[0m 	speed: 0.0063s/iter; left time: 26.8737s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2902050)[0m Updating learning rate to 2.7175592531516172e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2902050)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2902050)[0m Epoch: 4 cost time: 6.675044059753418[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2902050)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3041502 Vali Loss: 0.2221540 Best vali loss: 0.2205794[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2902050)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2902050)[0m 	iters: 1000, epoch: 5 | loss: 0.2716338[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2902050)[0m 	speed: 0.0043s/iter; left time: 13.9517s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2902050)[0m Updating learning rate to 1.3587796265758086e-05
[36m(_train_fn pid=2902050)[0m saving checkpoint...
[36m(_train_fn pid=2902050)[0m Epoch: 5 cost time: 4.725770473480225
[36m(_train_fn pid=2902050)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.3862146 Vali Loss: 0.2261215 Best vali loss: 0.2205794

Trial trial-174cf_00007 completed after 6 iterations at 2024-08-26 13:56:42. Total running time: 1min 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-174cf_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             5.85144 â”‚
â”‚ time_total_s                                 49.7226 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22058 â”‚
â”‚ train_loss                                   0.24951 â”‚
â”‚ valid_loss                                   0.22462 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
[36m(_train_fn pid=2902050)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed6/trial-174cf_00007_7_alpha_d_ff=2,batch_size=8,d_core=128,d_model=128,dropout=0.0027,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_13-55-51/checkpoint_000005)
Current time: 2024-08-26 13:56:42. Total running time: 1min 11s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 174cf_00004 with best_valid_loss=0.21551577203256653 and params={'batch_size': 64, 'learning_rate': 6.239315951508393e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0025807346600683527, 'lradj': 'type1', 'd_ff': 512}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-174cf_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5           10.5329        0.452909       0.22471             0.216383 â”‚
â”‚ trial-174cf_00001   TERMINATED             32       0.000242947         128              4         64            3   0.0115989    cosine         5           12.1521        0.511966       0.222715            0.216005 â”‚
â”‚ trial-174cf_00002   TERMINATED              8       0.00111588          256              4         64            3   0.00111613   type1          7           58.6685        0.47028        0.271284            0.270085 â”‚
â”‚ trial-174cf_00003   TERMINATED             64       0.00250815           32              4         32            4   0.0075114    type1          6            9.82457       0.358768       0.229378            0.229105 â”‚
â”‚ trial-174cf_00004   TERMINATED             64       6.23932e-05         256              2         64            3   0.00258073   type1          8            7.52072       0.392559       0.215536            0.215516 â”‚
â”‚ trial-174cf_00005   TERMINATED             32       8.78081e-05         256              4         32            3   0.00625593   cosine         4            8.43915       0.232573       0.216658            0.216434 â”‚
â”‚ trial-174cf_00006   TERMINATED              8       8.17956e-05         128              2        512            1   0.00263128   cosine         5           32.0122        0.403466       0.218658            0.217214 â”‚
â”‚ trial-174cf_00007   TERMINATED              8       0.000217405         128              2        128            4   0.0027365    type1          6           49.7226        0.249505       0.224625            0.220579 â”‚
â”‚ trial-174cf_00008   TERMINATED             64       0.00436464          128              1         64            4   0.0021314    type1          6            5.82681       0.257946       0.231698            0.230343 â”‚
â”‚ trial-174cf_00009   TERMINATED             64       0.00289433           64              2         32            3   0.00752697   type1          6            7.64391       0.269785       0.231301            0.229465 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

[36m(_train_fn pid=2902050)[0m Updating learning rate to 6.793898132879043e-06
[36m(_train_fn pid=2902050)[0m saving checkpoint...
[36m(_train_fn pid=2902050)[0m Epoch: 6 cost time: 5.216513156890869
[36m(_train_fn pid=2902050)[0m Epoch: 6, Steps: 1057 | Train Loss: 0.2495050 Vali Loss: 0.2246245 Best vali loss: 0.2205794
[36m(_train_fn pid=2902050)[0m Early stopping
[36m(_train_fn pid=2902050)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2902050)[0m 	iters: 1000, epoch: 6 | loss: 0.2678853[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2902050)[0m 	speed: 0.0049s/iter; left time: 10.5397s[32m [repeated 10x across cluster][0m
Best hyperparameters found were:  {'batch_size': 64, 'learning_rate': 6.239315951508393e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0025807346600683527, 'lradj': 'type1', 'd_ff': 512}


Time taken (4 parallel trials): 76 seconds


2024-08-26 13:56:46,565	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:56:46,935	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:56:46,940	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:56:46,947	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:56:50,595	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:50,623	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed7   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator    â”‚
â”‚ Scheduler                        FIFOScheduler            â”‚
â”‚ Number of trials                 10                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed7
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-56-46_137116_2904511/artifacts/2024-08-26_13-56-46/ETTh2_96_96_test_seed7/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 13:56:47. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-44c1a_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-44c1a_00001   PENDING             128       0.00311457          256              3        256            4   0.00180349   cosine  â”‚
â”‚ trial-44c1a_00002   PENDING              16       5.12272e-05         512              2        512            1   0.00284069   type1   â”‚
â”‚ trial-44c1a_00003   PENDING             128       0.000180228          64              3         64            4   0.00313679   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-44c1a_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00284 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00005 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-44c1a_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-44c1a_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                             0.0018 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00311 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-44c1a_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00314 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00018 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2906734)[0m configuration
[36m(_train_fn pid=2906734)[0m {'batch_size': 16, 'learning_rate': 5.1227199387913775e-05, 'd_model': 512, 'd_core': 512, 'e_layers': 1, 'dropout': 0.002840692305013047, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2906734)[0m Use GPU: cuda:0
[36m(_train_fn pid=2906735)[0m {'batch_size': 128, 'learning_rate': 0.00018022757056741147, 'd_model': 64, 'd_core': 64, 'e_layers': 4, 'dropout': 0.0031367852282998495, 'lradj': 'cosine', 'd_ff': 192}
[36m(_train_fn pid=2906734)[0m train 8449
[36m(_train_fn pid=2906734)[0m val 2785
[36m(_train_fn pid=2906734)[0m start_epoch 0
[36m(_train_fn pid=2906734)[0m max_epoch 8
[36m(_train_fn pid=2906734)[0m 	iters: 100, epoch: 1 | loss: 0.8631141
[36m(_train_fn pid=2906734)[0m 	speed: 0.0155s/iter; left time: 64.0587s
[36m(_train_fn pid=2906735)[0m Updating learning rate to 0.00017336806710443827
[36m(_train_fn pid=2906735)[0m saving checkpoint...
[36m(_train_fn pid=2906735)[0m Validation loss decreased (inf --> 0.2430).  Saving model state dict ...
[36m(_train_fn pid=2906735)[0m Epoch: 1 cost time: 1.3996899127960205
[36m(_train_fn pid=2906735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed7/trial-44c1a_00003_3_alpha_d_ff=3,batch_size=128,d_core=64,d_model=64,dropout=0.0031,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-56-46/checkpoint_000000)
2024-08-26 13:56:51,844	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:51,942	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:52,122	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:53,093	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:53,260	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:54,306	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:54,514	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:54,779	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:54,796	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:54,992	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:55,834	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2906735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed7/trial-44c1a_00003_3_alpha_d_ff=3,batch_size=128,d_core=64,d_model=64,dropout=0.0031,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-56-46/checkpoint_000005)[32m [repeated 12x across cluster][0m
2024-08-26 13:56:56,396	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:56,570	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:57,349	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:57,606	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:57,760	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:58,174	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2906735)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2430080 Best vali loss: 0.2430080
[36m(_train_fn pid=2906735)[0m Validation loss decreased (0.2430 --> 0.2280).  Saving model state dict ...
[36m(_train_fn pid=2906733)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2906732)[0m Epoch: 1, Steps: 265 | Train Loss: 0.3099937 Vali Loss: 0.2186950 Best vali loss: 0.2186950
[36m(_train_fn pid=2906735)[0m Validation loss decreased (0.2280 --> 0.2243).  Saving model state dict ...
[36m(_train_fn pid=2906733)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2906732)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2906732)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2906732)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2906732)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2906732)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2906735)[0m Validation loss decreased (0.2243 --> 0.2209).  Saving model state dict ...
[36m(_train_fn pid=2906732)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2906732)[0m max_epoch 8[32m [repeated 3x across cluster][0m

Trial trial-44c1a_00001 completed after 4 iterations at 2024-08-26 13:56:54. Total running time: 7s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.24769 â”‚
â”‚ time_total_s                                  5.9808 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.25667 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                    0.2721 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2906733)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2906733)[0m Early stopping
[36m(_train_fn pid=2906735)[0m Validation loss decreased (0.2209 --> 0.2191).  Saving model state dict ...
[36m(_train_fn pid=2906734)[0m 	iters: 100, epoch: 2 | loss: 0.4186288[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2906734)[0m 	speed: 0.0176s/iter; left time: 63.3827s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2906735)[0m Updating learning rate to 5.5628732627926516e-05[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2906735)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2906734)[0m Validation loss decreased (inf --> 0.2166).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2906735)[0m Epoch: 5 cost time: 0.543107271194458[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2906735)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2191245 Best vali loss: 0.2191245[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2906735)[0m Validation loss decreased (0.2191 --> 0.2187).  Saving model state dict ...

Trial trial-44c1a_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.01165 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                       0.0049 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2907982)[0m {'batch_size': 32, 'learning_rate': 0.004897456319510861, 'd_model': 128, 'd_core': 256, 'e_layers': 3, 'dropout': 0.011646552057741247, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2906735)[0m EarlyStopping counter: 1 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2906732)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3424784 Vali Loss: 0.2264597 Best vali loss: 0.2186950[32m [repeated 3x across cluster][0m

Trial trial-44c1a_00003 completed after 8 iterations at 2024-08-26 13:56:57. Total running time: 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.77712 â”‚
â”‚ time_total_s                                 8.79663 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21865 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.21873 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2906734)[0m Validation loss decreased (0.2166 --> 0.2164).  Saving model state dict ...

Trial trial-44c1a_00000 completed after 4 iterations at 2024-08-26 13:56:57. Total running time: 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.36217 â”‚
â”‚ time_total_s                                 9.23565 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                               0.2187 â”‚
â”‚ train_loss                                   0.34679 â”‚
â”‚ valid_loss                                   0.22644 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-44c1a_00005 started with configuration:
2024-08-26 13:56:59,290	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:56:59,465	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:00,769	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:00,904	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2908539)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed7/trial-44c1a_00005_5_alpha_d_ff=4,batch_size=32,d_core=256,d_model=64,dropout=0.0009,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_13-56-57/checkpoint_000000)[32m [repeated 10x across cluster][0m
2024-08-26 13:57:01,950	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:02,012	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:02,167	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:03,337	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:03,569	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:04,671	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:04,876	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:05,023	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:06,061	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2908539)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed7/trial-44c1a_00005_5_alpha_d_ff=4,batch_size=32,d_core=256,d_model=64,dropout=0.0009,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_13-56-57/checkpoint_000004)[32m [repeated 9x across cluster][0m
2024-08-26 13:57:07,653	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:08,143	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:09,302	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:10,731	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00088 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                       0.0001 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2908539)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2908539)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2908539)[0m {'batch_size': 32, 'learning_rate': 0.00010456686122968218, 'd_model': 64, 'd_core': 256, 'e_layers': 1, 'dropout': 0.0008811569093584206, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2908539)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2908539)[0m val 2785[32m [repeated 2x across cluster][0m

Trial trial-44c1a_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00322 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00043 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2908539)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2908539)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2906732)[0m Early stopping
[36m(_train_fn pid=2906734)[0m 	iters: 400, epoch: 4 | loss: 0.4231273[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2906734)[0m 	speed: 0.0043s/iter; left time: 9.6640s[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2907982)[0m Updating learning rate to 0.0012243640798777153[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2907982)[0m saving checkpoint...[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2907982)[0m Validation loss decreased (inf --> 0.2718).  Saving model state dict ...
[36m(_train_fn pid=2907982)[0m Epoch: 3 cost time: 1.096888780593872[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2906735)[0m Epoch: 8, Steps: 67 | Train Loss: nan Vali Loss: 0.2187322 Best vali loss: 0.2186542[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2908539)[0m Validation loss decreased (inf --> 0.2401).  Saving model state dict ...
[36m(_train_fn pid=2908670)[0m {'batch_size': 8, 'learning_rate': 0.0004279391847516988, 'd_model': 128, 'd_core': 512, 'e_layers': 4, 'dropout': 0.003219135499037774, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2907982)[0m EarlyStopping counter: 1 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2907982)[0m Epoch: 4, Steps: 265 | Train Loss: 0.6010812 Vali Loss: 0.2712044 Best vali loss: 0.2704682[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2908539)[0m Validation loss decreased (0.2401 --> 0.2254).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2908670)[0m configuration
[36m(_train_fn pid=2908670)[0m Use GPU: cuda:0
[36m(_train_fn pid=2908670)[0m train 8449
[36m(_train_fn pid=2908670)[0m val 2785
[36m(_train_fn pid=2908670)[0m start_epoch 0
[36m(_train_fn pid=2908670)[0m max_epoch 8

Trial trial-44c1a_00004 completed after 6 iterations at 2024-08-26 13:57:04. Total running time: 17s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.30499 â”‚
â”‚ time_total_s                                 8.78832 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.27047 â”‚
â”‚ train_loss                                   0.39289 â”‚
â”‚ valid_loss                                   0.27078 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2907982)[0m Early stopping
[36m(_train_fn pid=2906734)[0m 	iters: 200, epoch: 6 | loss: 0.2927754[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=2906734)[0m 	speed: 0.0040s/iter; left time: 5.5948s[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=2906734)[0m Updating learning rate to 3.201699961744611e-06[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2906734)[0m saving checkpoint...[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2906734)[0m Epoch: 5 cost time: 2.457305431365967[32m [repeated 9x across cluster][0m

Trial trial-44c1a_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00521 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00016 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2909730)[0m configuration
[36m(_train_fn pid=2909730)[0m {'batch_size': 16, 'learning_rate': 0.0001563147096956587, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.005211742082985821, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=2909730)[0m Use GPU: cuda:0
[36m(_train_fn pid=2909730)[0m train 8449
[36m(_train_fn pid=2909730)[0m val 2785
[36m(_train_fn pid=2909730)[0m start_epoch 0
[36m(_train_fn pid=2909730)[0m max_epoch 8
[36m(_train_fn pid=2907982)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2908539)[0m Epoch: 6, Steps: 265 | Train Loss: 0.2934280 Vali Loss: 0.2185559 Best vali loss: 0.2185559[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2908539)[0m Validation loss decreased (0.2186 --> 0.2186).  Saving model state dict ...[32m [repeated 5x across cluster][0m

Trial trial-44c1a_00005 completed after 8 iterations at 2024-08-26 13:57:10. Total running time: 23s
2024-08-26 13:57:10,756	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:11,001	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:11,456	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2906734)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed7/trial-44c1a_00002_2_alpha_d_ff=2,batch_size=16,d_core=512,d_model=512,dropout=0.0028,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-56-46/checkpoint_000006)[32m [repeated 7x across cluster][0m
2024-08-26 13:57:13,418	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:13,819	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:13,883	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:14,385	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:14,520	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:14,877	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:15,384	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:15,906	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:16,376	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:16,385	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2910292)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed7/trial-44c1a_00008_8_alpha_d_ff=2,batch_size=128,d_core=32,d_model=64,dropout=0.0012,e_layers=3,learning_rate=0.0003,lradj=type1_2024-08-26_13-57-10/checkpoint_000007)[32m [repeated 11x across cluster][0m
2024-08-26 13:57:16,860	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.42656 â”‚
â”‚ time_total_s                                11.77607 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21827 â”‚
â”‚ train_loss                                   0.42784 â”‚
â”‚ valid_loss                                   0.21827 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2909730)[0m Validation loss decreased (inf --> 0.2180).  Saving model state dict ...
[36m(_train_fn pid=2906734)[0m 	iters: 500, epoch: 7 | loss: 0.2073205[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2906734)[0m 	speed: 0.0046s/iter; left time: 2.5445s[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2908670)[0m Updating learning rate to 0.0004279391847516988[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2908670)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2908670)[0m Epoch: 1 cost time: 9.583055973052979[32m [repeated 7x across cluster][0m

Trial trial-44c1a_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00122 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00027 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2910292)[0m configuration
[36m(_train_fn pid=2910292)[0m {'batch_size': 128, 'learning_rate': 0.00027418432677158, 'd_model': 64, 'd_core': 32, 'e_layers': 3, 'dropout': 0.001215486988801725, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2910292)[0m Use GPU: cuda:0
[36m(_train_fn pid=2906734)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2910292)[0m train 8449
[36m(_train_fn pid=2910292)[0m val 2785
[36m(_train_fn pid=2910292)[0m start_epoch 0
[36m(_train_fn pid=2910292)[0m max_epoch 8
[36m(_train_fn pid=2906734)[0m Epoch: 7, Steps: 529 | Train Loss: 0.2961335 Vali Loss: 0.2165567 Best vali loss: 0.2155500[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2908539)[0m Validation loss decreased (0.2183 --> 0.2183).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2910292)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2360355 Best vali loss: 0.2360355
[36m(_train_fn pid=2910292)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2226390 Best vali loss: 0.2226390
[36m(_train_fn pid=2910292)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2206841 Best vali loss: 0.2206841

Trial trial-44c1a_00002 completed after 8 iterations at 2024-08-26 13:57:14. Total running time: 27s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             3.06065 â”‚
â”‚ time_total_s                                25.98311 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21555 â”‚
â”‚ train_loss                                   0.42471 â”‚
â”‚ valid_loss                                   0.21662 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2906734)[0m Early stopping
[36m(_train_fn pid=2910292)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2199357 Best vali loss: 0.2199357
[36m(_train_fn pid=2910292)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2195167 Best vali loss: 0.2195167
[36m(_train_fn pid=2910292)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2194275 Best vali loss: 0.2194275
[36m(_train_fn pid=2910292)[0m Validation loss decreased (inf --> 0.2360).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-44c1a_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00452 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00288 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2908670)[0m 	iters: 600, epoch: 2 | loss: 0.4386004[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2908670)[0m 	speed: 0.0064s/iter; left time: 43.6782s[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2910292)[0m Updating learning rate to 8.568260211611876e-06[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2910292)[0m saving checkpoint...[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2910292)[0m Epoch: 6 cost time: 0.3849475383758545[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2910292)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2193710 Best vali loss: 0.2193710
[36m(_train_fn pid=2910292)[0m Epoch: 8, Steps: 67 | Train Loss: nan Vali Loss: 0.2193456 Best vali loss: 0.2193456

Trial trial-44c1a_00008 completed after 8 iterations at 2024-08-26 13:57:16. Total running time: 29s
2024-08-26 13:57:19,109	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:19,202	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:20,551	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:21,389	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:22,145	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2909730)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed7/trial-44c1a_00007_7_alpha_d_ff=4,batch_size=16,d_core=32,d_model=128,dropout=0.0052,e_layers=1,learning_rate=0.0002,lradj=cosine_2024-08-26_13-57-04/checkpoint_000004)[32m [repeated 5x across cluster][0m
2024-08-26 13:57:23,596	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:25,197	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.47912 â”‚
â”‚ time_total_s                                 4.54672 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21935 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.21935 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 7 TERMINATED | 3 RUNNING
Current time: 2024-08-26 13:57:17. Total running time: 30s
Logical resource usage: 3.0/32 CPUs, 0.75/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 44c1a_00002 with best_valid_loss=0.2155500365865937 and params={'batch_size': 16, 'learning_rate': 5.1227199387913775e-05, 'd_model': 512, 'd_core': 512, 'e_layers': 1, 'dropout': 0.002840692305013047, 'lradj': 'type1', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-44c1a_00006   RUNNING                 8       0.000427939         128              4        512            4   0.00321914    type1          1           11.654         0.301115       0.221409            0.221409 â”‚
â”‚ trial-44c1a_00007   RUNNING                16       0.000156315         128              4         32            1   0.00521174    cosine         3            9.87883       0.401446       0.215747            0.215747 â”‚
â”‚ trial-44c1a_00009   RUNNING                32       0.0028816            64              1        128            4   0.0045233     type1                                                                                 â”‚
â”‚ trial-44c1a_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         4            9.23565       0.346789       0.22644             0.218695 â”‚
â”‚ trial-44c1a_00001   TERMINATED            128       0.00311457          256              3        256            4   0.00180349    cosine         4            5.9808      nan              0.272095            0.256671 â”‚
â”‚ trial-44c1a_00002   TERMINATED             16       5.12272e-05         512              2        512            1   0.00284069    type1          8           25.9831        0.424712       0.21662             0.21555  â”‚
â”‚ trial-44c1a_00003   TERMINATED            128       0.000180228          64              3         64            4   0.00313679    cosine         8            8.79663     nan              0.218732            0.218654 â”‚
â”‚ trial-44c1a_00004   TERMINATED             32       0.00489746          128              4        256            3   0.0116466     type1          6            8.78832       0.392888       0.270778            0.270468 â”‚
â”‚ trial-44c1a_00005   TERMINATED             32       0.000104567          64              4        256            1   0.000881157   cosine         8           11.7761        0.42784        0.218271            0.218271 â”‚
â”‚ trial-44c1a_00008   TERMINATED            128       0.000274184          64              2         32            3   0.00121549    type1          8            4.54672     nan              0.219346            0.219346 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2910906)[0m configuration
[36m(_train_fn pid=2910906)[0m {'batch_size': 32, 'learning_rate': 0.0028816040926980184, 'd_model': 64, 'd_core': 128, 'e_layers': 4, 'dropout': 0.004523303778396735, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=2910906)[0m Use GPU: cuda:0
[36m(_train_fn pid=2906734)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2910906)[0m train 8449
[36m(_train_fn pid=2910906)[0m val 2785
[36m(_train_fn pid=2910906)[0m start_epoch 0
[36m(_train_fn pid=2910906)[0m max_epoch 8
[36m(_train_fn pid=2909730)[0m Epoch: 3, Steps: 529 | Train Loss: 0.4014460 Vali Loss: 0.2157472 Best vali loss: 0.2157472[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2910292)[0m Validation loss decreased (0.2194 --> 0.2193).  Saving model state dict ...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2910906)[0m Validation loss decreased (inf --> 0.2276).  Saving model state dict ...
[36m(_train_fn pid=2909730)[0m 	iters: 500, epoch: 5 | loss: 0.4597667[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2909730)[0m 	speed: 0.0042s/iter; left time: 6.8445s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2908670)[0m Updating learning rate to 0.0002139695923758494[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2908670)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2908670)[0m Epoch: 2 cost time: 8.121745586395264[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2909730)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2909730)[0m Epoch: 5, Steps: 529 | Train Loss: 0.3300575 Vali Loss: 0.2167156 Best vali loss: 0.2157472[32m [repeated 5x across cluster][0m

Trial trial-44c1a_00007 completed after 6 iterations at 2024-08-26 13:57:25. Total running time: 38s
2024-08-26 13:57:25,843	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:28,052	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2908670)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed7/trial-44c1a_00006_6_alpha_d_ff=4,batch_size=8,d_core=512,d_model=128,dropout=0.0032,e_layers=4,learning_rate=0.0004,lradj=type1_2024-08-26_13-56-57/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 13:57:33,821	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed7' in 0.0108s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                               3.049 â”‚
â”‚ time_total_s                                18.69235 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21575 â”‚
â”‚ train_loss                                   0.37746 â”‚
â”‚ valid_loss                                   0.21737 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2909730)[0m Early stopping

Trial trial-44c1a_00009 completed after 4 iterations at 2024-08-26 13:57:25. Total running time: 38s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.24511 â”‚
â”‚ time_total_s                                 9.75309 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22755 â”‚
â”‚ train_loss                                   0.29777 â”‚
â”‚ valid_loss                                   0.22974 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2908670)[0m 	iters: 900, epoch: 3 | loss: 0.4354538[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2908670)[0m 	speed: 0.0043s/iter; left time: 23.1646s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2910906)[0m Updating learning rate to 0.0003602005115872523[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2910906)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2910906)[0m Epoch: 4 cost time: 1.9869678020477295[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2908670)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2908670)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.2621308 Vali Loss: 0.2261332 Best vali loss: 0.2214095[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2910906)[0m Early stopping
[36m(_train_fn pid=2908670)[0m 	iters: 800, epoch: 4 | loss: 0.1543167[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2908670)[0m 	speed: 0.0048s/iter; left time: 21.4944s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2908670)[0m Updating learning rate to 0.0001069847961879247
[36m(_train_fn pid=2908670)[0m saving checkpoint...
[36m(_train_fn pid=2908670)[0m Epoch: 3 cost time: 6.865613698959351

Trial trial-44c1a_00006 completed after 4 iterations at 2024-08-26 13:57:33. Total running time: 46s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-44c1a_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             5.75445 â”‚
â”‚ time_total_s                                34.45346 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22141 â”‚
â”‚ train_loss                                   0.24631 â”‚
â”‚ valid_loss                                    0.2293 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:57:33. Total running time: 46s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 44c1a_00002 with best_valid_loss=0.2155500365865937 and params={'batch_size': 16, 'learning_rate': 5.1227199387913775e-05, 'd_model': 512, 'd_core': 512, 'e_layers': 1, 'dropout': 0.002840692305013047, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2908670)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed7/trial-44c1a_00006_6_alpha_d_ff=4,batch_size=8,d_core=512,d_model=128,dropout=0.0032,e_layers=4,learning_rate=0.0004,lradj=type1_2024-08-26_13-56-57/checkpoint_000003)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-44c1a_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         4            9.23565       0.346789       0.22644             0.218695 â”‚
â”‚ trial-44c1a_00001   TERMINATED            128       0.00311457          256              3        256            4   0.00180349    cosine         4            5.9808      nan              0.272095            0.256671 â”‚
â”‚ trial-44c1a_00002   TERMINATED             16       5.12272e-05         512              2        512            1   0.00284069    type1          8           25.9831        0.424712       0.21662             0.21555  â”‚
â”‚ trial-44c1a_00003   TERMINATED            128       0.000180228          64              3         64            4   0.00313679    cosine         8            8.79663     nan              0.218732            0.218654 â”‚
â”‚ trial-44c1a_00004   TERMINATED             32       0.00489746          128              4        256            3   0.0116466     type1          6            8.78832       0.392888       0.270778            0.270468 â”‚
â”‚ trial-44c1a_00005   TERMINATED             32       0.000104567          64              4        256            1   0.000881157   cosine         8           11.7761        0.42784        0.218271            0.218271 â”‚
â”‚ trial-44c1a_00006   TERMINATED              8       0.000427939         128              4        512            4   0.00321914    type1          4           34.4535        0.246309       0.229299            0.221409 â”‚
â”‚ trial-44c1a_00007   TERMINATED             16       0.000156315         128              4         32            1   0.00521174    cosine         6           18.6924        0.37746        0.217366            0.215747 â”‚
â”‚ trial-44c1a_00008   TERMINATED            128       0.000274184          64              2         32            3   0.00121549    type1          8            4.54672     nan              0.219346            0.219346 â”‚
â”‚ trial-44c1a_00009   TERMINATED             32       0.0028816            64              1        128            4   0.0045233     type1          4            9.75309       0.297769       0.229742            0.227554 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

[36m(_train_fn pid=2908670)[0m Updating learning rate to 5.349239809396235e-05
[36m(_train_fn pid=2908670)[0m saving checkpoint...
[36m(_train_fn pid=2908670)[0m Epoch: 4 cost time: 5.097318887710571
[36m(_train_fn pid=2908670)[0m Early stopping
[36m(_train_fn pid=2908670)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2908670)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.2463089 Vali Loss: 0.2292986 Best vali loss: 0.2214095
Best hyperparameters found were:  {'batch_size': 16, 'learning_rate': 5.1227199387913775e-05, 'd_model': 512, 'd_core': 512, 'e_layers': 1, 'dropout': 0.002840692305013047, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2908670)[0m 	iters: 1000, epoch: 4 | loss: 0.1311911[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2908670)[0m 	speed: 0.0048s/iter; left time: 20.6954s[32m [repeated 2x across cluster][0m


Time taken (4 parallel trials): 52 seconds


2024-08-26 13:57:37,927	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:57:38,333	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:57:38,338	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:57:38,346	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:57:41,253	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2914345)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00003_3_alpha_d_ff=1,batch_size=128,d_core=32,d_model=64,dropout=0.0072,e_layers=2,learning_rate=0.0002,lradj=cosine_2024-08-26_13-57-38/checkpoint_000000)
2024-08-26 13:57:41,811	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:41,819	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed8   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator    â”‚
â”‚ Scheduler                        FIFOScheduler            â”‚
â”‚ Number of trials                 10                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-57-37_495219_2912121/artifacts/2024-08-26_13-57-38/ETTh2_96_96_test_seed8/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 13:57:38. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-635e4_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-635e4_00001   PENDING              64       0.00471546           64              1         64            3   0.0084366    type1   â”‚
â”‚ trial-635e4_00002   PENDING               8       0.000375607          64              2         64            1   0.00292683   cosine  â”‚
â”‚ trial-635e4_00003   PENDING             128       0.000163403          64              1         32            2   0.00724812   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-635e4_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2914342)[0m configuration
[36m(_train_fn pid=2914342)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2914342)[0m Use GPU: cuda:0

Trial trial-635e4_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00844 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00472 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-635e4_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00293 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00038 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-635e4_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00725 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00016 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2914343)[0m {'batch_size': 64, 'learning_rate': 0.004715455937161372, 'd_model': 64, 'd_core': 64, 'e_layers': 3, 'dropout': 0.008436604119425322, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=2914342)[0m train 8449
[36m(_train_fn pid=2914342)[0m val 2785
[36m(_train_fn pid=2914342)[0m start_epoch 0
[36m(_train_fn pid=2914342)[0m max_epoch 8
[36m(_train_fn pid=2914342)[0m 	iters: 100, epoch: 1 | loss: 0.7912382
[36m(_train_fn pid=2914342)[0m 	speed: 0.0110s/iter; left time: 22.2452s
[36m(_train_fn pid=2914345)[0m Updating learning rate to 0.00015718401580692024
[36m(_train_fn pid=2914345)[0m saving checkpoint...
[36m(_train_fn pid=2914345)[0m Validation loss decreased (inf --> 0.2571).  Saving model state dict ...
[36m(_train_fn pid=2914345)[0m Epoch: 1 cost time: 0.7073631286621094
[36m(_train_fn pid=2914345)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2571392 Best vali loss: 0.2571392
[36m(_train_fn pid=2914345)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00003_3_alpha_d_ff=1,batch_size=128,d_core=32,d_model=64,dropout=0.0072,e_layers=2,learning_rate=0.0002,lradj=cosine_2024-08-26_13-57-38/checkpoint_000001)
2024-08-26 13:57:42,327	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:42,369	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:42,904	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:42,929	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:43,502	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:44,014	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:44,027	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:44,034	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:44,589	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:45,137	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:45,164	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:45,571	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:46,262	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:46,814	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2914342)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00000_0_2024-08-26_13-57-38/checkpoint_000003)[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2914345)[0m Updating learning rate to 0.00013947333746405083
[36m(_train_fn pid=2914345)[0m saving checkpoint...
[36m(_train_fn pid=2914345)[0m Validation loss decreased (0.2571 --> 0.2376).  Saving model state dict ...
[36m(_train_fn pid=2914345)[0m Epoch: 2 cost time: 0.43945884704589844
[36m(_train_fn pid=2914345)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2376100 Best vali loss: 0.2376100
[36m(_train_fn pid=2914343)[0m Epoch: 1, Steps: 133 | Train Loss: 0.6211900 Vali Loss: 0.2263664 Best vali loss: 0.2263664
[36m(_train_fn pid=2914345)[0m Validation loss decreased (0.2376 --> 0.2295).  Saving model state dict ...
[36m(_train_fn pid=2914345)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2295464 Best vali loss: 0.2295464
[36m(_train_fn pid=2914345)[0m Validation loss decreased (0.2295 --> 0.2254).  Saving model state dict ...
[36m(_train_fn pid=2914345)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2253847 Best vali loss: 0.2253847
[36m(_train_fn pid=2914343)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2914345)[0m Validation loss decreased (0.2254 --> 0.2237).  Saving model state dict ...
[36m(_train_fn pid=2914345)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2236679 Best vali loss: 0.2236679
[36m(_train_fn pid=2914345)[0m Validation loss decreased (0.2237 --> 0.2227).  Saving model state dict ...
[36m(_train_fn pid=2914345)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2226978 Best vali loss: 0.2226978
[36m(_train_fn pid=2914345)[0m Validation loss decreased (0.2227 --> 0.2222).  Saving model state dict ...
[36m(_train_fn pid=2914345)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2222217 Best vali loss: 0.2222217
[36m(_train_fn pid=2914343)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2914345)[0m {'batch_size': 128, 'learning_rate': 0.00016340317899400295, 'd_model': 64, 'd_core': 32, 'e_layers': 2, 'dropout': 0.007248123693763373, 'lradj': 'cosine', 'd_ff': 64}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2914343)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m

Trial trial-635e4_00003 completed after 8 iterations at 2024-08-26 13:57:45. Total running time: 6s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.54533 â”‚
â”‚ time_total_s                                 5.13193 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                               0.2222 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                    0.2222 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-635e4_00001 completed after 4 iterations at 2024-08-26 13:57:45. Total running time: 6s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                              1.1473 â”‚
â”‚ time_total_s                                 5.22121 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22637 â”‚
â”‚ train_loss                                   0.34599 â”‚
â”‚ valid_loss                                   0.22867 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2914345)[0m Validation loss decreased (0.2222 --> 0.2222).  Saving model state dict ...
[36m(_train_fn pid=2914345)[0m Epoch: 8, Steps: 67 | Train Loss: nan Vali Loss: 0.2222030 Best vali loss: 0.2222030
[36m(_train_fn pid=2914343)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2914343)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2914343)[0m Early stopping
[36m(_train_fn pid=2914343)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2914343)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2914342)[0m 	iters: 200, epoch: 4 | loss: 0.3283677[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2914342)[0m 	speed: 0.0037s/iter; left time: 4.1870s[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2914342)[0m Updating learning rate to 0.00020740251485476345[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2914342)[0m saving checkpoint...[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2914342)[0m Validation loss decreased (inf --> 0.2196).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2914344)[0m Epoch: 1 cost time: 5.175068616867065[32m [repeated 14x across cluster][0m

Trial trial-635e4_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00084 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                       0.0001 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-635e4_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00461 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00016 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2915836)[0m {'batch_size': 128, 'learning_rate': 0.0001624841544219658, 'd_model': 128, 'd_core': 128, 'e_layers': 2, 'dropout': 0.004611625781563351, 'lradj': 'type1', 'd_ff': 256}
[36m(_train_fn pid=2914342)[0m Validation loss decreased (0.2196 --> 0.2186).  Saving model state dict ...
2024-08-26 13:57:47,895	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:48,261	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:48,313	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:48,727	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:49,144	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:49,581	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:50,017	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:50,065	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:50,471	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:50,896	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:52,161	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2914344)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00002_2_alpha_d_ff=2,batch_size=8,d_core=64,d_model=64,dropout=0.0029,e_layers=1,learning_rate=0.0004,lradj=cosine_2024-08-26_13-57-38/checkpoint_000001)[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2914342)[0m Epoch: 4, Steps: 265 | Train Loss: 0.3645904 Vali Loss: 0.2212210 Best vali loss: 0.2186028[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2915836)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2330602 Best vali loss: 0.2330602
[36m(_train_fn pid=2914342)[0m EarlyStopping counter: 1 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2915836)[0m Validation loss decreased (0.2331 --> 0.2207).  Saving model state dict ...
[36m(_train_fn pid=2915836)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2206627 Best vali loss: 0.2206627
[36m(_train_fn pid=2915836)[0m Validation loss decreased (0.2207 --> 0.2189).  Saving model state dict ...
[36m(_train_fn pid=2915836)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2188987 Best vali loss: 0.2188987
[36m(_train_fn pid=2915836)[0m Validation loss decreased (0.2189 --> 0.2187).  Saving model state dict ...
[36m(_train_fn pid=2915836)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2186663 Best vali loss: 0.2186663
[36m(_train_fn pid=2915836)[0m Validation loss decreased (0.2187 --> 0.2183).  Saving model state dict ...
[36m(_train_fn pid=2915836)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2183464 Best vali loss: 0.2183464
[36m(_train_fn pid=2915836)[0m Validation loss decreased (0.2183 --> 0.2182).  Saving model state dict ...

Trial trial-635e4_00000 completed after 6 iterations at 2024-08-26 13:57:50. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.80011 â”‚
â”‚ time_total_s                                10.13608 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                               0.2186 â”‚
â”‚ train_loss                                   0.28477 â”‚
â”‚ valid_loss                                   0.22536 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2915836)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2182216 Best vali loss: 0.2182216
[36m(_train_fn pid=2915838)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2915838)[0m {'batch_size': 8, 'learning_rate': 9.985785392342546e-05, 'd_model': 512, 'd_core': 64, 'e_layers': 4, 'dropout': 0.0008371961298578976, 'lradj': 'cosine', 'd_ff': 2048}
[36m(_train_fn pid=2915836)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2915838)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2915838)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2914342)[0m Early stopping
[36m(_train_fn pid=2915836)[0m Validation loss decreased (0.2182 --> 0.2182).  Saving model state dict ...
[36m(_train_fn pid=2915836)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2181745 Best vali loss: 0.2181745
[36m(_train_fn pid=2915838)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2915838)[0m max_epoch 8[32m [repeated 2x across cluster][0m

Trial trial-635e4_00004 completed after 8 iterations at 2024-08-26 13:57:50. Total running time: 12s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.42298 â”‚
â”‚ time_total_s                                    4.13 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21813 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.21813 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2915836)[0m Validation loss decreased (0.2182 --> 0.2181).  Saving model state dict ...
[36m(_train_fn pid=2915836)[0m Epoch: 8, Steps: 67 | Train Loss: nan Vali Loss: 0.2181276 Best vali loss: 0.2181276
[36m(_train_fn pid=2915838)[0m 	iters: 500, epoch: 1 | loss: 0.2805237[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2915838)[0m 	speed: 0.0063s/iter; left time: 49.8393s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2915836)[0m Updating learning rate to 1.269407456421608e-06[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2915836)[0m saving checkpoint...[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2915836)[0m Validation loss decreased (inf --> 0.2331).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-635e4_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00272 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00031 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2915836)[0m Epoch: 8 cost time: 0.30881357192993164[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2916832)[0m {'batch_size': 32, 'learning_rate': 0.00030782732399263675, 'd_model': 32, 'd_core': 64, 'e_layers': 4, 'dropout': 0.0027226806090255495, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2914342)[0m Epoch: 6, Steps: 265 | Train Loss: 0.2847656 Vali Loss: 0.2253629 Best vali loss: 0.2186028[32m [repeated 2x across cluster][0m

Trial trial-635e4_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00104 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00014 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2917003)[0m {'batch_size': 8, 'learning_rate': 0.00013966576842946708, 'd_model': 256, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0010378205730091792, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2914342)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
2024-08-26 13:57:55,699	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:57:58,463	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2915838)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00005_5_alpha_d_ff=4,batch_size=8,d_core=64,d_model=512,dropout=0.0008,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_13-57-45/checkpoint_000000)[32m [repeated 2x across cluster][0m
2024-08-26 13:57:58,822	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:00,410	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:02,249	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:05,659	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2916832)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00006_6_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0027,e_layers=4,learning_rate=0.0003,lradj=type1_2024-08-26_13-57-50/checkpoint_000003)[32m [repeated 4x across cluster][0m
2024-08-26 13:58:06,898	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:08,897	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:09,144	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2914344)[0m Validation loss decreased (0.2238 --> 0.2189).  Saving model state dict ...
[36m(_train_fn pid=2917003)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2917003)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2917003)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2917003)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2917003)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2917003)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2917003)[0m 	iters: 300, epoch: 1 | loss: 1.1086806[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2917003)[0m 	speed: 0.0112s/iter; left time: 91.2734s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2916832)[0m Updating learning rate to 0.00030782732399263675[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2916832)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2916832)[0m Validation loss decreased (inf --> 0.2274).  Saving model state dict ...
[36m(_train_fn pid=2916832)[0m Epoch: 1 cost time: 3.183215856552124[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2916832)[0m Epoch: 1, Steps: 265 | Train Loss: 0.5158234 Vali Loss: 0.2274393 Best vali loss: 0.2274393[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2915838)[0m Validation loss decreased (inf --> 0.2231).  Saving model state dict ...
[36m(_train_fn pid=2916832)[0m Validation loss decreased (0.2274 --> 0.2214).  Saving model state dict ...
[36m(_train_fn pid=2914344)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2914344)[0m 	iters: 200, epoch: 4 | loss: 1.2132803[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2914344)[0m 	speed: 0.0065s/iter; left time: 33.0449s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2914344)[0m Updating learning rate to 0.000259672831747571[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2914344)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2914344)[0m Epoch: 3 cost time: 6.301309823989868[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2916832)[0m Validation loss decreased (0.2214 --> 0.2208).  Saving model state dict ...
[36m(_train_fn pid=2916832)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3059014 Vali Loss: 0.2207821 Best vali loss: 0.2207821[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2916832)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2914344)[0m 	iters: 1000, epoch: 4 | loss: 0.1366831[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2914344)[0m 	speed: 0.0061s/iter; left time: 26.0724s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2916832)[0m Updating learning rate to 3.8478415499079593e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2916832)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2917003)[0m Validation loss decreased (inf --> 0.2150).  Saving model state dict ...
[36m(_train_fn pid=2917003)[0m Epoch: 1 cost time: 11.988960981369019[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2917003)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.5636634 Vali Loss: 0.2149675 Best vali loss: 0.2149675[32m [repeated 2x across cluster][0m

Trial status: 4 TERMINATED | 4 RUNNING
Current time: 2024-08-26 13:58:08. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 635e4_00007 with best_valid_loss=0.21496746113099144 and params={'batch_size': 8, 'learning_rate': 0.00013966576842946708, 'd_model': 256, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0010378205730091792, 'lradj': 'cosine', 'd_ff': 256}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-635e4_00002   RUNNING                 8       0.000375607          64              2         64            1   0.00292683    cosine         3           20.47          0.394327       0.222786            0.218922 â”‚
â”‚ trial-635e4_00005   RUNNING                 8       9.98579e-05         512              4         64            4   0.000837196   cosine         1           11.7664        0.326157       0.223091            0.223091 â”‚
â”‚ trial-635e4_00006   RUNNING                32       0.000307827          32              4         64            4   0.00272268    type1          4           13.9874        0.428164       0.220832            0.220782 â”‚
â”‚ trial-635e4_00007   RUNNING                 8       0.000139666         256              1         32            4   0.00103782    cosine         1           14.5107        0.563663       0.214967            0.214967 â”‚
â”‚ trial-635e4_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         6           10.1361        0.284766       0.225363            0.218603 â”‚
â”‚ trial-635e4_00001   TERMINATED             64       0.00471546           64              1         64            3   0.0084366     type1          4            5.22121       0.34599        0.228672            0.226366 â”‚
â”‚ trial-635e4_00003   TERMINATED            128       0.000163403          64              1         32            2   0.00724812    cosine         8            5.13193     nan              0.222203            0.222203 â”‚
â”‚ trial-635e4_00004   TERMINATED            128       0.000162484         128              2        128            2   0.00461163    type1          8            4.13        nan              0.218128            0.218128 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2916832)[0m Validation loss decreased (0.2208 --> 0.2207).  Saving model state dict ...
[36m(_train_fn pid=2914344)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2914344)[0m 	iters: 500, epoch: 5 | loss: 0.4365455[32m [repeated 15x across cluster][0m
2024-08-26 13:58:11,980	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:11,999	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2915838)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00005_5_alpha_d_ff=4,batch_size=8,d_core=64,d_model=512,dropout=0.0008,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_13-57-45/checkpoint_000001)[32m [repeated 4x across cluster][0m
2024-08-26 13:58:15,462	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:17,622	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2914344)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00002_2_alpha_d_ff=2,batch_size=8,d_core=64,d_model=64,dropout=0.0029,e_layers=1,learning_rate=0.0004,lradj=cosine_2024-08-26_13-57-38/checkpoint_000004)[32m [repeated 3x across cluster][0m
2024-08-26 13:58:18,713	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:19,817	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:23,944	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2915838)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00005_5_alpha_d_ff=4,batch_size=8,d_core=64,d_model=512,dropout=0.0008,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_13-57-45/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 13:58:24,456	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2914344)[0m 	speed: 0.0047s/iter; left time: 17.4297s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2914344)[0m Updating learning rate to 0.00018780353164671887[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2914344)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2915838)[0m Epoch: 2 cost time: 11.662467002868652[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2916832)[0m Epoch: 6, Steps: 265 | Train Loss: 0.3098487 Vali Loss: 0.2206938 Best vali loss: 0.2206938[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2916832)[0m Validation loss decreased (0.2207 --> 0.2207).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-635e4_00002 completed after 5 iterations at 2024-08-26 13:58:17. Total running time: 39s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             8.47614 â”‚
â”‚ time_total_s                                37.67704 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21892 â”‚
â”‚ train_loss                                   0.46331 â”‚
â”‚ valid_loss                                    0.2225 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2914344)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2914344)[0m Early stopping
[36m(_train_fn pid=2917003)[0m 	iters: 900, epoch: 2 | loss: 0.3933885[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2917003)[0m 	speed: 0.0109s/iter; left time: 70.6501s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2914344)[0m Updating learning rate to 0.00011593423154586674[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2914344)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2914344)[0m Epoch: 5 cost time: 6.530659437179565[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2914344)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.4633075 Vali Loss: 0.2225049 Best vali loss: 0.2189224[32m [repeated 2x across cluster][0m

Trial trial-635e4_00006 completed after 8 iterations at 2024-08-26 13:58:18. Total running time: 40s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             3.24776 â”‚
â”‚ time_total_s                                27.03107 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22068 â”‚
â”‚ train_loss                                    0.4793 â”‚
â”‚ valid_loss                                   0.22068 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-635e4_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00092 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00042 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2918245)[0m configuration
[36m(_train_fn pid=2918245)[0m {'batch_size': 8, 'learning_rate': 0.0004162254383095591, 'd_model': 64, 'd_core': 256, 'e_layers': 3, 'dropout': 0.0009205628504031689, 'lradj': 'type1', 'd_ff': 192}
[36m(_train_fn pid=2916832)[0m Validation loss decreased (0.2207 --> 0.2207).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2918245)[0m Use GPU: cuda:0
[36m(_train_fn pid=2918245)[0m train 8449
[36m(_train_fn pid=2918245)[0m val 2785
[36m(_train_fn pid=2918245)[0m start_epoch 0
[36m(_train_fn pid=2918245)[0m max_epoch 8

Trial trial-635e4_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00221 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00138 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2918379)[0m {'batch_size': 8, 'learning_rate': 0.0013777577250027324, 'd_model': 512, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0022087664441398144, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2917003)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2918245)[0m 	iters: 800, epoch: 1 | loss: 0.1341345[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2918245)[0m 	speed: 0.0036s/iter; left time: 27.8623s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2917003)[0m Updating learning rate to 0.00011921219019278663[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2917003)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2917003)[0m Epoch: 2 cost time: 11.427738904953003[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2917003)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.2488066 Vali Loss: 0.2215293 Best vali loss: 0.2149675[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2915838)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2918245)[0m Validation loss decreased (inf --> 0.2229).  Saving model state dict ...
[36m(_train_fn pid=2918379)[0m configuration
[36m(_train_fn pid=2918379)[0m Use GPU: cuda:0
[36m(_train_fn pid=2918379)[0m train 8449
[36m(_train_fn pid=2918379)[0m val 2785
[36m(_train_fn pid=2918379)[0m start_epoch 0
[36m(_train_fn pid=2918379)[0m max_epoch 8
[36m(_train_fn pid=2917003)[0m 	iters: 800, epoch: 3 | loss: 0.7947947[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=2917003)[0m 	speed: 0.0113s/iter; left time: 62.7598s[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=2918245)[0m Updating learning rate to 0.0004162254383095591[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2918245)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2918245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00008_8_alpha_d_ff=3,batch_size=8,d_core=256,d_model=64,dropout=0.0009,e_layers=3,learning_rate=0.0004,lradj=type1_2024-08-26_13-58-17/checkpoint_000001)[32m [repeated 2x across cluster][0m
2024-08-26 13:58:29,043	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:33,034	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:33,643	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:34,455	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2918379)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00009_9_alpha_d_ff=2,batch_size=8,d_core=512,d_model=512,dropout=0.0022,e_layers=4,learning_rate=0.0014,lradj=cosine_2024-08-26_13-58-18/checkpoint_000000)[32m [repeated 3x across cluster][0m
2024-08-26 13:58:37,110	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:38,168	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2918245)[0m Epoch: 1 cost time: 4.1930506229400635[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2918245)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.4696878 Vali Loss: 0.2228907 Best vali loss: 0.2228907[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2918245)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2917003)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2918245)[0m 	iters: 1000, epoch: 3 | loss: 0.1089949[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2918245)[0m 	speed: 0.0036s/iter; left time: 19.4893s[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2917003)[0m Updating learning rate to 9.655677203798167e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2917003)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2917003)[0m Epoch: 3 cost time: 11.052636861801147[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2917003)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4972144 Vali Loss: 0.2345777 Best vali loss: 0.2149675[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2918379)[0m Validation loss decreased (inf --> 0.2717).  Saving model state dict ...

Trial trial-635e4_00008 completed after 4 iterations at 2024-08-26 13:58:38. Total running time: 59s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             4.52225 â”‚
â”‚ time_total_s                                18.99819 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22289 â”‚
â”‚ train_loss                                   0.41878 â”‚
â”‚ valid_loss                                   0.22862 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2918245)[0m Early stopping
[36m(_train_fn pid=2918245)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2917003)[0m 	iters: 500, epoch: 4 | loss: 0.3645605[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2917003)[0m 	speed: 0.0110s/iter; left time: 52.7007s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2918245)[0m Updating learning rate to 5.202817978869489e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2918245)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2918245)[0m Epoch: 4 cost time: 3.937014579772949[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2918245)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.4187830 Vali Loss: 0.2286196 Best vali loss: 0.2228907[32m [repeated 4x across cluster][0m

Trial status: 7 TERMINATED | 3 RUNNING
Current time: 2024-08-26 13:58:38. Total running time: 1min 0s
Logical resource usage: 3.0/32 CPUs, 0.75/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 635e4_00007 with best_valid_loss=0.21496746113099144 and params={'batch_size': 8, 'learning_rate': 0.00013966576842946708, 'd_model': 256, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0010378205730091792, 'lradj': 'cosine', 'd_ff': 256}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-635e4_00005   RUNNING                 8       9.98579e-05         512              4         64            4   0.000837196   cosine         4           50.3921        0.310824       0.230516            0.218237 â”‚
â”‚ trial-635e4_00007   RUNNING                 8       0.000139666         256              1         32            4   0.00103782    cosine         3           40.6407        0.497214       0.234578            0.214967 â”‚
â”‚ trial-635e4_00009   RUNNING                 8       0.00137776          512              2        512            4   0.00220877    cosine         1           14.2257        0.332926       0.271677            0.271677 â”‚
â”‚ trial-635e4_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         6           10.1361        0.284766       0.225363            0.218603 â”‚
â”‚ trial-635e4_00001   TERMINATED             64       0.00471546           64              1         64            3   0.0084366     type1          4            5.22121       0.34599        0.228672            0.226366 â”‚
â”‚ trial-635e4_00002   TERMINATED              8       0.000375607          64              2         64            1   0.00292683    cosine         5           37.677         0.463307       0.222505            0.218922 â”‚
â”‚ trial-635e4_00003   TERMINATED            128       0.000163403          64              1         32            2   0.00724812    cosine         8            5.13193     nan              0.222203            0.222203 â”‚
â”‚ trial-635e4_00004   TERMINATED            128       0.000162484         128              2        128            2   0.00461163    type1          8            4.13        nan              0.218128            0.218128 â”‚
â”‚ trial-635e4_00006   TERMINATED             32       0.000307827          32              4         64            4   0.00272268    type1          8           27.0311        0.479302       0.220684            0.220684 â”‚
â”‚ trial-635e4_00008   TERMINATED              8       0.000416225          64              3        256            3   0.000920563   type1          4           18.9982        0.418783       0.22862             0.222891 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2918379)[0m 	iters: 800, epoch: 2 | loss: 0.3304290[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2917003)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00007_7_alpha_d_ff=1,batch_size=8,d_core=32,d_model=256,dropout=0.0010,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_13-57-50/checkpoint_000003)[32m [repeated 3x across cluster][0m
2024-08-26 13:58:47,773	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:49,234	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:58:53,894	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2918379)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00009_9_alpha_d_ff=2,batch_size=8,d_core=512,d_model=512,dropout=0.0022,e_layers=4,learning_rate=0.0014,lradj=cosine_2024-08-26_13-58-18/checkpoint_000002)[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2918379)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00009_9_alpha_d_ff=2,batch_size=8,d_core=512,d_model=512,dropout=0.0022,e_layers=4,learning_rate=0.0014,lradj=cosine_2024-08-26_13-58-18/checkpoint_000003)
[36m(_train_fn pid=2918379)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00009_9_alpha_d_ff=2,batch_size=8,d_core=512,d_model=512,dropout=0.0022,e_layers=4,learning_rate=0.0014,lradj=cosine_2024-08-26_13-58-18/checkpoint_000004)
[36m(_train_fn pid=2918379)[0m 	speed: 0.0114s/iter; left time: 75.2920s[32m [repeated 14x across cluster][0m

Trial trial-635e4_00007 completed after 4 iterations at 2024-08-26 13:58:46. Total running time: 1min 8s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            13.62026 â”‚
â”‚ time_total_s                                54.26097 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21497 â”‚
â”‚ train_loss                                   0.28247 â”‚
â”‚ valid_loss                                   0.22347 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2917003)[0m Updating learning rate to 6.983288421473354e-05
[36m(_train_fn pid=2917003)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2917003)[0m saving checkpoint...
[36m(_train_fn pid=2917003)[0m Epoch: 4 cost time: 11.467644929885864
[36m(_train_fn pid=2917003)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.2824748 Vali Loss: 0.2234685 Best vali loss: 0.2149675
[36m(_train_fn pid=2917003)[0m Early stopping
[36m(_train_fn pid=2918379)[0m Validation loss decreased (0.2717 --> 0.2711).  Saving model state dict ...
[36m(_train_fn pid=2918379)[0m 	iters: 100, epoch: 3 | loss: 1.1375769[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2918379)[0m 	speed: 0.0304s/iter; left time: 189.6566s[32m [repeated 9x across cluster][0m

Trial trial-635e4_00005 completed after 5 iterations at 2024-08-26 13:58:49. Total running time: 1min 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                            12.12191 â”‚
â”‚ time_total_s                                62.51405 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21824 â”‚
â”‚ train_loss                                   0.24454 â”‚
â”‚ valid_loss                                   0.24268 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2915838)[0m Updating learning rate to 3.082195381769863e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2915838)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2915838)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2915838)[0m Epoch: 5 cost time: 10.821041584014893[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2915838)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.2445425 Vali Loss: 0.2426840 Best vali loss: 0.2182374[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2915838)[0m Early stopping
[36m(_train_fn pid=2918379)[0m Validation loss decreased (0.2711 --> 0.2679).  Saving model state dict ...
[36m(_train_fn pid=2918379)[0m 	iters: 1000, epoch: 3 | loss: 0.4164349[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2918379)[0m 	speed: 0.0046s/iter; left time: 24.8206s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2918379)[0m Updating learning rate to 0.0009525013900871478
[36m(_train_fn pid=2918379)[0m saving checkpoint...
[36m(_train_fn pid=2918379)[0m Epoch: 3 cost time: 5.448140382766724
[36m(_train_fn pid=2918379)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4086536 Vali Loss: 0.2679482 Best vali loss: 0.2679482
[36m(_train_fn pid=2918379)[0m 	iters: 1000, epoch: 4 | loss: 0.3277037[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2918379)[0m 	speed: 0.0047s/iter; left time: 19.9301s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2918379)[0m Updating learning rate to 0.0006888788625013662
[36m(_train_fn pid=2918379)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2918379)[0m saving checkpoint...
[36m(_train_fn pid=2918379)[0m Epoch: 4 cost time: 5.228366374969482
[36m(_train_fn pid=2918379)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.4231047 Vali Loss: 0.2687604 Best vali loss: 0.2679482
[36m(_train_fn pid=2918379)[0m 	iters: 1000, epoch: 5 | loss: 0.3739271[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2918379)[0m 	speed: 0.0045s/iter; left time: 14.4793s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2918379)[0m Updating learning rate to 0.0004252563349155847
[36m(_train_fn pid=2918379)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2918379)[0m saving checkpoint...
[36m(_train_fn pid=2918379)[0m Epoch: 5 cost time: 4.84615421295166
[36m(_train_fn pid=2918379)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.2955311 Vali Loss: 0.2709386 Best vali loss: 0.2679482

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2024-08-26 13:59:08. Total running time: 1min 30s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 635e4_00007 with best_valid_loss=0.21496746113099144 and params={'batch_size': 8, 'learning_rate': 0.00013966576842946708, 'd_model': 256, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0010378205730091792, 'lradj': 'cosine', 'd_ff': 256}
2024-08-26 13:59:11,160	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8' in 0.0078s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-635e4_00009   RUNNING                 8       0.00137776          512              2        512            4   0.00220877    cosine         5           44.9713        0.295531       0.270939            0.267948 â”‚
â”‚ trial-635e4_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         6           10.1361        0.284766       0.225363            0.218603 â”‚
â”‚ trial-635e4_00001   TERMINATED             64       0.00471546           64              1         64            3   0.0084366     type1          4            5.22121       0.34599        0.228672            0.226366 â”‚
â”‚ trial-635e4_00002   TERMINATED              8       0.000375607          64              2         64            1   0.00292683    cosine         5           37.677         0.463307       0.222505            0.218922 â”‚
â”‚ trial-635e4_00003   TERMINATED            128       0.000163403          64              1         32            2   0.00724812    cosine         8            5.13193     nan              0.222203            0.222203 â”‚
â”‚ trial-635e4_00004   TERMINATED            128       0.000162484         128              2        128            2   0.00461163    type1          8            4.13        nan              0.218128            0.218128 â”‚
â”‚ trial-635e4_00005   TERMINATED              8       9.98579e-05         512              4         64            4   0.000837196   cosine         5           62.5141        0.244542       0.242684            0.218237 â”‚
â”‚ trial-635e4_00006   TERMINATED             32       0.000307827          32              4         64            4   0.00272268    type1          8           27.0311        0.479302       0.220684            0.220684 â”‚
â”‚ trial-635e4_00007   TERMINATED              8       0.000139666         256              1         32            4   0.00103782    cosine         4           54.261         0.282475       0.223468            0.214967 â”‚
â”‚ trial-635e4_00008   TERMINATED              8       0.000416225          64              3        256            3   0.000920563   type1          4           18.9982        0.418783       0.22862             0.222891 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2918379)[0m 	iters: 900, epoch: 6 | loss: 0.4276967[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2918379)[0m 	speed: 0.0049s/iter; left time: 11.1294s[32m [repeated 9x across cluster][0m

Trial trial-635e4_00009 completed after 6 iterations at 2024-08-26 13:59:11. Total running time: 1min 32s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-635e4_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             5.92145 â”‚
â”‚ time_total_s                                50.89278 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.26795 â”‚
â”‚ train_loss                                   0.60763 â”‚
â”‚ valid_loss                                   0.26905 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:59:11. Total running time: 1min 32s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 635e4_00007 with best_valid_loss=0.21496746113099144 and params={'batch_size': 8, 'learning_rate': 0.00013966576842946708, 'd_model': 256, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0010378205730091792, 'lradj': 'cosine', 'd_ff': 256}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-635e4_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         6           10.1361        0.284766       0.225363            0.218603 â”‚
â”‚ trial-635e4_00001   TERMINATED             64       0.00471546           64              1         64            3   0.0084366     type1          4            5.22121       0.34599        0.228672            0.226366 â”‚
â”‚ trial-635e4_00002   TERMINATED              8       0.000375607          64              2         64            1   0.00292683    cosine         5           37.677         0.463307       0.222505            0.218922 â”‚
â”‚ trial-635e4_00003   TERMINATED            128       0.000163403          64              1         32            2   0.00724812    cosine         8            5.13193     nan              0.222203            0.222203 â”‚
â”‚ trial-635e4_00004   TERMINATED            128       0.000162484         128              2        128            2   0.00461163    type1          8            4.13        nan              0.218128            0.218128 â”‚
â”‚ trial-635e4_00005   TERMINATED              8       9.98579e-05         512              4         64            4   0.000837196   cosine         5           62.5141        0.244542       0.242684            0.218237 â”‚
â”‚ trial-635e4_00006   TERMINATED             32       0.000307827          32              4         64            4   0.00272268    type1          8           27.0311        0.479302       0.220684            0.220684 â”‚
â”‚ trial-635e4_00007   TERMINATED              8       0.000139666         256              1         32            4   0.00103782    cosine         4           54.261         0.282475       0.223468            0.214967 â”‚
â”‚ trial-635e4_00008   TERMINATED              8       0.000416225          64              3        256            3   0.000920563   type1          4           18.9982        0.418783       0.22862             0.222891 â”‚
â”‚ trial-635e4_00009   TERMINATED              8       0.00137776          512              2        512            4   0.00220877    cosine         6           50.8928        0.607626       0.269052            0.267948 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 0.00013966576842946708, 'd_model': 256, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0010378205730091792, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2918379)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed8/trial-635e4_00009_9_alpha_d_ff=2,batch_size=8,d_core=512,d_model=512,dropout=0.0022,e_layers=4,learning_rate=0.0014,lradj=cosine_2024-08-26_13-58-18/checkpoint_000005)
[36m(_train_fn pid=2918379)[0m Updating learning rate to 0.00020176794741057493
[36m(_train_fn pid=2918379)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2918379)[0m saving checkpoint...
[36m(_train_fn pid=2918379)[0m Epoch: 6 cost time: 5.311589956283569
[36m(_train_fn pid=2918379)[0m Epoch: 6, Steps: 1057 | Train Loss: 0.6076258 Vali Loss: 0.2690523 Best vali loss: 0.2679482
[36m(_train_fn pid=2918379)[0m Early stopping
[36m(_train_fn pid=2918379)[0m 	iters: 1000, epoch: 6 | loss: 0.2551279
[36m(_train_fn pid=2918379)[0m 	speed: 0.0050s/iter; left time: 10.9014s


Time taken (4 parallel trials): 97 seconds


2024-08-26 13:59:15,190	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:59:15,567	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:59:15,572	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:59:15,580	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:59:18,674	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed9   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator    â”‚
â”‚ Scheduler                        FIFOScheduler            â”‚
â”‚ Number of trials                 10                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed9
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-59-14_762884_2919729/artifacts/2024-08-26_13-59-15/ETTh2_96_96_test_seed9/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 13:59:15. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-9d581_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-9d581_00001   PENDING              32       0.000187329         512              1        256            3   0.00556236   type1   â”‚
â”‚ trial-9d581_00002   PENDING             128       0.0026288           512              3        512            1   0.00261417   cosine  â”‚
â”‚ trial-9d581_00003   PENDING              64       6.74961e-05         128              3         32            1   0.0114695    type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-9d581_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00556 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00019 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-9d581_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-9d581_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00261 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00263 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-9d581_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.01147 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00007 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2921951)[0m configuration
[36m(_train_fn pid=2921951)[0m {'batch_size': 32, 'learning_rate': 0.000187328744532316, 'd_model': 512, 'd_core': 256, 'e_layers': 3, 'dropout': 0.0055623600820537975, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2921951)[0m Use GPU: cuda:0
[36m(_train_fn pid=2921952)[0m {'batch_size': 128, 'learning_rate': 0.002628798526756906, 'd_model': 512, 'd_core': 512, 'e_layers': 1, 'dropout': 0.0026141682638013202, 'lradj': 'cosine', 'd_ff': 1536}
[36m(_train_fn pid=2921951)[0m train 8449
[36m(_train_fn pid=2921951)[0m val 2785
[36m(_train_fn pid=2921951)[0m start_epoch 0
[36m(_train_fn pid=2921951)[0m max_epoch 8
[36m(_train_fn pid=2921953)[0m 	iters: 100, epoch: 1 | loss: 0.2842421
[36m(_train_fn pid=2921953)[0m 	speed: 0.0123s/iter; left time: 11.8418s
[36m(_train_fn pid=2921952)[0m Updating learning rate to 0.0025287458403617175
[36m(_train_fn pid=2921952)[0m saving checkpoint...
[36m(_train_fn pid=2921952)[0m Validation loss decreased (inf --> 0.2551).  Saving model state dict ...
[36m(_train_fn pid=2921952)[0m Epoch: 1 cost time: 0.8828473091125488
[36m(_train_fn pid=2921952)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed9/trial-9d581_00002_2_alpha_d_ff=3,batch_size=128,d_core=512,d_model=512,dropout=0.0026,e_layers=1,learning_rate=0.0026,lradj=cosine_2024-08-26_13-59-15/checkpoint_000000)
2024-08-26 13:59:19,006	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:19,409	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:20,059	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:20,157	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:20,924	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:21,094	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:21,152	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:21,633	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:22,104	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:22,122	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:22,288	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:23,023	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:23,130	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:23,847	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2921950)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed9/trial-9d581_00000_0_2024-08-26_13-59-15/checkpoint_000001)[32m [repeated 14x across cluster][0m
2024-08-26 13:59:23,930	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:24,694	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:24,766	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:25,472	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:25,747	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:26,709	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2921952)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2551466 Best vali loss: 0.2551466
[36m(_train_fn pid=2921953)[0m Epoch: 1, Steps: 133 | Train Loss: 0.2842421 Vali Loss: 0.2455556 Best vali loss: 0.2455556
[36m(_train_fn pid=2921952)[0m Validation loss decreased (0.2551 --> 0.2361).  Saving model state dict ...
[36m(_train_fn pid=2921952)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2361319 Best vali loss: 0.2361319
[36m(_train_fn pid=2921953)[0m Epoch: 2, Steps: 133 | Train Loss: 0.5792442 Vali Loss: 0.2352058 Best vali loss: 0.2352058
[36m(_train_fn pid=2921952)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2252102 Best vali loss: 0.2252102
[36m(_train_fn pid=2921952)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2199028 Best vali loss: 0.2199028
[36m(_train_fn pid=2921953)[0m Epoch: 3, Steps: 133 | Train Loss: 0.2523716 Vali Loss: 0.2311688 Best vali loss: 0.2311688
[36m(_train_fn pid=2921952)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2921952)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2260781 Best vali loss: 0.2199028
[36m(_train_fn pid=2921952)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2921952)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2272163 Best vali loss: 0.2199028
[36m(_train_fn pid=2921950)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2921953)[0m {'batch_size': 64, 'learning_rate': 6.749605641221e-05, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.0114694650279097, 'lradj': 'type1', 'd_ff': 384}
[36m(_train_fn pid=2921953)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2921950)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2921950)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2921950)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2921950)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2921950)[0m max_epoch 8[32m [repeated 3x across cluster][0m

Trial trial-9d581_00002 completed after 7 iterations at 2024-08-26 13:59:23. Total running time: 7s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             0.73252 â”‚
â”‚ time_total_s                                  5.8389 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                               0.2199 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.23554 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2921952)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2921952)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2355420 Best vali loss: 0.2199028
[36m(_train_fn pid=2921952)[0m Early stopping
[36m(_train_fn pid=2921953)[0m 	iters: 100, epoch: 6 | loss: 0.2996429[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2921953)[0m 	speed: 0.0087s/iter; left time: 2.6113s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2921950)[0m Updating learning rate to 0.00025606601717798207[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2921950)[0m saving checkpoint...[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2921951)[0m Validation loss decreased (inf --> 0.2164).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2921950)[0m Epoch: 2 cost time: 2.332824230194092[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2921953)[0m Epoch: 6, Steps: 133 | Train Loss: 0.2996429 Vali Loss: 0.2282455 Best vali loss: 0.2282455[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2921953)[0m Validation loss decreased (0.2287 --> 0.2282).  Saving model state dict ...[32m [repeated 7x across cluster][0m

Trial trial-9d581_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00595 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00021 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2923455)[0m {'batch_size': 16, 'learning_rate': 0.00021388335323464807, 'd_model': 256, 'd_core': 256, 'e_layers': 4, 'dropout': 0.005946724279559878, 'lradj': 'cosine', 'd_ff': 1024}

Trial trial-9d581_00003 completed after 8 iterations at 2024-08-26 13:59:25. Total running time: 9s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.77523 â”‚
â”‚ time_total_s                                 8.28091 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22795 â”‚
â”‚ train_loss                                   0.35631 â”‚
â”‚ valid_loss                                   0.22795 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2921951)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m

Trial trial-9d581_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00874 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00184 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
2024-08-26 13:59:27,437	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:28,284	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:28,374	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:28,538	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:28,688	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:29,091	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2923965)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed9/trial-9d581_00005_5_alpha_d_ff=4,batch_size=128,d_core=512,d_model=64,dropout=0.0087,e_layers=1,learning_rate=0.0018,lradj=type1_2024-08-26_13-59-25/checkpoint_000002)[32m [repeated 12x across cluster][0m
2024-08-26 13:59:29,477	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:29,886	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:30,172	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:30,341	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:30,646	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:31,146	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:31,671	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2923965)[0m {'batch_size': 128, 'learning_rate': 0.0018371805332619146, 'd_model': 64, 'd_core': 512, 'e_layers': 1, 'dropout': 0.008738098480065038, 'lradj': 'type1', 'd_ff': 256}

Trial trial-9d581_00000 completed after 4 iterations at 2024-08-26 13:59:27. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.68776 â”‚
â”‚ time_total_s                                10.27165 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21735 â”‚
â”‚ train_loss                                    0.3672 â”‚
â”‚ valid_loss                                   0.22206 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2923965)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2923965)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2923965)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2923965)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2923965)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2923965)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2921950)[0m Early stopping

Trial trial-9d581_00001 completed after 4 iterations at 2024-08-26 13:59:28. Total running time: 12s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                              1.6615 â”‚
â”‚ time_total_s                                11.21919 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21641 â”‚
â”‚ train_loss                                   0.24289 â”‚
â”‚ valid_loss                                   0.22434 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2921951)[0m Early stopping
[36m(_train_fn pid=2923965)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2197632 Best vali loss: 0.2197632
[36m(_train_fn pid=2923965)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2219468 Best vali loss: 0.2197632

Trial trial-9d581_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00102 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                       0.0013 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2923455)[0m 	iters: 500, epoch: 1 | loss: 0.2697085[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2923455)[0m 	speed: 0.0051s/iter; left time: 19.1964s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2923965)[0m Updating learning rate to 0.0009185902666309573[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2923965)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2923455)[0m Validation loss decreased (inf --> 0.2247).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2923965)[0m Epoch: 2 cost time: 0.29039835929870605[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2923965)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2196040 Best vali loss: 0.2196040
[36m(_train_fn pid=2923965)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2231733 Best vali loss: 0.2196040
[36m(_train_fn pid=2923455)[0m Epoch: 1, Steps: 529 | Train Loss: 0.4081970 Vali Loss: 0.2246957 Best vali loss: 0.2246957[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2923965)[0m Validation loss decreased (0.2198 --> 0.2196).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2923965)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2226632 Best vali loss: 0.2196040

Trial trial-9d581_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00136 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00081 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-9d581_00005 completed after 6 iterations at 2024-08-26 13:59:30. Total running time: 14s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             0.45143 â”‚
â”‚ time_total_s                                  3.3052 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                               0.2196 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.22358 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-9d581_00006 completed after 4 iterations at 2024-08-26 13:59:31. Total running time: 16s
2024-08-26 13:59:31,851	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:31,895	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:32,785	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:33,463	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:33,749	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:34,529	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2925011)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed9/trial-9d581_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=32,dropout=0.0015,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_13-59-30/checkpoint_000001)[32m [repeated 13x across cluster][0m
2024-08-26 13:59:34,864	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:35,119	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:35,687	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:36,136	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:36,773	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:37,464	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:37,910	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:38,425	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:39,087	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:40,164	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2925011)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed9/trial-9d581_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=32,dropout=0.0015,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_13-59-30/checkpoint_000006)[32m [repeated 10x across cluster][0m
2024-08-26 13:59:40,477	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:41,056	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.52246 â”‚
â”‚ time_total_s                                 2.67797 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22524 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.23655 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2924187)[0m EarlyStopping counter: 3 out of 3[32m [repeated 9x across cluster][0m

Trial trial-9d581_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00148 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00009 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2925011)[0m {'batch_size': 32, 'learning_rate': 8.953788236959522e-05, 'd_model': 32, 'd_core': 512, 'e_layers': 1, 'dropout': 0.0014757141905222552, 'lradj': 'cosine', 'd_ff': 64}
[36m(_train_fn pid=2924435)[0m {'batch_size': 32, 'learning_rate': 0.0008144189894313412, 'd_model': 64, 'd_core': 128, 'e_layers': 1, 'dropout': 0.001356842346013679, 'lradj': 'type1', 'd_ff': 192}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2925011)[0m configuration[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2925011)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2925011)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2925011)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2925011)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2925011)[0m max_epoch 8[32m [repeated 3x across cluster][0m

Trial trial-9d581_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00996 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00088 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2924187)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2924187)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2365466 Best vali loss: 0.2252409[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2923455)[0m 	iters: 400, epoch: 3 | loss: 0.5649872[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2923455)[0m 	speed: 0.0053s/iter; left time: 14.7210s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2924435)[0m Updating learning rate to 0.0002036047473578353[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2924435)[0m saving checkpoint...[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2925011)[0m Validation loss decreased (inf --> 0.2613).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2924435)[0m Epoch: 3 cost time: 0.7660934925079346[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2924435)[0m Epoch: 3, Steps: 265 | Train Loss: 0.5625214 Vali Loss: 0.2216254 Best vali loss: 0.2216254[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2924435)[0m Validation loss decreased (0.2226 --> 0.2216).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2924435)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2925323)[0m {'batch_size': 8, 'learning_rate': 0.0008827003260732831, 'd_model': 64, 'd_core': 256, 'e_layers': 3, 'dropout': 0.009961166875846042, 'lradj': 'cosine', 'd_ff': 192}

Trial trial-9d581_00007 completed after 6 iterations at 2024-08-26 13:59:37. Total running time: 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.32461 â”‚
â”‚ time_total_s                                 7.51778 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22163 â”‚
â”‚ train_loss                                   0.41387 â”‚
â”‚ valid_loss                                   0.22368 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2925323)[0m configuration
[36m(_train_fn pid=2925323)[0m Use GPU: cuda:0
[36m(_train_fn pid=2925323)[0m train 8449
[36m(_train_fn pid=2925323)[0m val 2785
[36m(_train_fn pid=2925323)[0m start_epoch 0
[36m(_train_fn pid=2925323)[0m max_epoch 8
[36m(_train_fn pid=2924435)[0m Early stopping
[36m(_train_fn pid=2925323)[0m 	iters: 900, epoch: 1 | loss: 0.1172410[32m [repeated 31x across cluster][0m
[36m(_train_fn pid=2925323)[0m 	speed: 0.0048s/iter; left time: 36.0329s[32m [repeated 31x across cluster][0m
[36m(_train_fn pid=2925011)[0m Updating learning rate to 1.3112519286485513e-05[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2925011)[0m saving checkpoint...[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2925011)[0m Epoch: 6 cost time: 0.8847498893737793[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2925011)[0m Epoch: 6, Steps: 265 | Train Loss: 0.3331765 Vali Loss: 0.2273013 Best vali loss: 0.2273013[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2925011)[0m Validation loss decreased (0.2288 --> 0.2273).  Saving model state dict ...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2925323)[0m Validation loss decreased (inf --> 0.2258).  Saving model state dict ...

Trial trial-9d581_00008 completed after 8 iterations at 2024-08-26 13:59:41. Total running time: 25s
2024-08-26 13:59:41,612	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:45,156	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:50,014	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.88976 â”‚
â”‚ time_total_s                                 9.15331 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22672 â”‚
â”‚ train_loss                                   0.30336 â”‚
â”‚ valid_loss                                   0.22672 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-9d581_00004 completed after 5 iterations at 2024-08-26 13:59:41. Total running time: 26s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             3.18403 â”‚
â”‚ time_total_s                                16.99293 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22019 â”‚
â”‚ train_loss                                   0.26273 â”‚
â”‚ valid_loss                                   0.22652 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2923455)[0m Early stopping
[36m(_train_fn pid=2923455)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2925323)[0m 	iters: 1000, epoch: 2 | loss: 1.3291726[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2925323)[0m 	speed: 0.0037s/iter; left time: 23.5576s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2923455)[0m Updating learning rate to 6.601686874652902e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2923455)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2923455)[0m Epoch: 5 cost time: 2.834787607192993[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2925323)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.3770444 Vali Loss: 0.2202284 Best vali loss: 0.2202284[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2925323)[0m Validation loss decreased (0.2258 --> 0.2202).  Saving model state dict ...[32m [repeated 3x across cluster][0m

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2024-08-26 13:59:45. Total running time: 30s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 9d581_00001 with best_valid_loss=0.21641409640305775 and params={'batch_size': 32, 'learning_rate': 0.000187328744532316, 'd_model': 512, 'd_core': 256, 'e_layers': 3, 'dropout': 0.0055623600820537975, 'lradj': 'type1', 'd_ff': 512}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-9d581_00009   RUNNING                 8       0.0008827            64              3        256            3   0.00996117   cosine         2           11.8989        0.377044       0.220228            0.220228 â”‚
â”‚ trial-9d581_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           10.2716        0.367201       0.222058            0.217349 â”‚
â”‚ trial-9d581_00001   TERMINATED             32       0.000187329         512              1        256            3   0.00556236   type1          4           11.2192        0.242886       0.224336            0.216414 â”‚
â”‚ trial-9d581_00002   TERMINATED            128       0.0026288           512              3        512            1   0.00261417   cosine         7            5.8389      nan              0.235542            0.219903 â”‚
â”‚ trial-9d581_00003   TERMINATED             64       6.74961e-05         128              3         32            1   0.0114695    type1          8            8.28091       0.356311       0.227947            0.227947 â”‚
â”‚ trial-9d581_00004   TERMINATED             16       0.000213883         256              4        256            4   0.00594672   cosine         5           16.9929        0.26273        0.226516            0.220194 â”‚
â”‚ trial-9d581_00005   TERMINATED            128       0.00183718           64              4        512            1   0.0087381    type1          6            3.3052      nan              0.223581            0.219604 â”‚
â”‚ trial-9d581_00006   TERMINATED            128       0.00130432          512              3        128            1   0.00102005   type1          4            2.67797     nan              0.236547            0.225241 â”‚
â”‚ trial-9d581_00007   TERMINATED             32       0.000814419          64              3        128            1   0.00135684   type1          6            7.51778       0.413871       0.223685            0.221625 â”‚
â”‚ trial-9d581_00008   TERMINATED             32       8.95379e-05          32              2        512            1   0.00147571   cosine         8            9.15331       0.303362       0.226721            0.226721 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2925323)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2925323)[0m 	iters: 1000, epoch: 3 | loss: 0.3699044[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2925323)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed9/trial-9d581_00009_9_alpha_d_ff=3,batch_size=8,d_core=256,d_model=64,dropout=0.0100,e_layers=3,learning_rate=0.0009,lradj=cosine_2024-08-26_13-59-31/checkpoint_000002)[32m [repeated 5x across cluster][0m
2024-08-26 13:59:54,760	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:59,626	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:59:59,637	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed9' in 0.0021s.
[36m(_train_fn pid=2925323)[0m 	speed: 0.0041s/iter; left time: 21.6611s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2925323)[0m Updating learning rate to 0.0006102475583023956[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2925323)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2925323)[0m Epoch: 3 cost time: 4.311897277832031[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2925323)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4613075 Vali Loss: 0.2265479 Best vali loss: 0.2202284
[36m(_train_fn pid=2925323)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2925323)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.2801194 Vali Loss: 0.2332260 Best vali loss: 0.2202284
[36m(_train_fn pid=2925323)[0m 	iters: 100, epoch: 5 | loss: 0.1477334[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2925323)[0m 	speed: 0.0122s/iter; left time: 50.3654s[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2925323)[0m Updating learning rate to 0.00044135016303664155
[36m(_train_fn pid=2925323)[0m saving checkpoint...
[36m(_train_fn pid=2925323)[0m Epoch: 4 cost time: 4.202208518981934

Trial trial-9d581_00009 completed after 5 iterations at 2024-08-26 13:59:59. Total running time: 44s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9d581_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             4.86387 â”‚
â”‚ time_total_s                                26.36174 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22023 â”‚
â”‚ train_loss                                   0.41962 â”‚
â”‚ valid_loss                                   0.23215 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:59:59. Total running time: 44s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 9d581_00001 with best_valid_loss=0.21641409640305775 and params={'batch_size': 32, 'learning_rate': 0.000187328744532316, 'd_model': 512, 'd_core': 256, 'e_layers': 3, 'dropout': 0.0055623600820537975, 'lradj': 'type1', 'd_ff': 512}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-9d581_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           10.2716        0.367201       0.222058            0.217349 â”‚
â”‚ trial-9d581_00001   TERMINATED             32       0.000187329         512              1        256            3   0.00556236   type1          4           11.2192        0.242886       0.224336            0.216414 â”‚
â”‚ trial-9d581_00002   TERMINATED            128       0.0026288           512              3        512            1   0.00261417   cosine         7            5.8389      nan              0.235542            0.219903 â”‚
â”‚ trial-9d581_00003   TERMINATED             64       6.74961e-05         128              3         32            1   0.0114695    type1          8            8.28091       0.356311       0.227947            0.227947 â”‚
â”‚ trial-9d581_00004   TERMINATED             16       0.000213883         256              4        256            4   0.00594672   cosine         5           16.9929        0.26273        0.226516            0.220194 â”‚
â”‚ trial-9d581_00005   TERMINATED            128       0.00183718           64              4        512            1   0.0087381    type1          6            3.3052      nan              0.223581            0.219604 â”‚
â”‚ trial-9d581_00006   TERMINATED            128       0.00130432          512              3        128            1   0.00102005   type1          4            2.67797     nan              0.236547            0.225241 â”‚
â”‚ trial-9d581_00007   TERMINATED             32       0.000814419          64              3        128            1   0.00135684   type1          6            7.51778       0.413871       0.223685            0.221625 â”‚
â”‚ trial-9d581_00008   TERMINATED             32       8.95379e-05          32              2        512            1   0.00147571   cosine         8            9.15331       0.303362       0.226721            0.226721 â”‚
â”‚ trial-9d581_00009   TERMINATED              8       0.0008827            64              3        256            3   0.00996117   cosine         5           26.3617        0.41962        0.232147            0.220228 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 32, 'learning_rate': 0.000187328744532316, 'd_model': 512, 'd_core': 256, 'e_layers': 3, 'dropout': 0.0055623600820537975, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2925323)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed9/trial-9d581_00009_9_alpha_d_ff=3,batch_size=8,d_core=256,d_model=64,dropout=0.0100,e_layers=3,learning_rate=0.0009,lradj=cosine_2024-08-26_13-59-31/checkpoint_000004)[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2925323)[0m Updating learning rate to 0.0002724527677708876
[36m(_train_fn pid=2925323)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2925323)[0m saving checkpoint...
[36m(_train_fn pid=2925323)[0m Epoch: 5 cost time: 4.311009645462036
[36m(_train_fn pid=2925323)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.4196196 Vali Loss: 0.2321470 Best vali loss: 0.2202284
[36m(_train_fn pid=2925323)[0m Early stopping
[36m(_train_fn pid=2925323)[0m 	iters: 1000, epoch: 5 | loss: 1.0169078[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2925323)[0m 	speed: 0.0040s/iter; left time: 12.8957s[32m [repeated 9x across cluster][0m


Time taken (4 parallel trials): 48 seconds


2024-08-26 14:00:03,671	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:00:04,066	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:00:04,071	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:00:04,079	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:00:08,850	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2929320)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10/trial-ba3db_00001_1_alpha_d_ff=4,batch_size=64,d_core=512,d_model=512,dropout=0.0032,e_layers=3,learning_rate=0.0001,lradj=cosine_2024-08-26_14-00-04/checkpoint_000000)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed10   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator     â”‚
â”‚ Scheduler                        FIFOScheduler             â”‚
â”‚ Number of trials                 10                        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-00-03_243455_2927096/artifacts/2024-08-26_14-00-04/ETTh2_96_96_test_seed10/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:00:04. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-ba3db_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-ba3db_00001   PENDING              64       0.000130117         512              4        512            3   0.00320771   cosine  â”‚
â”‚ trial-ba3db_00002   PENDING               8       0.00119411           32              2        512            1   0.0106788    cosine  â”‚
â”‚ trial-ba3db_00003   PENDING              16       0.000237356         512              3        128            4   0.00751346   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ba3db_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00321 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00013 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2929320)[0m configuration
[36m(_train_fn pid=2929320)[0m {'batch_size': 64, 'learning_rate': 0.00013011687504587557, 'd_model': 512, 'd_core': 512, 'e_layers': 3, 'dropout': 0.003207709379107476, 'lradj': 'cosine', 'd_ff': 2048}

Trial trial-ba3db_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ba3db_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.01068 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00119 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ba3db_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00751 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00024 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2929321)[0m Use GPU: cuda:0
[36m(_train_fn pid=2929322)[0m {'batch_size': 16, 'learning_rate': 0.0002373562266906166, 'd_model': 512, 'd_core': 128, 'e_layers': 4, 'dropout': 0.007513455138487355, 'lradj': 'type1', 'd_ff': 1536}
[36m(_train_fn pid=2929321)[0m train 8449
[36m(_train_fn pid=2929321)[0m val 2785
[36m(_train_fn pid=2929321)[0m start_epoch 0
[36m(_train_fn pid=2929321)[0m max_epoch 8
[36m(_train_fn pid=2929321)[0m 	iters: 100, epoch: 1 | loss: 0.2620414
[36m(_train_fn pid=2929321)[0m 	speed: 0.0162s/iter; left time: 135.1662s
[36m(_train_fn pid=2929320)[0m Updating learning rate to 0.0001251645963675443
[36m(_train_fn pid=2929320)[0m saving checkpoint...
[36m(_train_fn pid=2929320)[0m Validation loss decreased (inf --> 0.2194).  Saving model state dict ...
[36m(_train_fn pid=2929320)[0m Epoch: 1 cost time: 2.2972733974456787
2024-08-26 14:00:09,652	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:11,192	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:13,082	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:13,594	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2929320)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10/trial-ba3db_00001_1_alpha_d_ff=4,batch_size=64,d_core=512,d_model=512,dropout=0.0032,e_layers=3,learning_rate=0.0001,lradj=cosine_2024-08-26_14-00-04/checkpoint_000003)[32m [repeated 5x across cluster][0m
2024-08-26 14:00:16,004	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:16,270	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:17,315	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:18,016	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:18,073	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:21,014	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2930596)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10/trial-ba3db_00005_5_alpha_d_ff=3,batch_size=64,d_core=64,d_model=64,dropout=0.0033,e_layers=1,learning_rate=0.0025,lradj=type1_2024-08-26_14-00-18/checkpoint_000000)[32m [repeated 5x across cluster][0m
2024-08-26 14:00:21,696	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:22,076	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:22,414	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2929320)[0m Epoch: 1, Steps: 133 | Train Loss: 0.2865166 Vali Loss: 0.2194044 Best vali loss: 0.2194044
[36m(_train_fn pid=2929322)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2929319)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2929322)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2929320)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2929322)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2929322)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2929322)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2929322)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2929322)[0m 	iters: 300, epoch: 1 | loss: 0.3816100[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2929322)[0m 	speed: 0.0200s/iter; left time: 78.5708s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2929320)[0m Updating learning rate to 8.995522369852538e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2929320)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2929319)[0m Validation loss decreased (inf --> 0.2209).  Saving model state dict ...
[36m(_train_fn pid=2929320)[0m Epoch: 3 cost time: 2.0581727027893066[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2929320)[0m Epoch: 3, Steps: 133 | Train Loss: 0.5295755 Vali Loss: 0.2210726 Best vali loss: 0.2194044[32m [repeated 4x across cluster][0m

Trial trial-ba3db_00001 completed after 4 iterations at 2024-08-26 14:00:16. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.40748 â”‚
â”‚ time_total_s                                10.33166 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                               0.2194 â”‚
â”‚ train_loss                                   0.25446 â”‚
â”‚ valid_loss                                   0.22363 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2929320)[0m Early stopping
[36m(_train_fn pid=2929319)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2929322)[0m Validation loss decreased (inf --> 0.2216).  Saving model state dict ...

Trial trial-ba3db_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00932 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                       0.0001 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2930326)[0m configuration
[36m(_train_fn pid=2930326)[0m {'batch_size': 8, 'learning_rate': 9.748984821147534e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 2, 'dropout': 0.009316451622308177, 'lradj': 'cosine', 'd_ff': 2048}
[36m(_train_fn pid=2930326)[0m Use GPU: cuda:0
[36m(_train_fn pid=2929319)[0m 	iters: 200, epoch: 4 | loss: 0.2562964[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2929319)[0m 	speed: 0.0041s/iter; left time: 4.5774s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2930326)[0m train 8449
[36m(_train_fn pid=2930326)[0m val 2785

Trial trial-ba3db_00000 completed after 4 iterations at 2024-08-26 14:00:18. Total running time: 13s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.74351 â”‚
â”‚ time_total_s                                12.29024 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22089 â”‚
â”‚ train_loss                                   0.36643 â”‚
â”‚ valid_loss                                   0.22882 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2930326)[0m start_epoch 0
[36m(_train_fn pid=2930326)[0m max_epoch 8
[36m(_train_fn pid=2929321)[0m Updating learning rate to 0.0011486582670012194[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2929321)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2929321)[0m Epoch: 1 cost time: 10.537485837936401[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2929321)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.5680445 Vali Loss: 0.2177283 Best vali loss: 0.2177283[32m [repeated 5x across cluster][0m

Trial trial-ba3db_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00325 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00254 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2930596)[0m {'batch_size': 64, 'learning_rate': 0.002543604028557193, 'd_model': 64, 'd_core': 64, 'e_layers': 1, 'dropout': 0.003251698564377634, 'lradj': 'type1', 'd_ff': 192}
[36m(_train_fn pid=2929319)[0m Early stopping
[36m(_train_fn pid=2929319)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2930596)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2930596)[0m Validation loss decreased (inf --> 0.2163).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2930596)[0m configuration
[36m(_train_fn pid=2930596)[0m Use GPU: cuda:0
[36m(_train_fn pid=2930596)[0m 	iters: 100, epoch: 4 | loss: 0.3191159[32m [repeated 28x across cluster][0m
2024-08-26 14:00:23,198	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:23,373	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:24,621	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:26,212	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2931167)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10/trial-ba3db_00006_6_alpha_d_ff=4,batch_size=64,d_core=256,d_model=64,dropout=0.0050,e_layers=1,learning_rate=0.0002,lradj=cosine_2024-08-26_14-00-23/checkpoint_000000)[32m [repeated 7x across cluster][0m
2024-08-26 14:00:26,646	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:26,952	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:27,709	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:28,293	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:28,525	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:29,331	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:30,100	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:30,870	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:31,495	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:31,572	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2929321)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10/trial-ba3db_00002_2_alpha_d_ff=2,batch_size=8,d_core=512,d_model=32,dropout=0.0107,e_layers=1,learning_rate=0.0012,lradj=cosine_2024-08-26_14-00-04/checkpoint_000002)[32m [repeated 9x across cluster][0m
2024-08-26 14:00:31,755	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:33,176	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2930596)[0m 	speed: 0.0079s/iter; left time: 4.4690s[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=2930596)[0m train 8449
[36m(_train_fn pid=2930596)[0m val 2785

Trial trial-ba3db_00005 completed after 4 iterations at 2024-08-26 14:00:23. Total running time: 19s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.78137 â”‚
â”‚ time_total_s                                 3.66336 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21632 â”‚
â”‚ train_loss                                   0.31912 â”‚
â”‚ valid_loss                                   0.22127 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2930596)[0m Early stopping
[36m(_train_fn pid=2930596)[0m start_epoch 0
[36m(_train_fn pid=2930596)[0m max_epoch 8
[36m(_train_fn pid=2930326)[0m Updating learning rate to 9.377936180084475e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2930326)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2930326)[0m Epoch: 1 cost time: 4.590348482131958[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2930326)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.3283698 Vali Loss: 0.2160139 Best vali loss: 0.2160139[32m [repeated 6x across cluster][0m

Trial trial-ba3db_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00503 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00018 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2931167)[0m configuration
[36m(_train_fn pid=2931167)[0m {'batch_size': 64, 'learning_rate': 0.00018358493440440702, 'd_model': 64, 'd_core': 256, 'e_layers': 1, 'dropout': 0.0050262360121312655, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2931167)[0m Use GPU: cuda:0
[36m(_train_fn pid=2931167)[0m train 8449
[36m(_train_fn pid=2931167)[0m val 2785
[36m(_train_fn pid=2931167)[0m start_epoch 0
[36m(_train_fn pid=2931167)[0m max_epoch 8
[36m(_train_fn pid=2929322)[0m EarlyStopping counter: 2 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2931167)[0m Validation loss decreased (0.2339 --> 0.2234).  Saving model state dict ...
[36m(_train_fn pid=2931167)[0m Validation loss decreased (inf --> 0.2339).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2931167)[0m Validation loss decreased (0.2234 --> 0.2200).  Saving model state dict ...
[36m(_train_fn pid=2929321)[0m 	iters: 700, epoch: 3 | loss: 0.2745217[32m [repeated 27x across cluster][0m
[36m(_train_fn pid=2929321)[0m 	speed: 0.0060s/iter; left time: 33.6042s[32m [repeated 27x across cluster][0m
[36m(_train_fn pid=2931167)[0m Validation loss decreased (0.2200 --> 0.2189).  Saving model state dict ...
[36m(_train_fn pid=2931167)[0m Validation loss decreased (0.2189 --> 0.2179).  Saving model state dict ...
[36m(_train_fn pid=2931167)[0m Updating learning rate to 5.666501078800435e-05[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2931167)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2931167)[0m Epoch: 5 cost time: 0.606957197189331[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2931167)[0m Epoch: 5, Steps: 133 | Train Loss: 0.3257131 Vali Loss: 0.2178653 Best vali loss: 0.2178653[32m [repeated 8x across cluster][0m

Trial trial-ba3db_00006 completed after 8 iterations at 2024-08-26 14:00:31. Total running time: 27s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.69966 â”‚
â”‚ time_total_s                                 6.80596 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21787 â”‚
â”‚ train_loss                                   0.22605 â”‚
â”‚ valid_loss                                   0.21825 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2931167)[0m Early stopping

Trial trial-ba3db_00003 completed after 4 iterations at 2024-08-26 14:00:31. Total running time: 27s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             5.10634 â”‚
â”‚ time_total_s                                25.99801 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22159 â”‚
â”‚ train_loss                                   0.32353 â”‚
â”‚ valid_loss                                   0.23112 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2929322)[0m EarlyStopping counter: 3 out of 3[32m [repeated 6x across cluster][0m

Trial trial-ba3db_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00163 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00016 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2929321)[0m 	iters: 700, epoch: 4 | loss: 0.2427548[32m [repeated 29x across cluster][0m
2024-08-26 14:00:34,816	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:34,996	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:35,956	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2929321)[0m 	speed: 0.0021s/iter; left time: 9.6650s[32m [repeated 29x across cluster][0m
[36m(_train_fn pid=2932212)[0m configuration
[36m(_train_fn pid=2932212)[0m {'batch_size': 16, 'learning_rate': 0.000157950513858545, 'd_model': 32, 'd_core': 512, 'e_layers': 2, 'dropout': 0.0016263198737349487, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2932212)[0m Use GPU: cuda:0

Trial trial-ba3db_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00464 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00009 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2932212)[0m train 8449
[36m(_train_fn pid=2932212)[0m val 2785
[36m(_train_fn pid=2932218)[0m {'batch_size': 32, 'learning_rate': 9.387464405668754e-05, 'd_model': 32, 'd_core': 64, 'e_layers': 1, 'dropout': 0.0046394467235250075, 'lradj': 'cosine', 'd_ff': 32}
[36m(_train_fn pid=2932212)[0m start_epoch 0
[36m(_train_fn pid=2932212)[0m max_epoch 8

Trial status: 5 TERMINATED | 4 RUNNING
Current time: 2024-08-26 14:00:34. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: ba3db_00004 with best_valid_loss=0.21601388387823448 and params={'batch_size': 8, 'learning_rate': 9.748984821147534e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 2, 'dropout': 0.009316451622308177, 'lradj': 'cosine', 'd_ff': 2048}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-ba3db_00002   RUNNING                 8       0.00119411           32              2        512            1   0.0106788    cosine         3           25.7689        0.239776       0.227834            0.217728 â”‚
â”‚ trial-ba3db_00004   RUNNING                 8       9.74898e-05         512              4        256            2   0.00931645   cosine         3           15.4984        0.299908       0.226347            0.216014 â”‚
â”‚ trial-ba3db_00007   RUNNING                16       0.000157951          32              4        512            2   0.00162632   type1                                                                                 â”‚
â”‚ trial-ba3db_00008   RUNNING                32       9.38746e-05          32              1         64            1   0.00463945   cosine                                                                                â”‚
â”‚ trial-ba3db_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           12.2902        0.36643        0.228816            0.220891 â”‚
â”‚ trial-ba3db_00001   TERMINATED             64       0.000130117         512              4        512            3   0.00320771   cosine         4           10.3317        0.254455       0.223628            0.219404 â”‚
â”‚ trial-ba3db_00003   TERMINATED             16       0.000237356         512              3        128            4   0.00751346   type1          4           25.998         0.323533       0.231121            0.221589 â”‚
â”‚ trial-ba3db_00005   TERMINATED             64       0.0025436            64              3         64            1   0.0032517    type1          4            3.66336       0.319116       0.221265            0.216318 â”‚
â”‚ trial-ba3db_00006   TERMINATED             64       0.000183585          64              4        256            1   0.00502624   cosine         8            6.80596       0.226051       0.218251            0.217865 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2930326)[0m Updating learning rate to 6.739879897289717e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2930326)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2930326)[0m Epoch: 3 cost time: 4.320660591125488[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2930326)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.2999078 Vali Loss: 0.2263474 Best vali loss: 0.2160139[32m [repeated 6x across cluster][0m

Trial trial-ba3db_00002 completed after 4 iterations at 2024-08-26 14:00:34. Total running time: 30s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             3.31809 â”‚
â”‚ time_total_s                                29.08698 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21773 â”‚
â”‚ train_loss                                   0.43758 â”‚
â”‚ valid_loss                                   0.22665 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2932218)[0m Validation loss decreased (inf --> 0.2647).  Saving model state dict ...
[36m(_train_fn pid=2932218)[0m Validation loss decreased (0.2647 --> 0.2466).  Saving model state dict ...

2024-08-26 14:00:36,462	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:36,874	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2932218)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10/trial-ba3db_00008_8_alpha_d_ff=1,batch_size=32,d_core=64,d_model=32,dropout=0.0046,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_14-00-31/checkpoint_000002)[32m [repeated 8x across cluster][0m
2024-08-26 14:00:37,713	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:37,995	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:38,219	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:38,353	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:39,007	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:39,593	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:39,602	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:39,798	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:40,760	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:41,870	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:42,176	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2932212)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10/trial-ba3db_00007_7_alpha_d_ff=4,batch_size=16,d_core=512,d_model=32,dropout=0.0016,e_layers=2,learning_rate=0.0002,lradj=type1_2024-08-26_14-00-31/checkpoint_000002)[32m [repeated 11x across cluster][0m
2024-08-26 14:00:42,817	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:44,110	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:45,901	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:47,735	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2932212)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10/trial-ba3db_00007_7_alpha_d_ff=4,batch_size=16,d_core=512,d_model=32,dropout=0.0016,e_layers=2,learning_rate=0.0002,lradj=type1_2024-08-26_14-00-31/checkpoint_000005)[32m [repeated 4x across cluster][0m
2024-08-26 14:00:49,812	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
Trial trial-ba3db_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00276 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00178 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2929321)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2932218)[0m Validation loss decreased (0.2466 --> 0.2375).  Saving model state dict ...
[36m(_train_fn pid=2929321)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m

Trial trial-ba3db_00004 completed after 4 iterations at 2024-08-26 14:00:37. Total running time: 33s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             4.81715 â”‚
â”‚ time_total_s                                20.31557 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21601 â”‚
â”‚ train_loss                                   0.30279 â”‚
â”‚ valid_loss                                   0.22511 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2932218)[0m Validation loss decreased (0.2375 --> 0.2323).  Saving model state dict ...
[36m(_train_fn pid=2932581)[0m 	iters: 100, epoch: 2 | loss: 0.4449706[32m [repeated 31x across cluster][0m
[36m(_train_fn pid=2932581)[0m 	speed: 0.0065s/iter; left time: 5.4103s[32m [repeated 31x across cluster][0m
[36m(_train_fn pid=2932581)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2932581)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2932581)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2932581)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2932581)[0m {'batch_size': 64, 'learning_rate': 0.00178251789440524, 'd_model': 512, 'd_core': 64, 'e_layers': 1, 'dropout': 0.0027579506635571453, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=2932581)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2932581)[0m max_epoch 8[32m [repeated 2x across cluster][0m

Trial trial-ba3db_00009 completed after 4 iterations at 2024-08-26 14:00:39. Total running time: 35s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.59345 â”‚
â”‚ time_total_s                                 3.22948 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22731 â”‚
â”‚ train_loss                                   0.40225 â”‚
â”‚ valid_loss                                   0.24885 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2932218)[0m Validation loss decreased (0.2323 --> 0.2294).  Saving model state dict ...
[36m(_train_fn pid=2932218)[0m Updating learning rate to 2.897518652851164e-05[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2932218)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2932218)[0m Epoch: 5 cost time: 1.0192182064056396[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2932218)[0m Epoch: 5, Steps: 265 | Train Loss: 0.6472332 Vali Loss: 0.2294317 Best vali loss: 0.2294317[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2932581)[0m Validation loss decreased (inf --> 0.2273).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2932218)[0m Validation loss decreased (0.2294 --> 0.2279).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2932581)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2932581)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m

Trial trial-ba3db_00008 completed after 8 iterations at 2024-08-26 14:00:42. Total running time: 38s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.94412 â”‚
â”‚ time_total_s                                 9.50751 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22718 â”‚
â”‚ train_loss                                    0.3388 â”‚
â”‚ valid_loss                                   0.22718 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2932212)[0m 	iters: 400, epoch: 4 | loss: 0.3200999[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2932212)[0m 	speed: 0.0028s/iter; left time: 6.2068s[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2932212)[0m Updating learning rate to 1.9743814232318126e-05[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2932212)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2932212)[0m Epoch: 4 cost time: 1.6457042694091797[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2932212)[0m Epoch: 4, Steps: 529 | Train Loss: 0.3454494 Vali Loss: 0.2210241 Best vali loss: 0.2210241[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2932212)[0m Validation loss decreased (0.2211 --> 0.2210).  Saving model state dict ...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2932212)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2932212)[0m 	iters: 300, epoch: 7 | loss: 0.3462768[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2932212)[0m 	speed: 0.0033s/iter; left time: 2.5286s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2932212)[0m Updating learning rate to 2.4679767790397657e-06[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2932212)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2932212)[0m Epoch: 7 cost time: 1.7813503742218018[32m [repeated 3x across cluster][0m
2024-08-26 14:00:51,644	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:00:51,647	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10' in 0.0021s.
[36m(_train_fn pid=2932212)[0m Epoch: 7, Steps: 529 | Train Loss: 0.3548394 Vali Loss: 0.2209544 Best vali loss: 0.2209544[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2932212)[0m Validation loss decreased (0.2210 --> 0.2210).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-ba3db_00007 completed after 8 iterations at 2024-08-26 14:00:51. Total running time: 47s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ba3db_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.82894 â”‚
â”‚ time_total_s                                18.49309 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22095 â”‚
â”‚ train_loss                                   0.40842 â”‚
â”‚ valid_loss                                   0.22099 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:00:51. Total running time: 47s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: ba3db_00004 with best_valid_loss=0.21601388387823448 and params={'batch_size': 8, 'learning_rate': 9.748984821147534e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 2, 'dropout': 0.009316451622308177, 'lradj': 'cosine', 'd_ff': 2048}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-ba3db_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           12.2902        0.36643        0.228816            0.220891 â”‚
â”‚ trial-ba3db_00001   TERMINATED             64       0.000130117         512              4        512            3   0.00320771   cosine         4           10.3317        0.254455       0.223628            0.219404 â”‚
â”‚ trial-ba3db_00002   TERMINATED              8       0.00119411           32              2        512            1   0.0106788    cosine         4           29.087         0.43758        0.22665             0.217728 â”‚
â”‚ trial-ba3db_00003   TERMINATED             16       0.000237356         512              3        128            4   0.00751346   type1          4           25.998         0.323533       0.231121            0.221589 â”‚
â”‚ trial-ba3db_00004   TERMINATED              8       9.74898e-05         512              4        256            2   0.00931645   cosine         4           20.3156        0.302788       0.225107            0.216014 â”‚
â”‚ trial-ba3db_00005   TERMINATED             64       0.0025436            64              3         64            1   0.0032517    type1          4            3.66336       0.319116       0.221265            0.216318 â”‚
â”‚ trial-ba3db_00006   TERMINATED             64       0.000183585          64              4        256            1   0.00502624   cosine         8            6.80596       0.226051       0.218251            0.217865 â”‚
â”‚ trial-ba3db_00007   TERMINATED             16       0.000157951          32              4        512            2   0.00162632   type1          8           18.4931        0.408422       0.220994            0.220954 â”‚
â”‚ trial-ba3db_00008   TERMINATED             32       9.38746e-05          32              1         64            1   0.00463945   cosine         8            9.50751       0.338799       0.227178            0.227178 â”‚
â”‚ trial-ba3db_00009   TERMINATED             64       0.00178252          512              1         64            1   0.00275795   cosine         4            3.22948       0.402249       0.248854            0.227306 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 9.748984821147534e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 2, 'dropout': 0.009316451622308177, 'lradj': 'cosine', 'd_ff': 2048}
[36m(_train_fn pid=2932212)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2932212)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10/trial-ba3db_00007_7_alpha_d_ff=4,batch_size=16,d_core=512,d_model=32,dropout=0.0016,e_layers=2,learning_rate=0.0002,lradj=type1_2024-08-26_14-00-31/checkpoint_000007)[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2932212)[0m 	iters: 500, epoch: 8 | loss: 0.3127838[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2932212)[0m 	speed: 0.0028s/iter; left time: 0.0844s[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2932212)[0m Updating learning rate to 1.2339883895198829e-06
[36m(_train_fn pid=2932212)[0m saving checkpoint...
[36m(_train_fn pid=2932212)[0m Epoch: 8 cost time: 1.55202317237854
[36m(_train_fn pid=2932212)[0m Epoch: 8, Steps: 529 | Train Loss: 0.4084216 Vali Loss: 0.2209936 Best vali loss: 0.2209544


Time taken (4 parallel trials): 52 seconds


N=10
for ((i=1; i<=N; i++))
do
horizon=96
maxconcurrent=4
gpu_fraction=$(echo "scale=2; 1/$maxconcurrent" | bc)  # Calculate GPU fraction with 2 decimal places
start_time=$(date +%s)  # Get the current time in seconds
python3 tune_softs.py \
    --model SOFTS \
    --data ETTh2 \
    --root_path ./dataset/ETT-small/ \
    --data_path ETTh2.csv \
    --features M \
    --seq_len 96 \
    --label_len 48 \
    --pred_len $horizon \
    --enc_in 7 \
    --dec_in 7 \
    --c_out 7 \
    --embed timeF \
    --activation gelu \
    --train_epochs 8 \
    --patience 3 \
    --loss MSE \
    --num_workers 1 \
    --gpu 0 \
    --tune_search_algorithm random_search \
    --tune_trial_scheduler fifo \
    --tune_storage_path ./checkpoints/hptunning/random_search/ \
    --tune_experiment_name ETTh2_96_${horizon}_test_seed10${i} \
    --tune_objective best_valid_loss \
    --tune_num_samples 10 \
    --tune_max_trial_time_s 70 \
    --tune_time_budget_s 14400 \
    --tune_max_concurrent $maxconcurrent \
    --tune_gpu_resources $gpu_fraction \
    --tune_cpu_resources 1 \
    --tune_default_config "{
        \"batch_size\": 32, \
        \"learning_rate\": 0.0003, \
        \"d_model\": 128, \
        \"alpha_d_ff\": 1, \
        \"d_core\": 64, \
        \"e_layers\": 2, \
        \"dropout\": 0.0, \
        \"lradj\": \"cosine\"
    }" \
    --tune_param_space "{
        \"batch_size\": [\"choice\", [8, 16, 32, 64, 128]], \
        \"learning_rate\": [\"loguniform\", [0.00005, 0.005]], \
        \"d_model\": [\"choice\", [32, 64, 128, 256, 512]], \
        \"alpha_d_ff\": [\"choice\", [1, 2, 3, 4]], \
        \"d_core\": [\"choice\", [32, 64, 128, 256, 512]], \
        \"e_layers\": [\"choice\", [1, 2, 3, 4]], \
        \"dropout\": [\"loguniform\", [0.0008, 0.012]], \
        \"lradj\": [\"choice\", [\"cosine\", \"type1\"]]
    }" \
    --seed $((100+i));
end_time=$(date +%s)  # Get the current time in seconds
elapsed_time=$((end_time - start_time))  # Calculate the elapsed time
echo ""
echo ""
echo "Time taken ($maxconcurrent parallel trials): $elapsed_time seconds"
echo ""
echo ""
done2024-08-26 14:02:26,175	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:02:26,544	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:02:26,549	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:02:26,556	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:02:30,111	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2936906)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed101/trial-0f2dd_00003_3_alpha_d_ff=1,batch_size=128,d_core=128,d_model=512,dropout=0.0090,e_layers=3,learning_rate=0.0012,lradj=cosine_2024-08-26_14-02-26/checkpoint_000000)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed101   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator      â”‚
â”‚ Scheduler                        FIFOScheduler              â”‚
â”‚ Number of trials                 10                         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed101
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-02-25_747381_2934667/artifacts/2024-08-26_14-02-26/ETTh2_96_96_test_seed101/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:02:26. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-0f2dd_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-0f2dd_00001   PENDING              16       0.000261705         512              1        256            2   0.0017752    type1   â”‚
â”‚ trial-0f2dd_00002   PENDING              16       0.00273926          512              1         64            1   0.00146944   type1   â”‚
â”‚ trial-0f2dd_00003   PENDING             128       0.00115017          512              1        128            3   0.00902515   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-0f2dd_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-0f2dd_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00903 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00115 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-0f2dd_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00178 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00026 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-0f2dd_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00147 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00274 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2936906)[0m configuration
[36m(_train_fn pid=2936906)[0m {'batch_size': 128, 'learning_rate': 0.0011501711420679502, 'd_model': 512, 'd_core': 128, 'e_layers': 3, 'dropout': 0.00902514799561019, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=2936906)[0m Use GPU: cuda:0
[36m(_train_fn pid=2936904)[0m {'batch_size': 16, 'learning_rate': 0.0002617050306691626, 'd_model': 512, 'd_core': 256, 'e_layers': 2, 'dropout': 0.0017751997787905807, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2936906)[0m train 8449
[36m(_train_fn pid=2936903)[0m val 2785
[36m(_train_fn pid=2936906)[0m start_epoch 0
[36m(_train_fn pid=2936906)[0m max_epoch 8
[36m(_train_fn pid=2936903)[0m 	iters: 100, epoch: 1 | loss: 0.7890191
[36m(_train_fn pid=2936903)[0m 	speed: 0.0166s/iter; left time: 33.5709s
[36m(_train_fn pid=2936906)[0m Updating learning rate to 0.0011063953595548304
[36m(_train_fn pid=2936906)[0m saving checkpoint...
[36m(_train_fn pid=2936906)[0m Validation loss decreased (inf --> 0.2270).  Saving model state dict ...
[36m(_train_fn pid=2936906)[0m Epoch: 1 cost time: 1.320951223373413
2024-08-26 14:02:31,351	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2936906)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed101/trial-0f2dd_00003_3_alpha_d_ff=1,batch_size=128,d_core=128,d_model=512,dropout=0.0090,e_layers=3,learning_rate=0.0012,lradj=cosine_2024-08-26_14-02-26/checkpoint_000001)
2024-08-26 14:02:32,026	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:32,562	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:33,761	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:34,520	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:34,576	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:35,284	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2936904)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed101/trial-0f2dd_00001_1_alpha_d_ff=1,batch_size=16,d_core=256,d_model=512,dropout=0.0018,e_layers=2,learning_rate=0.0003,lradj=type1_2024-08-26_14-02-26/checkpoint_000000)[32m [repeated 6x across cluster][0m
2024-08-26 14:02:36,175	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:36,710	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:37,361	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:37,855	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:37,982	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:38,048	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:38,675	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:39,160	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:39,305	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:39,539	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:39,975	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:40,636	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2936906)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2270386 Best vali loss: 0.2270386
[36m(_train_fn pid=2936906)[0m Updating learning rate to 0.0009817324780746369
[36m(_train_fn pid=2936906)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2936906)[0m saving checkpoint...
[36m(_train_fn pid=2936906)[0m Epoch: 2 cost time: 1.0699164867401123
[36m(_train_fn pid=2936906)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2319976 Best vali loss: 0.2270386
[36m(_train_fn pid=2936903)[0m Epoch: 1, Steps: 265 | Train Loss: 0.6269687 Vali Loss: 0.2196808 Best vali loss: 0.2196808
[36m(_train_fn pid=2936906)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2936906)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2330570 Best vali loss: 0.2270386

Trial trial-0f2dd_00003 completed after 4 iterations at 2024-08-26 14:02:33. Total running time: 7s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.19615 â”‚
â”‚ time_total_s                                 5.60858 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22704 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.24075 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2936906)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2936906)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2407519 Best vali loss: 0.2270386
[36m(_train_fn pid=2936906)[0m Early stopping
[36m(_train_fn pid=2936905)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2936903)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2936905)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2936905)[0m {'batch_size': 16, 'learning_rate': 0.002739258977729537, 'd_model': 512, 'd_core': 64, 'e_layers': 1, 'dropout': 0.0014694374018215123, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2936905)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2936905)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2936905)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2936905)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2936903)[0m Validation loss decreased (0.2197 --> 0.2178).  Saving model state dict ...
[36m(_train_fn pid=2936903)[0m Epoch: 2, Steps: 265 | Train Loss: 0.4233832 Vali Loss: 0.2177826 Best vali loss: 0.2177826
[36m(_train_fn pid=2936903)[0m 	iters: 100, epoch: 3 | loss: 0.2408213[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2936903)[0m 	speed: 0.0110s/iter; left time: 16.4573s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2936904)[0m Updating learning rate to 0.0002617050306691626[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2936904)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2936904)[0m Validation loss decreased (inf --> 0.2189).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2936904)[0m Epoch: 1 cost time: 5.926957368850708[32m [repeated 6x across cluster][0m

Trial trial-0f2dd_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00205 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                       0.0003 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2937801)[0m {'batch_size': 64, 'learning_rate': 0.0003017077872045259, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.002047514961139705, 'lradj': 'type1', 'd_ff': 256}
[36m(_train_fn pid=2936903)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2937801)[0m Epoch: 1, Steps: 133 | Train Loss: 0.5094968 Vali Loss: 0.2205267 Best vali loss: 0.2205267[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2936905)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2937801)[0m configuration
[36m(_train_fn pid=2937801)[0m Use GPU: cuda:0
[36m(_train_fn pid=2937801)[0m train 8449
[36m(_train_fn pid=2937801)[0m val 2785
[36m(_train_fn pid=2937801)[0m start_epoch 0
[36m(_train_fn pid=2937801)[0m max_epoch 8

Trial trial-0f2dd_00000 completed after 5 iterations at 2024-08-26 14:02:39. Total running time: 12s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.55386 â”‚
â”‚ time_total_s                                11.41339 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21778 â”‚
â”‚ train_loss                                   0.25101 â”‚
â”‚ valid_loss                                   0.22451 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2936903)[0m Early stopping
[36m(_train_fn pid=2937801)[0m Validation loss decreased (0.2177 --> 0.2165).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2936905)[0m 	iters: 500, epoch: 3 | loss: 0.2511215[32m [repeated 27x across cluster][0m
[36m(_train_fn pid=2936905)[0m 	speed: 0.0045s/iter; left time: 12.0449s[32m [repeated 27x across cluster][0m
[36m(_train_fn pid=2937801)[0m Updating learning rate to 9.428368350141435e-06[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2937801)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2937801)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...
[36m(_train_fn pid=2937801)[0m Epoch: 6 cost time: 0.5363767147064209[32m [repeated 11x across cluster][0m

Trial trial-0f2dd_00004 completed after 7 iterations at 2024-08-26 14:02:40. Total running time: 14s
[36m(_train_fn pid=2937801)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed101/trial-0f2dd_00004_4_alpha_d_ff=2,batch_size=64,d_core=64,d_model=128,dropout=0.0020,e_layers=2,learning_rate=0.0003,lradj=type1_2024-08-26_14-02-33/checkpoint_000006)[32m [repeated 12x across cluster][0m
2024-08-26 14:02:40,978	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:42,148	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:42,497	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:43,268	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:43,993	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:44,262	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:45,050	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:45,060	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:45,277	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:46,069	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2939055)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed101/trial-0f2dd_00006_6_alpha_d_ff=3,batch_size=128,d_core=128,d_model=512,dropout=0.0078,e_layers=2,learning_rate=0.0011,lradj=type1_2024-08-26_14-02-40/checkpoint_000002)[32m [repeated 10x across cluster][0m
2024-08-26 14:02:46,282	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:47,059	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:47,281	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             0.65888 â”‚
â”‚ time_total_s                                 5.29274 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.21654 â”‚
â”‚ train_loss                                   0.18895 â”‚
â”‚ valid_loss                                   0.21727 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-0f2dd_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                             0.0078 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00028 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2938845)[0m configuration
[36m(_train_fn pid=2938845)[0m {'batch_size': 64, 'learning_rate': 0.0002759569966857953, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.007800802191130523, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2938845)[0m Use GPU: cuda:0
[36m(_train_fn pid=2938845)[0m train 8449
[36m(_train_fn pid=2938845)[0m val 2785
[36m(_train_fn pid=2938845)[0m start_epoch 0
[36m(_train_fn pid=2938845)[0m max_epoch 8
[36m(_train_fn pid=2936905)[0m Epoch: 3, Steps: 529 | Train Loss: 0.3754934 Vali Loss: 0.2386980 Best vali loss: 0.2354288[32m [repeated 11x across cluster][0m

Trial trial-0f2dd_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00779 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00108 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2939055)[0m {'batch_size': 128, 'learning_rate': 0.0010804902612104347, 'd_model': 512, 'd_core': 128, 'e_layers': 2, 'dropout': 0.007785980560642377, 'lradj': 'type1', 'd_ff': 1536}
[36m(_train_fn pid=2938845)[0m Validation loss decreased (inf --> 0.2211).  Saving model state dict ...
[36m(_train_fn pid=2936904)[0m EarlyStopping counter: 2 out of 3[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2939055)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2319058 Best vali loss: 0.2319058
[36m(_train_fn pid=2937801)[0m Early stopping
[36m(_train_fn pid=2938845)[0m Validation loss decreased (0.2178 --> 0.2148).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-0f2dd_00002 completed after 4 iterations at 2024-08-26 14:02:45. Total running time: 18s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             4.07926 â”‚
â”‚ time_total_s                                16.87823 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.23543 â”‚
â”‚ train_loss                                   0.39743 â”‚
â”‚ valid_loss                                   0.24912 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2936905)[0m Early stopping
[36m(_train_fn pid=2939055)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2302420 Best vali loss: 0.2302420
[36m(_train_fn pid=2936904)[0m 	iters: 300, epoch: 4 | loss: 0.1819332[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2936904)[0m 	speed: 0.0110s/iter; left time: 25.8435s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2938845)[0m Updating learning rate to 0.00013797849834289765[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2938845)[0m saving checkpoint...[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2938845)[0m Epoch: 4 cost time: 0.7886254787445068[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2939055)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2285227 Best vali loss: 0.2285227
[36m(_train_fn pid=2939055)[0m configuration
[36m(_train_fn pid=2939055)[0m Use GPU: cuda:0
[36m(_train_fn pid=2939055)[0m train 8449
[36m(_train_fn pid=2939055)[0m val 2785

Trial trial-0f2dd_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00218 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00068 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2939774)[0m configuration
[36m(_train_fn pid=2939774)[0m Use GPU: cuda:0
[36m(_train_fn pid=2939055)[0m start_epoch 0
[36m(_train_fn pid=2939055)[0m max_epoch 8
[36m(_train_fn pid=2939774)[0m train 8449
[36m(_train_fn pid=2939774)[0m val 2785
[36m(_train_fn pid=2939055)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2375471 Best vali loss: 0.2285227
[36m(_train_fn pid=2939774)[0m start_epoch 0
[36m(_train_fn pid=2939774)[0m max_epoch 8
[36m(_train_fn pid=2938845)[0m Epoch: 6, Steps: 133 | Train Loss: 0.3591688 Vali Loss: 0.2159040 Best vali loss: 0.2141404[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2939774)[0m {'batch_size': 16, 'learning_rate': 0.0006824395424321212, 'd_model': 64, 'd_core': 512, 'e_layers': 1, 'dropout': 0.0021751019738947093, 'lradj': 'type1', 'd_ff': 192}
2024-08-26 14:02:47,911	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:48,152	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:48,365	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:48,648	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:48,804	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:50,332	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:50,833	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:51,212	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2940365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed101/trial-0f2dd_00008_8_alpha_d_ff=4,batch_size=128,d_core=512,d_model=128,dropout=0.0024,e_layers=1,learning_rate=0.0005,lradj=type1_2024-08-26_14-02-48/checkpoint_000001)[32m [repeated 11x across cluster][0m
2024-08-26 14:02:51,627	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:52,046	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:52,080	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:52,422	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:52,827	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2939055)[0m Validation loss decreased (inf --> 0.2319).  Saving model state dict ...
[36m(_train_fn pid=2939055)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2381264 Best vali loss: 0.2285227
[36m(_train_fn pid=2939055)[0m EarlyStopping counter: 2 out of 3[32m [repeated 5x across cluster][0m

Trial trial-0f2dd_00005 completed after 7 iterations at 2024-08-26 14:02:48. Total running time: 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             0.86815 â”‚
â”‚ time_total_s                                  7.0632 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.21414 â”‚
â”‚ train_loss                                   0.32136 â”‚
â”‚ valid_loss                                   0.21595 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-0f2dd_00001 completed after 4 iterations at 2024-08-26 14:02:48. Total running time: 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             6.21502 â”‚
â”‚ time_total_s                                20.21105 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21888 â”‚
â”‚ train_loss                                   0.25556 â”‚
â”‚ valid_loss                                   0.24411 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-0f2dd_00006 completed after 6 iterations at 2024-08-26 14:02:48. Total running time: 22s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             0.73416 â”‚
â”‚ time_total_s                                 6.45713 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22852 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.24101 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2939055)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2410082 Best vali loss: 0.2285227
[36m(_train_fn pid=2939774)[0m Validation loss decreased (inf --> 0.2190).  Saving model state dict ...

Trial trial-0f2dd_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00235 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00054 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2940365)[0m {'batch_size': 128, 'learning_rate': 0.0005427797778114463, 'd_model': 128, 'd_core': 512, 'e_layers': 1, 'dropout': 0.002351454169993176, 'lradj': 'type1', 'd_ff': 512}

Trial trial-0f2dd_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00103 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00008 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2940370)[0m {'batch_size': 16, 'learning_rate': 7.648735034717312e-05, 'd_model': 128, 'd_core': 128, 'e_layers': 3, 'dropout': 0.0010294890958341026, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2939055)[0m Validation loss decreased (0.2302 --> 0.2285).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2939055)[0m Early stopping[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2939774)[0m 	iters: 500, epoch: 2 | loss: 0.9503265[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2939774)[0m 	speed: 0.0023s/iter; left time: 7.2864s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2939774)[0m Updating learning rate to 0.0003412197712160606[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2939774)[0m saving checkpoint...[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2939774)[0m Epoch: 2 cost time: 1.2635202407836914[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2940365)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2211590 Best vali loss: 0.2211590
[36m(_train_fn pid=2940365)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2195292 Best vali loss: 0.2195292
[36m(_train_fn pid=2940365)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2189725 Best vali loss: 0.2189725
[36m(_train_fn pid=2940370)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940365)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2201827 Best vali loss: 0.2189725
[36m(_train_fn pid=2939774)[0m Epoch: 3, Steps: 529 | Train Loss: 0.3823569 Vali Loss: 0.2242032 Best vali loss: 0.2190183[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2940365)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2212729 Best vali loss: 0.2189725

2024-08-26 14:02:53,366	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:53,559	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:55,885	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:02:58,135	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2940370)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed101/trial-0f2dd_00009_9_alpha_d_ff=2,batch_size=16,d_core=128,d_model=128,dropout=0.0010,e_layers=3,learning_rate=0.0001,lradj=cosine_2024-08-26_14-02-48/checkpoint_000002)[32m [repeated 9x across cluster][0m
2024-08-26 14:03:00,694	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:03,210	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2940370)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed101/trial-0f2dd_00009_9_alpha_d_ff=2,batch_size=16,d_core=128,d_model=128,dropout=0.0010,e_layers=3,learning_rate=0.0001,lradj=cosine_2024-08-26_14-02-48/checkpoint_000004)[32m [repeated 2x across cluster][0m
2024-08-26 14:03:05,609	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:05,621	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed101' in 0.0109s.
Trial trial-0f2dd_00008 completed after 6 iterations at 2024-08-26 14:02:52. Total running time: 26s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             0.40279 â”‚
â”‚ time_total_s                                 3.08129 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21897 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.22181 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2940365)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2218134 Best vali loss: 0.2189725
[36m(_train_fn pid=2940365)[0m EarlyStopping counter: 3 out of 3[32m [repeated 8x across cluster][0m

Trial trial-0f2dd_00007 completed after 4 iterations at 2024-08-26 14:02:53. Total running time: 27s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.47665 â”‚
â”‚ time_total_s                                 6.90213 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21902 â”‚
â”‚ train_loss                                   0.32112 â”‚
â”‚ valid_loss                                   0.22464 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2940370)[0m Validation loss decreased (inf --> 0.2223).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940365)[0m Validation loss decreased (0.2195 --> 0.2190).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2939774)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m 	iters: 500, epoch: 2 | loss: 0.3999258[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2940370)[0m 	speed: 0.0040s/iter; left time: 12.9042s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2940370)[0m Updating learning rate to 6.528603722632522e-05[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2940370)[0m saving checkpoint...[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2940370)[0m Epoch: 2 cost time: 2.204704761505127[32m [repeated 10x across cluster][0m

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2024-08-26 14:02:56. Total running time: 30s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 0f2dd_00005 with best_valid_loss=0.21414041707502876 and params={'batch_size': 64, 'learning_rate': 0.0002759569966857953, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.007800802191130523, 'lradj': 'cosine', 'd_ff': 256}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-0f2dd_00009   RUNNING                16       7.64874e-05         128              2        128            3   0.00102949   cosine         2            6.01484       0.826582       0.219616            0.219616 â”‚
â”‚ trial-0f2dd_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5           11.4134        0.251011       0.224506            0.217783 â”‚
â”‚ trial-0f2dd_00001   TERMINATED             16       0.000261705         512              1        256            2   0.0017752    type1          4           20.2111        0.255558       0.244114            0.218885 â”‚
â”‚ trial-0f2dd_00002   TERMINATED             16       0.00273926          512              1         64            1   0.00146944   type1          4           16.8782        0.397428       0.249123            0.235429 â”‚
â”‚ trial-0f2dd_00003   TERMINATED            128       0.00115017          512              1        128            3   0.00902515   cosine         4            5.60858     nan              0.240752            0.227039 â”‚
â”‚ trial-0f2dd_00004   TERMINATED             64       0.000301708         128              2         64            2   0.00204751   type1          7            5.29274       0.18895        0.217275            0.216543 â”‚
â”‚ trial-0f2dd_00005   TERMINATED             64       0.000275957         128              2         32            1   0.0078008    cosine         7            7.0632        0.321362       0.215949            0.21414  â”‚
â”‚ trial-0f2dd_00006   TERMINATED            128       0.00108049          512              3        128            2   0.00778598   type1          6            6.45713     nan              0.241008            0.228523 â”‚
â”‚ trial-0f2dd_00007   TERMINATED             16       0.00068244           64              3        512            1   0.0021751    type1          4            6.90213       0.321121       0.224641            0.219018 â”‚
â”‚ trial-0f2dd_00008   TERMINATED            128       0.00054278          128              4        512            1   0.00235145   type1          6            3.08129     nan              0.221813            0.218972 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2940370)[0m Epoch: 2, Steps: 529 | Train Loss: 0.8265821 Vali Loss: 0.2196164 Best vali loss: 0.2196164[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2939774)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2940370)[0m Validation loss decreased (0.2196 --> 0.2160).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2940370)[0m 	iters: 500, epoch: 4 | loss: 0.9817333[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2940370)[0m 	speed: 0.0041s/iter; left time: 8.8101s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2940370)[0m Updating learning rate to 3.824367517358656e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m Epoch: 4 cost time: 2.233365058898926[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m Epoch: 4, Steps: 529 | Train Loss: 0.5156719 Vali Loss: 0.2203736 Best vali loss: 0.2159804[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m EarlyStopping counter: 2 out of 3

Trial trial-0f2dd_00009 completed after 6 iterations at 2024-08-26 14:03:05. Total running time: 39s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-0f2dd_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             2.39579 â”‚
â”‚ time_total_s                                15.72746 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21598 â”‚
â”‚ train_loss                                   0.30849 â”‚
â”‚ valid_loss                                    0.2201 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:03:05. Total running time: 39s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 0f2dd_00005 with best_valid_loss=0.21414041707502876 and params={'batch_size': 64, 'learning_rate': 0.0002759569966857953, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.007800802191130523, 'lradj': 'cosine', 'd_ff': 256}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-0f2dd_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5           11.4134        0.251011       0.224506            0.217783 â”‚
â”‚ trial-0f2dd_00001   TERMINATED             16       0.000261705         512              1        256            2   0.0017752    type1          4           20.2111        0.255558       0.244114            0.218885 â”‚
â”‚ trial-0f2dd_00002   TERMINATED             16       0.00273926          512              1         64            1   0.00146944   type1          4           16.8782        0.397428       0.249123            0.235429 â”‚
â”‚ trial-0f2dd_00003   TERMINATED            128       0.00115017          512              1        128            3   0.00902515   cosine         4            5.60858     nan              0.240752            0.227039 â”‚
â”‚ trial-0f2dd_00004   TERMINATED             64       0.000301708         128              2         64            2   0.00204751   type1          7            5.29274       0.18895        0.217275            0.216543 â”‚
â”‚ trial-0f2dd_00005   TERMINATED             64       0.000275957         128              2         32            1   0.0078008    cosine         7            7.0632        0.321362       0.215949            0.21414  â”‚
â”‚ trial-0f2dd_00006   TERMINATED            128       0.00108049          512              3        128            2   0.00778598   type1          6            6.45713     nan              0.241008            0.228523 â”‚
â”‚ trial-0f2dd_00007   TERMINATED             16       0.00068244           64              3        512            1   0.0021751    type1          4            6.90213       0.321121       0.224641            0.219018 â”‚
â”‚ trial-0f2dd_00008   TERMINATED            128       0.00054278          128              4        512            1   0.00235145   type1          6            3.08129     nan              0.221813            0.218972 â”‚
â”‚ trial-0f2dd_00009   TERMINATED             16       7.64874e-05         128              2        128            3   0.00102949   cosine         6           15.7275        0.308487       0.220103            0.21598  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 64, 'learning_rate': 0.0002759569966857953, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.007800802191130523, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2940370)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed101/trial-0f2dd_00009_9_alpha_d_ff=2,batch_size=16,d_core=128,d_model=128,dropout=0.0010,e_layers=3,learning_rate=0.0001,lradj=cosine_2024-08-26_14-02-48/checkpoint_000005)
[36m(_train_fn pid=2940370)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2940370)[0m Early stopping
[36m(_train_fn pid=2940370)[0m 	iters: 500, epoch: 6 | loss: 0.1908325[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2940370)[0m 	speed: 0.0036s/iter; left time: 3.9106s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2940370)[0m Updating learning rate to 1.1201313120847891e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m Epoch: 6 cost time: 2.0764970779418945[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2940370)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3084874 Vali Loss: 0.2201033 Best vali loss: 0.2159804[32m [repeated 2x across cluster][0m


Time taken (4 parallel trials): 43 seconds


2024-08-26 14:03:09,721	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:03:10,121	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:03:10,126	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:03:10,133	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:03:13,627	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed102   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator      â”‚
â”‚ Scheduler                        FIFOScheduler              â”‚
â”‚ Number of trials                 10                         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-03-09_289332_2941766/artifacts/2024-08-26_14-03-10/ETTh2_96_96_test_seed102/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:03:10. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-2921e_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-2921e_00001   PENDING              32       0.000742775          32              1        512            2   0.00511204   cosine  â”‚
â”‚ trial-2921e_00002   PENDING              64       0.00303169          128              4        128            3   0.00154283   type1   â”‚
â”‚ trial-2921e_00003   PENDING               8       0.00024551           32              4        256            4   0.00977459   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-2921e_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00977 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00025 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-2921e_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00154 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00303 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-2921e_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00511 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00074 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-2921e_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2943991)[0m configuration
[36m(_train_fn pid=2943991)[0m {'batch_size': 8, 'learning_rate': 0.00024551037327984437, 'd_model': 32, 'd_core': 256, 'e_layers': 4, 'dropout': 0.009774594362224054, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2943989)[0m {'batch_size': 32, 'learning_rate': 0.0007427745826841511, 'd_model': 32, 'd_core': 512, 'e_layers': 2, 'dropout': 0.005112040991599395, 'lradj': 'cosine', 'd_ff': 32}
[36m(_train_fn pid=2943991)[0m Use GPU: cuda:0
[36m(_train_fn pid=2943991)[0m train 8449
[36m(_train_fn pid=2943991)[0m val 2785
[36m(_train_fn pid=2943991)[0m start_epoch 0
[36m(_train_fn pid=2943991)[0m max_epoch 8
[36m(_train_fn pid=2943989)[0m 	iters: 100, epoch: 1 | loss: 0.2917015
[36m(_train_fn pid=2943989)[0m 	speed: 0.0117s/iter; left time: 23.7369s
[36m(_train_fn pid=2943990)[0m Updating learning rate to 0.003031690508549005
[36m(_train_fn pid=2943990)[0m saving checkpoint...
[36m(_train_fn pid=2943990)[0m Validation loss decreased (inf --> 0.2454).  Saving model state dict ...
[36m(_train_fn pid=2943990)[0m Epoch: 1 cost time: 1.2455055713653564
[36m(_train_fn pid=2943990)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102/trial-2921e_00002_2_alpha_d_ff=4,batch_size=64,d_core=128,d_model=128,dropout=0.0015,e_layers=3,learning_rate=0.0030,lradj=type1_2024-08-26_14-03-10/checkpoint_000000)
2024-08-26 14:03:14,399	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:14,502	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:14,776	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:16,028	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:16,395	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:16,492	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:17,147	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:18,329	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:18,490	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:18,501	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:19,474	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2943990)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102/trial-2921e_00002_2_alpha_d_ff=4,batch_size=64,d_core=128,d_model=128,dropout=0.0015,e_layers=3,learning_rate=0.0030,lradj=type1_2024-08-26_14-03-10/checkpoint_000005)[32m [repeated 11x across cluster][0m
2024-08-26 14:03:20,301	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:20,312	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:21,818	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:21,849	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:23,019	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:23,126	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:23,920	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:24,265	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2943990)[0m Epoch: 1, Steps: 133 | Train Loss: 0.3662341 Vali Loss: 0.2453706 Best vali loss: 0.2453706
[36m(_train_fn pid=2943990)[0m Validation loss decreased (0.2454 --> 0.2372).  Saving model state dict ...
[36m(_train_fn pid=2943990)[0m Validation loss decreased (0.2372 --> 0.2237).  Saving model state dict ...
[36m(_train_fn pid=2943990)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2943990)[0m {'batch_size': 64, 'learning_rate': 0.003031690508549005, 'd_model': 128, 'd_core': 128, 'e_layers': 3, 'dropout': 0.0015428278645144157, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2943988)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2943990)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2943990)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2943990)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2943990)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2943990)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2943990)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2943991)[0m 	iters: 500, epoch: 1 | loss: 0.3217292[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2943991)[0m 	speed: 0.0118s/iter; left time: 93.8788s[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2943990)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2943988)[0m Updating learning rate to 0.00020740251485476345[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2943988)[0m saving checkpoint...[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2943988)[0m Validation loss decreased (inf --> 0.2192).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2943988)[0m Epoch: 3 cost time: 1.684178113937378[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2943988)[0m Epoch: 3, Steps: 265 | Train Loss: 0.4363888 Vali Loss: 0.2238688 Best vali loss: 0.2187237[32m [repeated 10x across cluster][0m

Trial trial-2921e_00002 completed after 6 iterations at 2024-08-26 14:03:19. Total running time: 9s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                              1.1419 â”‚
â”‚ time_total_s                                 7.76653 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22368 â”‚
â”‚ train_loss                                   0.33262 â”‚
â”‚ valid_loss                                   0.22866 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2943990)[0m Early stopping
[36m(_train_fn pid=2943988)[0m Validation loss decreased (0.2192 --> 0.2187).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-2921e_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00571 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                       0.0035 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2945394)[0m {'batch_size': 64, 'learning_rate': 0.0034950964096075675, 'd_model': 512, 'd_core': 32, 'e_layers': 3, 'dropout': 0.0057083745965185815, 'lradj': 'type1', 'd_ff': 512}

Trial trial-2921e_00000 completed after 5 iterations at 2024-08-26 14:03:21. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.54514 â”‚
â”‚ time_total_s                                10.13697 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21872 â”‚
â”‚ train_loss                                   0.41705 â”‚
â”‚ valid_loss                                   0.22475 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2945394)[0m configuration
[36m(_train_fn pid=2945394)[0m Use GPU: cuda:0
[36m(_train_fn pid=2945394)[0m train 8449
[36m(_train_fn pid=2945394)[0m val 2785
[36m(_train_fn pid=2943988)[0m EarlyStopping counter: 3 out of 3[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2945394)[0m start_epoch 0
[36m(_train_fn pid=2945394)[0m max_epoch 8

Trial trial-2921e_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00598 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                       0.0001 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2945762)[0m configuration
[36m(_train_fn pid=2945762)[0m {'batch_size': 16, 'learning_rate': 9.812752493332668e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 4, 'dropout': 0.005984920543553974, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2945762)[0m Use GPU: cuda:0
[36m(_train_fn pid=2943989)[0m 	iters: 200, epoch: 6 | loss: 0.3347480[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2943989)[0m 	speed: 0.0039s/iter; left time: 2.3165s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2945762)[0m train 8449
[36m(_train_fn pid=2945762)[0m val 2785
[36m(_train_fn pid=2945762)[0m start_epoch 0
[36m(_train_fn pid=2945762)[0m max_epoch 8
[36m(_train_fn pid=2945394)[0m Updating learning rate to 0.0017475482048037838[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2945394)[0m saving checkpoint...[32m [repeated 9x across cluster][0m
2024-08-26 14:03:24,534	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:25,547	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2945394)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102/trial-2921e_00004_4_alpha_d_ff=1,batch_size=64,d_core=32,d_model=512,dropout=0.0057,e_layers=3,learning_rate=0.0035,lradj=type1_2024-08-26_14-03-19/checkpoint_000002)[32m [repeated 10x across cluster][0m
2024-08-26 14:03:26,802	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:27,738	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:28,052	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:28,719	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:29,294	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:29,442	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:29,575	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:30,551	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2946222)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102/trial-2921e_00006_6_alpha_d_ff=4,batch_size=64,d_core=32,d_model=128,dropout=0.0030,e_layers=2,learning_rate=0.0001,lradj=type1_2024-08-26_14-03-24/checkpoint_000003)[32m [repeated 8x across cluster][0m
2024-08-26 14:03:31,583	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:32,736	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:33,925	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:35,032	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2943991)[0m Validation loss decreased (inf --> 0.2237).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2945394)[0m Epoch: 2 cost time: 1.080817699432373[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2945394)[0m Epoch: 2, Steps: 133 | Train Loss: 0.4451868 Vali Loss: 0.2742514 Best vali loss: 0.2719665[32m [repeated 9x across cluster][0m

Trial trial-2921e_00001 completed after 7 iterations at 2024-08-26 14:03:24. Total running time: 14s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             1.40518 â”‚
â”‚ time_total_s                                12.82357 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.22128 â”‚
â”‚ train_loss                                   0.30683 â”‚
â”‚ valid_loss                                   0.22718 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2943989)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2943989)[0m Validation loss decreased (0.2246 --> 0.2213).  Saving model state dict ...
[36m(_train_fn pid=2945394)[0m Validation loss decreased (0.2720 --> 0.2698).  Saving model state dict ...

Trial trial-2921e_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00305 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00009 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2946222)[0m {'batch_size': 64, 'learning_rate': 8.957894590743325e-05, 'd_model': 128, 'd_core': 32, 'e_layers': 2, 'dropout': 0.003048538499745396, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2945394)[0m EarlyStopping counter: 1 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2946222)[0m configuration
[36m(_train_fn pid=2946222)[0m Use GPU: cuda:0
[36m(_train_fn pid=2946222)[0m 	iters: 100, epoch: 2 | loss: 0.4741593[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2946222)[0m 	speed: 0.0100s/iter; left time: 8.3448s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2946222)[0m train 8449
[36m(_train_fn pid=2946222)[0m val 2785
[36m(_train_fn pid=2946222)[0m start_epoch 0
[36m(_train_fn pid=2946222)[0m max_epoch 8

Trial trial-2921e_00004 completed after 6 iterations at 2024-08-26 14:03:29. Total running time: 19s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.24027 â”‚
â”‚ time_total_s                                 8.26454 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.26983 â”‚
â”‚ train_loss                                   0.88316 â”‚
â”‚ valid_loss                                   0.27134 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2945762)[0m Updating learning rate to 9.439276839760909e-05[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2945762)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2945762)[0m Validation loss decreased (inf --> 0.2227).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2945394)[0m Epoch: 6 cost time: 1.0881555080413818[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2945394)[0m Epoch: 6, Steps: 133 | Train Loss: 0.8831646 Vali Loss: 0.2713417 Best vali loss: 0.2698306[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2945394)[0m Early stopping
[36m(_train_fn pid=2946222)[0m Validation loss decreased (0.2194 --> 0.2190).  Saving model state dict ...[32m [repeated 3x across cluster][0m

Trial trial-2921e_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00098 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                       0.0045 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2946778)[0m configuration
[36m(_train_fn pid=2946778)[0m {'batch_size': 16, 'learning_rate': 0.004499239274499522, 'd_model': 128, 'd_core': 512, 'e_layers': 3, 'dropout': 0.0009797192501478404, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2946778)[0m Use GPU: cuda:0
[36m(_train_fn pid=2946778)[0m train 8449
[36m(_train_fn pid=2946778)[0m val 2785
[36m(_train_fn pid=2946778)[0m start_epoch 0
[36m(_train_fn pid=2946778)[0m max_epoch 8
[36m(_train_fn pid=2945394)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2943991)[0m 	iters: 1000, epoch: 2 | loss: 0.3386078[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2943991)[0m 	speed: 0.0132s/iter; left time: 84.2383s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2946222)[0m Updating learning rate to 1.3996710298036445e-06[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2946222)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2946222)[0m Epoch: 7 cost time: 0.9628002643585205[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2946222)[0m Epoch: 7, Steps: 133 | Train Loss: 0.4934702 Vali Loss: 0.2183821 Best vali loss: 0.2183821[32m [repeated 6x across cluster][0m

Trial trial-2921e_00006 completed after 8 iterations at 2024-08-26 14:03:35. Total running time: 24s
2024-08-26 14:03:36,122	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2943991)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102/trial-2921e_00003_3_alpha_d_ff=4,batch_size=8,d_core=256,d_model=32,dropout=0.0098,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_14-03-10/checkpoint_000001)[32m [repeated 5x across cluster][0m
2024-08-26 14:03:36,278	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:37,900	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:40,016	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:42,117	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.10505 â”‚
â”‚ time_total_s                                 8.99655 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21836 â”‚
â”‚ train_loss                                   0.78833 â”‚
â”‚ valid_loss                                   0.21836 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2946222)[0m Validation loss decreased (0.2184 --> 0.2184).  Saving model state dict ...[32m [repeated 4x across cluster][0m

Trial trial-2921e_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00411 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00212 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2947383)[0m configuration
[36m(_train_fn pid=2947383)[0m {'batch_size': 16, 'learning_rate': 0.002118267062187805, 'd_model': 64, 'd_core': 64, 'e_layers': 4, 'dropout': 0.0041052163703840525, 'lradj': 'cosine', 'd_ff': 192}
[36m(_train_fn pid=2947383)[0m Use GPU: cuda:0
[36m(_train_fn pid=2947383)[0m train 8449
[36m(_train_fn pid=2947383)[0m val 2785
[36m(_train_fn pid=2947383)[0m start_epoch 0
[36m(_train_fn pid=2947383)[0m max_epoch 8
[36m(_train_fn pid=2946778)[0m Validation loss decreased (inf --> 0.2720).  Saving model state dict ...
[36m(_train_fn pid=2947383)[0m 	iters: 400, epoch: 1 | loss: 0.2831513[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2947383)[0m 	speed: 0.0044s/iter; left time: 16.8645s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2946778)[0m Updating learning rate to 0.0043279971760402805[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2946778)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2946778)[0m Epoch: 1 cost time: 5.644428730010986[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2946778)[0m Epoch: 1, Steps: 529 | Train Loss: 0.4560101 Vali Loss: 0.2720375 Best vali loss: 0.2720375[32m [repeated 4x across cluster][0m

Trial status: 5 TERMINATED | 4 RUNNING
Current time: 2024-08-26 14:03:40. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 2921e_00006 with best_valid_loss=0.21835883045217935 and params={'batch_size': 64, 'learning_rate': 8.957894590743325e-05, 'd_model': 128, 'd_core': 32, 'e_layers': 2, 'dropout': 0.003048538499745396, 'lradj': 'type1', 'd_ff': 512}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-2921e_00003   RUNNING                 8       0.00024551           32              4        256            4   0.00977459    type1          2           24.4256        0.512959       0.222572            0.222572 â”‚
â”‚ trial-2921e_00005   RUNNING                16       9.81275e-05         512              2        256            4   0.00598492    cosine         2           12.8725        0.34144        0.219032            0.219032 â”‚
â”‚ trial-2921e_00007   RUNNING                16       0.00449924          128              2        512            3   0.000979719   cosine         1            7.08207       0.45601        0.272038            0.272038 â”‚
â”‚ trial-2921e_00008   RUNNING                16       0.00211827           64              3         64            4   0.00410522    cosine         1            3.46061       0.462391       0.252792            0.252792 â”‚
â”‚ trial-2921e_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5           10.137         0.417053       0.22475             0.218724 â”‚
â”‚ trial-2921e_00001   TERMINATED             32       0.000742775          32              1        512            2   0.00511204    cosine         7           12.8236        0.306826       0.227184            0.221283 â”‚
â”‚ trial-2921e_00002   TERMINATED             64       0.00303169          128              4        128            3   0.00154283    type1          6            7.76653       0.332623       0.228657            0.22368  â”‚
â”‚ trial-2921e_00004   TERMINATED             64       0.0034951           512              1         32            3   0.00570837    type1          6            8.26454       0.883165       0.271342            0.269831 â”‚
â”‚ trial-2921e_00006   TERMINATED             64       8.95789e-05         128              4         32            2   0.00304854    type1          8            8.99655       0.78833        0.218359            0.218359 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2945762)[0m Validation loss decreased (0.2227 --> 0.2190).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2945762)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102/trial-2921e_00005_5_alpha_d_ff=2,batch_size=16,d_core=256,d_model=512,dropout=0.0060,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-03-21/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 14:03:42,899	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:43,720	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:45,712	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:47,727	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:47,736	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2943991)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102/trial-2921e_00003_3_alpha_d_ff=4,batch_size=8,d_core=256,d_model=32,dropout=0.0098,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_14-03-10/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 14:03:48,548	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:49,225	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:51,477	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:53,562	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2945762)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102/trial-2921e_00005_5_alpha_d_ff=2,batch_size=16,d_core=256,d_model=512,dropout=0.0060,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-03-21/checkpoint_000004)[32m [repeated 5x across cluster][0m
2024-08-26 14:03:54,382	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:54,479	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:57,049	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:57,383	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:58,720	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2943991)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102/trial-2921e_00003_3_alpha_d_ff=4,batch_size=8,d_core=256,d_model=32,dropout=0.0098,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_14-03-10/checkpoint_000003)[32m [repeated 5x across cluster][0m
2024-08-26 14:03:58,898	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:59,269	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:03:59,971	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:00,420	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:02,000	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:03,594	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2945762)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2947383)[0m Validation loss decreased (inf --> 0.2528).  Saving model state dict ...
[36m(_train_fn pid=2947383)[0m 	iters: 300, epoch: 3 | loss: 0.8362364[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2947383)[0m 	speed: 0.0044s/iter; left time: 12.7136s[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2946778)[0m Updating learning rate to 0.0038403409378394877[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2946778)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2946778)[0m Epoch: 2 cost time: 4.846653461456299[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2946778)[0m Epoch: 2, Steps: 529 | Train Loss: 0.3890040 Vali Loss: 0.2716250 Best vali loss: 0.2716250[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2946778)[0m Validation loss decreased (0.2720 --> 0.2716).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2947383)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2943991)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2943991)[0m 	iters: 200, epoch: 4 | loss: 0.1693627[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2943991)[0m 	speed: 0.0092s/iter; left time: 47.0218s[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2946778)[0m Updating learning rate to 0.0031105118015484073[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2946778)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2946778)[0m Epoch: 3 cost time: 4.57882833480835[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2946778)[0m Epoch: 3, Steps: 529 | Train Loss: 0.7769172 Vali Loss: 0.2722674 Best vali loss: 0.2716250[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2947383)[0m Validation loss decreased (0.2333 --> 0.2272).  Saving model state dict ...
[36m(_train_fn pid=2947383)[0m EarlyStopping counter: 1 out of 3[32m [repeated 3x across cluster][0m

Trial trial-2921e_00005 completed after 5 iterations at 2024-08-26 14:03:53. Total running time: 43s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.83269 â”‚
â”‚ time_total_s                                 30.1422 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21903 â”‚
â”‚ train_loss                                   0.27647 â”‚
â”‚ valid_loss                                   0.22683 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2945762)[0m Early stopping
[36m(_train_fn pid=2947383)[0m Validation loss decreased (0.2272 --> 0.2249).  Saving model state dict ...
[36m(_train_fn pid=2947383)[0m 	iters: 100, epoch: 7 | loss: 0.3225746[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2947383)[0m 	speed: 0.0102s/iter; left time: 9.7719s[32m [repeated 25x across cluster][0m

Trial trial-2921e_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00152 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2948501)[0m configuration
[36m(_train_fn pid=2948501)[0m {'batch_size': 64, 'learning_rate': 0.00011135229340643438, 'd_model': 256, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0015172279405402203, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2948501)[0m Use GPU: cuda:0
[36m(_train_fn pid=2948501)[0m train 8449
[36m(_train_fn pid=2948501)[0m val 2785
[36m(_train_fn pid=2946778)[0m Updating learning rate to 0.002249619637249761[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2946778)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2946778)[0m Epoch: 4 cost time: 4.706503868103027[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2946778)[0m Epoch: 4, Steps: 529 | Train Loss: 0.5000696 Vali Loss: 0.2711224 Best vali loss: 0.2711224[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2948501)[0m start_epoch 0
[36m(_train_fn pid=2948501)[0m max_epoch 8
[36m(_train_fn pid=2948501)[0m Validation loss decreased (inf --> 0.2182).  Saving model state dict ...
[36m(_train_fn pid=2947383)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2948501)[0m Validation loss decreased (0.2182 --> 0.2168).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2943991)[0m 	iters: 100, epoch: 5 | loss: 0.2533609[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2943991)[0m 	speed: 0.0396s/iter; left time: 163.5272s[32m [repeated 20x across cluster][0m

Trial trial-2921e_00008 completed after 8 iterations at 2024-08-26 14:03:59. Total running time: 49s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             2.92096 â”‚
â”‚ time_total_s                                23.39643 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22493 â”‚
â”‚ train_loss                                   0.35951 â”‚
â”‚ valid_loss                                   0.23354 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2948501)[0m Updating learning rate to 7.698248562446663e-05[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2948501)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2948501)[0m Epoch: 3 cost time: 1.318885087966919[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2948501)[0m Epoch: 3, Steps: 133 | Train Loss: 0.3928850 Vali Loss: 0.2192001 Best vali loss: 0.2167628[32m [repeated 7x across cluster][0m

Trial trial-2921e_00009 completed after 5 iterations at 2024-08-26 14:04:03. Total running time: 53s
2024-08-26 14:04:04,455	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2946778)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102/trial-2921e_00007_7_alpha_d_ff=2,batch_size=16,d_core=512,d_model=128,dropout=0.0010,e_layers=3,learning_rate=0.0045,lradj=cosine_2024-08-26_14-03-29/checkpoint_000005)[32m [repeated 7x across cluster][0m
2024-08-26 14:04:07,730	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:08,255	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2943991)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102/trial-2921e_00003_3_alpha_d_ff=4,batch_size=8,d_core=256,d_model=32,dropout=0.0098,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_14-03-10/checkpoint_000005)[32m [repeated 3x across cluster][0m
2024-08-26 14:04:19,092	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102' in 0.0089s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.59131 â”‚
â”‚ time_total_s                                  8.4714 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21676 â”‚
â”‚ train_loss                                   0.34808 â”‚
â”‚ valid_loss                                   0.22131 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2948501)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2948501)[0m Early stopping
[36m(_train_fn pid=2946778)[0m 	iters: 100, epoch: 7 | loss: 0.9687335[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2946778)[0m 	speed: 0.0123s/iter; left time: 11.7538s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2946778)[0m Updating learning rate to 0.0006588983366600339[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2946778)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2946778)[0m Epoch: 6 cost time: 4.689246892929077[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2946778)[0m Epoch: 6, Steps: 529 | Train Loss: 0.5608687 Vali Loss: 0.2726106 Best vali loss: 0.2711224[32m [repeated 3x across cluster][0m

Trial trial-2921e_00007 completed after 7 iterations at 2024-08-26 14:04:07. Total running time: 57s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             3.27209 â”‚
â”‚ time_total_s                                36.89399 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.27112 â”‚
â”‚ train_loss                                   0.77684 â”‚
â”‚ valid_loss                                   0.27145 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2943991)[0m EarlyStopping counter: 1 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2946778)[0m Early stopping
[36m(_train_fn pid=2943991)[0m 	iters: 400, epoch: 6 | loss: 1.1850337[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2943991)[0m 	speed: 0.0043s/iter; left time: 11.9096s[32m [repeated 12x across cluster][0m

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2024-08-26 14:04:10. Total running time: 1min 0s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 2921e_00009 with best_valid_loss=0.21676284878964483 and params={'batch_size': 64, 'learning_rate': 0.00011135229340643438, 'd_model': 256, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0015172279405402203, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-2921e_00003   RUNNING                 8       0.00024551           32              4        256            4   0.00977459    type1          5           56.5512        0.278887       0.221322            0.220695 â”‚
â”‚ trial-2921e_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5           10.137         0.417053       0.22475             0.218724 â”‚
â”‚ trial-2921e_00001   TERMINATED             32       0.000742775          32              1        512            2   0.00511204    cosine         7           12.8236        0.306826       0.227184            0.221283 â”‚
â”‚ trial-2921e_00002   TERMINATED             64       0.00303169          128              4        128            3   0.00154283    type1          6            7.76653       0.332623       0.228657            0.22368  â”‚
â”‚ trial-2921e_00004   TERMINATED             64       0.0034951           512              1         32            3   0.00570837    type1          6            8.26454       0.883165       0.271342            0.269831 â”‚
â”‚ trial-2921e_00005   TERMINATED             16       9.81275e-05         512              2        256            4   0.00598492    cosine         5           30.1422        0.276466       0.22683             0.219032 â”‚
â”‚ trial-2921e_00006   TERMINATED             64       8.95789e-05         128              4         32            2   0.00304854    type1          8            8.99655       0.78833        0.218359            0.218359 â”‚
â”‚ trial-2921e_00007   TERMINATED             16       0.00449924          128              2        512            3   0.000979719   cosine         7           36.894         0.776839       0.271449            0.271122 â”‚
â”‚ trial-2921e_00008   TERMINATED             16       0.00211827           64              3         64            4   0.00410522    cosine         8           23.3964        0.35951        0.233542            0.224932 â”‚
â”‚ trial-2921e_00009   TERMINATED             64       0.000111352         256              4         32            4   0.00151723    cosine         5            8.4714        0.348081       0.221311            0.216763 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2943991)[0m Updating learning rate to 1.5344398329990273e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2943991)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2943991)[0m Epoch: 5 cost time: 8.859037160873413[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2943991)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.2788872 Vali Loss: 0.2213220 Best vali loss: 0.2206945[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2943991)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2943991)[0m 	iters: 400, epoch: 7 | loss: 0.2745807[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2943991)[0m 	speed: 0.0045s/iter; left time: 7.7992s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2943991)[0m Updating learning rate to 7.672199164995137e-06
[36m(_train_fn pid=2943991)[0m saving checkpoint...
[36m(_train_fn pid=2943991)[0m Epoch: 6 cost time: 4.686270236968994
[36m(_train_fn pid=2943991)[0m Epoch: 6, Steps: 1057 | Train Loss: 0.4316684 Vali Loss: 0.2211875 Best vali loss: 0.2206945

Trial trial-2921e_00003 completed after 7 iterations at 2024-08-26 14:04:19. Total running time: 1min 8s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-2921e_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             5.51182 â”‚
â”‚ time_total_s                                67.37293 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.22069 â”‚
â”‚ train_loss                                   0.44165 â”‚
â”‚ valid_loss                                   0.22186 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:04:19. Total running time: 1min 8s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 2921e_00009 with best_valid_loss=0.21676284878964483 and params={'batch_size': 64, 'learning_rate': 0.00011135229340643438, 'd_model': 256, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0015172279405402203, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-2921e_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5           10.137         0.417053       0.22475             0.218724 â”‚
â”‚ trial-2921e_00001   TERMINATED             32       0.000742775          32              1        512            2   0.00511204    cosine         7           12.8236        0.306826       0.227184            0.221283 â”‚
â”‚ trial-2921e_00002   TERMINATED             64       0.00303169          128              4        128            3   0.00154283    type1          6            7.76653       0.332623       0.228657            0.22368  â”‚
â”‚ trial-2921e_00003   TERMINATED              8       0.00024551           32              4        256            4   0.00977459    type1          7           67.3729        0.441649       0.221858            0.220695 â”‚
â”‚ trial-2921e_00004   TERMINATED             64       0.0034951           512              1         32            3   0.00570837    type1          6            8.26454       0.883165       0.271342            0.269831 â”‚
â”‚ trial-2921e_00005   TERMINATED             16       9.81275e-05         512              2        256            4   0.00598492    cosine         5           30.1422        0.276466       0.22683             0.219032 â”‚
â”‚ trial-2921e_00006   TERMINATED             64       8.95789e-05         128              4         32            2   0.00304854    type1          8            8.99655       0.78833        0.218359            0.218359 â”‚
â”‚ trial-2921e_00007   TERMINATED             16       0.00449924          128              2        512            3   0.000979719   cosine         7           36.894         0.776839       0.271449            0.271122 â”‚
â”‚ trial-2921e_00008   TERMINATED             16       0.00211827           64              3         64            4   0.00410522    cosine         8           23.3964        0.35951        0.233542            0.224932 â”‚
â”‚ trial-2921e_00009   TERMINATED             64       0.000111352         256              4         32            4   0.00151723    cosine         5            8.4714        0.348081       0.221311            0.216763 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 64, 'learning_rate': 0.00011135229340643438, 'd_model': 256, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0015172279405402203, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2943991)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed102/trial-2921e_00003_3_alpha_d_ff=4,batch_size=8,d_core=256,d_model=32,dropout=0.0098,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_14-03-10/checkpoint_000006)
[36m(_train_fn pid=2943991)[0m Updating learning rate to 3.836099582497568e-06
[36m(_train_fn pid=2943991)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2943991)[0m saving checkpoint...
[36m(_train_fn pid=2943991)[0m Epoch: 7 cost time: 4.868086814880371
[36m(_train_fn pid=2943991)[0m Epoch: 7, Steps: 1057 | Train Loss: 0.4416492 Vali Loss: 0.2218580 Best vali loss: 0.2206945
[36m(_train_fn pid=2943991)[0m Early stopping
[36m(_train_fn pid=2943991)[0m 	iters: 1000, epoch: 7 | loss: 0.2114795[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2943991)[0m 	speed: 0.0047s/iter; left time: 5.2774s[32m [repeated 6x across cluster][0m


Time taken (4 parallel trials): 74 seconds


2024-08-26 14:04:23,077	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:04:23,477	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:04:23,482	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:04:23,489	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:04:27,587	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2951953)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed103/trial-54db2_00000_0_2024-08-26_14-04-23/checkpoint_000000)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed103   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator      â”‚
â”‚ Scheduler                        FIFOScheduler              â”‚
â”‚ Number of trials                 10                         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed103
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-04-22_645221_2949732/artifacts/2024-08-26_14-04-23/ETTh2_96_96_test_seed103/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:04:23. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-54db2_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-54db2_00001   PENDING              32       0.000140477          64              3         32            3   0.00815902   type1   â”‚
â”‚ trial-54db2_00002   PENDING              16       0.0001893            32              3        256            3   0.00262433   type1   â”‚
â”‚ trial-54db2_00003   PENDING               8       0.00150986          128              1        128            1   0.00184945   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-54db2_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00816 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00014 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-54db2_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-54db2_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00185 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00151 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-54db2_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00262 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00019 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2951956)[0m configuration
[36m(_train_fn pid=2951956)[0m {'batch_size': 8, 'learning_rate': 0.0015098586556275362, 'd_model': 128, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0018494477324409366, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2951956)[0m Use GPU: cuda:0
[36m(_train_fn pid=2951953)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2951956)[0m train 8449
[36m(_train_fn pid=2951956)[0m val 2785
[36m(_train_fn pid=2951956)[0m start_epoch 0
[36m(_train_fn pid=2951956)[0m max_epoch 8
[36m(_train_fn pid=2951956)[0m 	iters: 100, epoch: 1 | loss: 0.3706624
[36m(_train_fn pid=2951956)[0m 	speed: 0.0113s/iter; left time: 94.4591s
[36m(_train_fn pid=2951953)[0m Updating learning rate to 0.000288581929876693
[36m(_train_fn pid=2951953)[0m saving checkpoint...
[36m(_train_fn pid=2951953)[0m Validation loss decreased (inf --> 0.2180).  Saving model state dict ...
[36m(_train_fn pid=2951953)[0m Epoch: 1 cost time: 1.7209885120391846
2024-08-26 14:04:28,053	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:29,333	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:30,203	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:31,003	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:31,053	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:32,245	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:32,685	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:32,694	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2951956)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed103/trial-54db2_00003_3_alpha_d_ff=1,batch_size=8,d_core=128,d_model=128,dropout=0.0018,e_layers=1,learning_rate=0.0015,lradj=type1_2024-08-26_14-04-23/checkpoint_000000)[32m [repeated 7x across cluster][0m
2024-08-26 14:04:34,099	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:35,460	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:35,895	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:37,645	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:37,936	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2953119)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed103/trial-54db2_00004_4_alpha_d_ff=2,batch_size=16,d_core=32,d_model=32,dropout=0.0046,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_14-04-32/checkpoint_000000)[32m [repeated 6x across cluster][0m
2024-08-26 14:04:38,197	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:39,382	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:39,429	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:40,901	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:41,268	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:42,792	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2951953)[0m Epoch: 1, Steps: 265 | Train Loss: 0.8630882 Vali Loss: 0.2180439 Best vali loss: 0.2180439
[36m(_train_fn pid=2951953)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2951954)[0m Validation loss decreased (0.2301 --> 0.2209).  Saving model state dict ...
[36m(_train_fn pid=2951953)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2951954)[0m {'batch_size': 32, 'learning_rate': 0.0001404773964454432, 'd_model': 64, 'd_core': 32, 'e_layers': 3, 'dropout': 0.008159024022997424, 'lradj': 'type1', 'd_ff': 192}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2951953)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2951953)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2951953)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2951953)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2951953)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2951953)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2951954)[0m 	iters: 200, epoch: 3 | loss: 0.2827173[32m [repeated 26x across cluster][0m
[36m(_train_fn pid=2951954)[0m 	speed: 0.0062s/iter; left time: 8.5984s[32m [repeated 26x across cluster][0m
[36m(_train_fn pid=2951954)[0m Validation loss decreased (0.2209 --> 0.2190).  Saving model state dict ...

Trial trial-54db2_00000 completed after 4 iterations at 2024-08-26 14:04:32. Total running time: 9s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.67982 â”‚
â”‚ time_total_s                                  7.6286 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21804 â”‚
â”‚ train_loss                                   0.36996 â”‚
â”‚ valid_loss                                   0.22132 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2951956)[0m Updating learning rate to 0.0015098586556275362[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2951956)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2951956)[0m Validation loss decreased (inf --> 0.2193).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2951956)[0m Epoch: 1 cost time: 5.647556781768799[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2951956)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.3082074 Vali Loss: 0.2193182 Best vali loss: 0.2193182[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2951953)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2951953)[0m Early stopping
[36m(_train_fn pid=2951954)[0m Validation loss decreased (0.2190 --> 0.2185).  Saving model state dict ...

Trial trial-54db2_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00464 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00016 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2953119)[0m configuration
[36m(_train_fn pid=2953119)[0m {'batch_size': 16, 'learning_rate': 0.00016049029706421172, 'd_model': 32, 'd_core': 32, 'e_layers': 4, 'dropout': 0.004636439377854756, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=2953119)[0m Use GPU: cuda:0
[36m(_train_fn pid=2951955)[0m Validation loss decreased (0.2335 --> 0.2223).  Saving model state dict ...
[36m(_train_fn pid=2953119)[0m train 8449
[36m(_train_fn pid=2953119)[0m val 2785
[36m(_train_fn pid=2953119)[0m start_epoch 0
[36m(_train_fn pid=2953119)[0m max_epoch 8
[36m(_train_fn pid=2951955)[0m 	iters: 200, epoch: 3 | loss: 0.5319872[32m [repeated 26x across cluster][0m
[36m(_train_fn pid=2951955)[0m 	speed: 0.0065s/iter; left time: 19.3155s[32m [repeated 26x across cluster][0m
[36m(_train_fn pid=2951954)[0m Updating learning rate to 4.3899186389201e-06[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2951954)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2951954)[0m Epoch: 6 cost time: 1.4876325130462646[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2951954)[0m Epoch: 6, Steps: 265 | Train Loss: 0.3041672 Vali Loss: 0.2182819 Best vali loss: 0.2182819[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2953119)[0m Validation loss decreased (inf --> 0.2367).  Saving model state dict ...
[36m(_train_fn pid=2951956)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2951955)[0m Validation loss decreased (0.2223 --> 0.2208).  Saving model state dict ...[32m [repeated 4x across cluster][0m

Trial trial-54db2_00001 completed after 8 iterations at 2024-08-26 14:04:41. Total running time: 17s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.88307 â”‚
â”‚ time_total_s                                16.20254 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                               0.2182 â”‚
â”‚ train_loss                                   0.24715 â”‚
â”‚ valid_loss                                    0.2182 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2953119)[0m 	iters: 200, epoch: 3 | loss: 0.4581717[32m [repeated 30x across cluster][0m
[36m(_train_fn pid=2953119)[0m 	speed: 0.0046s/iter; left time: 13.7094s[32m [repeated 30x across cluster][0m

Trial trial-54db2_00005 started with configuration:
2024-08-26 14:04:42,969	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:43,798	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2953119)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed103/trial-54db2_00004_4_alpha_d_ff=2,batch_size=16,d_core=32,d_model=32,dropout=0.0046,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_14-04-32/checkpoint_000002)[32m [repeated 8x across cluster][0m
2024-08-26 14:04:44,993	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:46,487	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:46,600	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:46,779	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:47,793	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:47,993	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:49,302	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2953984)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed103/trial-54db2_00005_5_alpha_d_ff=2,batch_size=32,d_core=512,d_model=32,dropout=0.0014,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_14-04-41/checkpoint_000003)[32m [repeated 7x across cluster][0m
2024-08-26 14:04:49,413	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:50,040	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:50,614	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:50,678	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:51,241	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:51,890	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:52,238	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:52,283	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:52,514	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:53,159	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00141 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2951956)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2953984)[0m configuration
[36m(_train_fn pid=2953984)[0m {'batch_size': 32, 'learning_rate': 5.922145718812574e-05, 'd_model': 32, 'd_core': 512, 'e_layers': 2, 'dropout': 0.0014101301495383844, 'lradj': 'cosine', 'd_ff': 64}
[36m(_train_fn pid=2953984)[0m Use GPU: cuda:0
[36m(_train_fn pid=2953984)[0m train 8449
[36m(_train_fn pid=2953984)[0m val 2785
[36m(_train_fn pid=2951955)[0m Updating learning rate to 2.3662449037078538e-05[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2951955)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2951955)[0m Epoch: 4 cost time: 3.1018083095550537[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2951955)[0m Epoch: 4, Steps: 529 | Train Loss: 0.4751502 Vali Loss: 0.2200650 Best vali loss: 0.2200650[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2953984)[0m start_epoch 0
[36m(_train_fn pid=2953984)[0m max_epoch 8
[36m(_train_fn pid=2953984)[0m Validation loss decreased (inf --> 0.2682).  Saving model state dict ...
[36m(_train_fn pid=2953119)[0m Validation loss decreased (0.2229 --> 0.2208).  Saving model state dict ...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2951955)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2953984)[0m 	iters: 100, epoch: 3 | loss: 0.2728575[32m [repeated 31x across cluster][0m
[36m(_train_fn pid=2953984)[0m 	speed: 0.0101s/iter; left time: 15.0004s[32m [repeated 31x across cluster][0m

Trial trial-54db2_00003 completed after 4 iterations at 2024-08-26 14:04:47. Total running time: 24s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             4.99836 â”‚
â”‚ time_total_s                                22.72299 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21932 â”‚
â”‚ train_loss                                   0.32489 â”‚
â”‚ valid_loss                                   0.24398 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2951956)[0m Early stopping
[36m(_train_fn pid=2953984)[0m Updating learning rate to 4.0942263847269955e-05[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2953984)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2953984)[0m Epoch: 3 cost time: 1.262831211090088[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2953984)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3071596 Vali Loss: 0.2427610 Best vali loss: 0.2427610[32m [repeated 7x across cluster][0m

Trial trial-54db2_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00082 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2954690)[0m configuration
[36m(_train_fn pid=2954690)[0m {'batch_size': 64, 'learning_rate': 0.00010585774945267125, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.0008187308260078068, 'lradj': 'cosine', 'd_ff': 384}
[36m(_train_fn pid=2954690)[0m Use GPU: cuda:0
[36m(_train_fn pid=2954690)[0m train 8449
[36m(_train_fn pid=2954690)[0m val 2785
[36m(_train_fn pid=2954690)[0m start_epoch 0
[36m(_train_fn pid=2954690)[0m max_epoch 8
[36m(_train_fn pid=2954690)[0m Validation loss decreased (inf --> 0.2362).  Saving model state dict ...
[36m(_train_fn pid=2953984)[0m Validation loss decreased (0.2386 --> 0.2352).  Saving model state dict ...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2951956)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2951955)[0m 	iters: 300, epoch: 7 | loss: 0.5055267[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=2951955)[0m 	speed: 0.0061s/iter; left time: 4.6514s[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=2954690)[0m Updating learning rate to 3.267387127483966e-05[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2954690)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2954690)[0m Epoch: 5 cost time: 0.5021188259124756[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2954690)[0m Epoch: 5, Steps: 133 | Train Loss: 0.2996188 Vali Loss: 0.2178492 Best vali loss: 0.2178492[32m [repeated 11x across cluster][0m

Trial status: 3 TERMINATED | 4 RUNNING
Current time: 2024-08-26 14:04:53. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 54db2_00006 with best_valid_loss=0.21784915151994147 and params={'batch_size': 64, 'learning_rate': 0.00010585774945267125, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.0008187308260078068, 'lradj': 'cosine', 'd_ff': 384}
2024-08-26 14:04:53,790	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:53,799	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:53,985	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:54,395	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2954690)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed103/trial-54db2_00006_6_alpha_d_ff=3,batch_size=64,d_core=32,d_model=128,dropout=0.0008,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_14-04-47/checkpoint_000006)[32m [repeated 14x across cluster][0m
2024-08-26 14:04:55,001	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:55,250	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:55,296	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-54db2_00002   RUNNING                16       0.0001893            32              3        256            3   0.00262433    type1          6           24.9675        0.458412       0.219787            0.219787 â”‚
â”‚ trial-54db2_00004   RUNNING                16       0.00016049           32              2         32            4   0.00463644    type1          6           17.9046        0.514219       0.219696            0.219696 â”‚
â”‚ trial-54db2_00005   RUNNING                32       5.92215e-05          32              2        512            2   0.00141013    cosine         6            9.42221       0.32731        0.233586            0.233586 â”‚
â”‚ trial-54db2_00006   RUNNING                64       0.000105858         128              3         32            1   0.000818731   cosine         5            3.84289       0.299619       0.217849            0.217849 â”‚
â”‚ trial-54db2_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         4            7.6286        0.369963       0.221324            0.218044 â”‚
â”‚ trial-54db2_00001   TERMINATED             32       0.000140477          64              3         32            3   0.00815902    type1          8           16.2025        0.247149       0.218197            0.218197 â”‚
â”‚ trial-54db2_00003   TERMINATED              8       0.00150986          128              1        128            1   0.00184945    type1          4           22.723         0.324888       0.243976            0.219318 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-54db2_00006 completed after 8 iterations at 2024-08-26 14:04:55. Total running time: 31s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.60416 â”‚
â”‚ time_total_s                                 5.67698 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21709 â”‚
â”‚ train_loss                                   0.30035 â”‚
â”‚ valid_loss                                   0.21709 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-54db2_00005 completed after 8 iterations at 2024-08-26 14:04:55. Total running time: 31s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.49877 â”‚
â”‚ time_total_s                                12.47268 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.23275 â”‚
â”‚ train_loss                                    0.2941 â”‚
â”‚ valid_loss                                   0.23275 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2953984)[0m Validation loss decreased (0.2329 --> 0.2328).  Saving model state dict ...[32m [repeated 13x across cluster][0m

Trial trial-54db2_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00441 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00023 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2956010)[0m configuration
[36m(_train_fn pid=2956010)[0m {'batch_size': 32, 'learning_rate': 0.00022843666628067372, 'd_model': 256, 'd_core': 64, 'e_layers': 4, 'dropout': 0.00441141173479148, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=2956010)[0m Use GPU: cuda:0
[36m(_train_fn pid=2956010)[0m train 8449
[36m(_train_fn pid=2956010)[0m val 2785

Trial trial-54db2_00008 started with configuration:
2024-08-26 14:04:56,850	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:57,946	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:04:59,221	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2956268)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed103/trial-54db2_00009_9_alpha_d_ff=1,batch_size=32,d_core=32,d_model=128,dropout=0.0012,e_layers=4,learning_rate=0.0046,lradj=cosine_2024-08-26_14-04-56/checkpoint_000000)[32m [repeated 7x across cluster][0m
2024-08-26 14:05:01,544	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:01,669	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:03,306	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:03,860	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:03,952	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:06,332	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:06,484	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00792 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00009 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-54db2_00002 completed after 8 iterations at 2024-08-26 14:04:56. Total running time: 33s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             2.86131 â”‚
â”‚ time_total_s                                31.77075 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21965 â”‚
â”‚ train_loss                                   0.37592 â”‚
â”‚ valid_loss                                   0.21973 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2951955)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2953119)[0m 	iters: 400, epoch: 8 | loss: 0.3699167[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2953119)[0m 	speed: 0.0044s/iter; left time: 0.5686s[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2956010)[0m start_epoch 0
[36m(_train_fn pid=2956010)[0m max_epoch 8

Trial trial-54db2_00004 completed after 8 iterations at 2024-08-26 14:04:57. Total running time: 34s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             2.69342 â”‚
â”‚ time_total_s                                23.56178 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21967 â”‚
â”‚ train_loss                                   0.43959 â”‚
â”‚ valid_loss                                   0.21967 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-54db2_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00123 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00462 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2953119)[0m Updating learning rate to 1.253830445814154e-06[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2953119)[0m saving checkpoint...[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2953119)[0m Epoch: 8 cost time: 2.3280603885650635[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2953119)[0m Epoch: 8, Steps: 529 | Train Loss: 0.4395852 Vali Loss: 0.2196710 Best vali loss: 0.2196710[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2956010)[0m Validation loss decreased (inf --> 0.2156).  Saving model state dict ...
[36m(_train_fn pid=2953119)[0m Validation loss decreased (0.2197 --> 0.2197).  Saving model state dict ...
[36m(_train_fn pid=2956268)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956268)[0m {'batch_size': 32, 'learning_rate': 0.0046169589940309825, 'd_model': 128, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0012288522523975533, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956268)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956268)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956268)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956010)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2956268)[0m 	iters: 100, epoch: 2 | loss: 0.2365709[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2956268)[0m 	speed: 0.0162s/iter; left time: 28.4380s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2956268)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956268)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956268)[0m Validation loss decreased (0.2740 --> 0.2729).  Saving model state dict ...
[36m(_train_fn pid=2956268)[0m Updating learning rate to 0.003940821003585255[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2956268)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2956268)[0m Epoch: 2 cost time: 2.0025651454925537[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2956268)[0m Epoch: 2, Steps: 265 | Train Loss: 0.5144437 Vali Loss: 0.2728516 Best vali loss: 0.2728516[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2956010)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2956140)[0m Validation loss decreased (inf --> 0.2289).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-54db2_00007 completed after 4 iterations at 2024-08-26 14:05:06. Total running time: 42s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.37729 â”‚
â”‚ time_total_s                                 9.75826 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21563 â”‚
â”‚ train_loss                                   0.37093 â”‚
â”‚ valid_loss                                   0.21846 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2956010)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2956010)[0m Early stopping
2024-08-26 14:05:08,169	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2956140)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed103/trial-54db2_00008_8_alpha_d_ff=1,batch_size=16,d_core=256,d_model=64,dropout=0.0079,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-04-55/checkpoint_000001)[32m [repeated 7x across cluster][0m
2024-08-26 14:05:08,347	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:10,265	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:11,923	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:12,047	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:14,995	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2956140)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed103/trial-54db2_00008_8_alpha_d_ff=1,batch_size=16,d_core=256,d_model=64,dropout=0.0079,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-04-55/checkpoint_000003)[32m [repeated 5x across cluster][0m
2024-08-26 14:05:17,755	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2956140)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed103/trial-54db2_00008_8_alpha_d_ff=1,batch_size=16,d_core=256,d_model=64,dropout=0.0079,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-04-55/checkpoint_000005)[32m [repeated 2x across cluster][0m
2024-08-26 14:05:20,532	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:23,335	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:23,345	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed103' in 0.0085s.
[36m(_train_fn pid=2956268)[0m Validation loss decreased (0.2729 --> 0.2676).  Saving model state dict ...
[36m(_train_fn pid=2956140)[0m 	iters: 500, epoch: 2 | loss: 0.1654587[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2956140)[0m 	speed: 0.0063s/iter; left time: 20.2640s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2956140)[0m Validation loss decreased (0.2289 --> 0.2217).  Saving model state dict ...
[36m(_train_fn pid=2956268)[0m Updating learning rate to 0.0023084794970154913[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2956268)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2956268)[0m Epoch: 4 cost time: 1.6180860996246338[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2956268)[0m Epoch: 4, Steps: 265 | Train Loss: 0.7428929 Vali Loss: 0.2676136 Best vali loss: 0.2676090[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2956268)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2956268)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2956140)[0m Validation loss decreased (0.2217 --> 0.2193).  Saving model state dict ...

Trial trial-54db2_00009 completed after 6 iterations at 2024-08-26 14:05:12. Total running time: 48s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.77933 â”‚
â”‚ time_total_s                                13.65139 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.26761 â”‚
â”‚ train_loss                                   0.30053 â”‚
â”‚ valid_loss                                    0.2705 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2956268)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2956268)[0m Early stopping
[36m(_train_fn pid=2956140)[0m 	iters: 100, epoch: 4 | loss: 0.5097662[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2956140)[0m 	speed: 0.0125s/iter; left time: 31.9152s[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2956268)[0m Updating learning rate to 0.0006761379904457271[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2956268)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2956268)[0m Epoch: 6 cost time: 1.5547056198120117[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2956268)[0m Epoch: 6, Steps: 265 | Train Loss: 0.3005336 Vali Loss: 0.2705013 Best vali loss: 0.2676090[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2956140)[0m Validation loss decreased (0.2193 --> 0.2183).  Saving model state dict ...
[36m(_train_fn pid=2956140)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2956140)[0m 	iters: 500, epoch: 5 | loss: 0.9298189[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2956140)[0m 	speed: 0.0045s/iter; left time: 7.3083s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2956140)[0m Updating learning rate to 2.788038530629719e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956140)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956140)[0m Epoch: 5 cost time: 2.4037632942199707[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956140)[0m Epoch: 5, Steps: 529 | Train Loss: 0.3928304 Vali Loss: 0.2203987 Best vali loss: 0.2183481[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956140)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2956140)[0m 	iters: 500, epoch: 7 | loss: 0.2569965[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2956140)[0m 	speed: 0.0045s/iter; left time: 2.5354s[32m [repeated 10x across cluster][0m

Trial trial-54db2_00008 completed after 7 iterations at 2024-08-26 14:05:23. Total running time: 59s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-54db2_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             2.80036 â”‚
â”‚ time_total_s                                26.50388 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.21835 â”‚
â”‚ train_loss                                   0.47429 â”‚
â”‚ valid_loss                                   0.22047 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:05:23. Total running time: 59s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 54db2_00007 with best_valid_loss=0.2156294611000073 and params={'batch_size': 32, 'learning_rate': 0.00022843666628067372, 'd_model': 256, 'd_core': 64, 'e_layers': 4, 'dropout': 0.00441141173479148, 'lradj': 'cosine', 'd_ff': 512}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-54db2_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         4            7.6286        0.369963       0.221324            0.218044 â”‚
â”‚ trial-54db2_00001   TERMINATED             32       0.000140477          64              3         32            3   0.00815902    type1          8           16.2025        0.247149       0.218197            0.218197 â”‚
â”‚ trial-54db2_00002   TERMINATED             16       0.0001893            32              3        256            3   0.00262433    type1          8           31.7707        0.375916       0.219729            0.219654 â”‚
â”‚ trial-54db2_00003   TERMINATED              8       0.00150986          128              1        128            1   0.00184945    type1          4           22.723         0.324888       0.243976            0.219318 â”‚
â”‚ trial-54db2_00004   TERMINATED             16       0.00016049           32              2         32            4   0.00463644    type1          8           23.5618        0.439585       0.219671            0.219671 â”‚
â”‚ trial-54db2_00005   TERMINATED             32       5.92215e-05          32              2        512            2   0.00141013    cosine         8           12.4727        0.294104       0.23275             0.23275  â”‚
â”‚ trial-54db2_00006   TERMINATED             64       0.000105858         128              3         32            1   0.000818731   cosine         8            5.67698       0.300353       0.217095            0.217095 â”‚
â”‚ trial-54db2_00007   TERMINATED             32       0.000228437         256              2         64            4   0.00441141    cosine         4            9.75826       0.370933       0.218465            0.215629 â”‚
â”‚ trial-54db2_00008   TERMINATED             16       9.03277e-05          64              1        256            4   0.00792046    cosine         7           26.5039        0.474292       0.220475            0.218348 â”‚
â”‚ trial-54db2_00009   TERMINATED             32       0.00461696          128              1         32            4   0.00122885    cosine         6           13.6514        0.300534       0.270501            0.267609 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 32, 'learning_rate': 0.00022843666628067372, 'd_model': 256, 'd_core': 64, 'e_layers': 4, 'dropout': 0.00441141173479148, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=2956140)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed103/trial-54db2_00008_8_alpha_d_ff=1,batch_size=16,d_core=256,d_model=64,dropout=0.0079,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-04-55/checkpoint_000006)
[36m(_train_fn pid=2956140)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2956140)[0m Early stopping
[36m(_train_fn pid=2956140)[0m Updating learning rate to 3.437892443761421e-06[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956140)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956140)[0m Epoch: 7 cost time: 2.4428839683532715[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2956140)[0m Epoch: 7, Steps: 529 | Train Loss: 0.4742921 Vali Loss: 0.2204746 Best vali loss: 0.2183481[32m [repeated 2x across cluster][0m


Time taken (4 parallel trials): 64 seconds


2024-08-26 14:05:27,250	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:05:27,649	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:05:27,655	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:05:27,662	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:05:30,785	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed104   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator      â”‚
â”‚ Scheduler                        FIFOScheduler              â”‚
â”‚ Number of trials                 10                         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-05-26_817478_2957758/artifacts/2024-08-26_14-05-27/ETTh2_96_96_test_seed104/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:05:27. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-7b1b1_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-7b1b1_00001   PENDING              64       0.00121134          512              2         64            1   0.00220905   cosine  â”‚
â”‚ trial-7b1b1_00002   PENDING               8       0.00147688          256              2        128            4   0.00745471   type1   â”‚
â”‚ trial-7b1b1_00003   PENDING              16       0.000596437          32              4        256            4   0.00144806   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-7b1b1_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00221 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00121 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-7b1b1_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00145 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                       0.0006 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2959981)[0m configuration
[36m(_train_fn pid=2959981)[0m {'batch_size': 64, 'learning_rate': 0.0012113406782578878, 'd_model': 512, 'd_core': 64, 'e_layers': 1, 'dropout': 0.0022090507814568152, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2959981)[0m Use GPU: cuda:0
[36m(_train_fn pid=2959983)[0m {'batch_size': 16, 'learning_rate': 0.0005964368643223593, 'd_model': 32, 'd_core': 256, 'e_layers': 4, 'dropout': 0.0014480605405577248, 'lradj': 'type1', 'd_ff': 128}

Trial trial-7b1b1_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00745 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00148 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-7b1b1_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2959981)[0m train 8449
[36m(_train_fn pid=2959981)[0m val 2785
[36m(_train_fn pid=2959981)[0m start_epoch 0
[36m(_train_fn pid=2959981)[0m max_epoch 8
[36m(_train_fn pid=2959981)[0m 	iters: 100, epoch: 1 | loss: 0.3081862
[36m(_train_fn pid=2959981)[0m 	speed: 0.0105s/iter; left time: 10.1184s
[36m(_train_fn pid=2959981)[0m Updating learning rate to 0.001165236768899345
[36m(_train_fn pid=2959981)[0m saving checkpoint...
[36m(_train_fn pid=2959981)[0m Validation loss decreased (inf --> 0.2325).  Saving model state dict ...
[36m(_train_fn pid=2959981)[0m Epoch: 1 cost time: 0.8938663005828857
[36m(_train_fn pid=2959981)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104/trial-7b1b1_00001_1_alpha_d_ff=2,batch_size=64,d_core=64,d_model=512,dropout=0.0022,e_layers=1,learning_rate=0.0012,lradj=cosine_2024-08-26_14-05-27/checkpoint_000000)
2024-08-26 14:05:31,561	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2959981)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104/trial-7b1b1_00001_1_alpha_d_ff=2,batch_size=64,d_core=64,d_model=512,dropout=0.0022,e_layers=1,learning_rate=0.0012,lradj=cosine_2024-08-26_14-05-27/checkpoint_000001)
2024-08-26 14:05:32,207	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:32,307	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:33,069	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:33,825	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:34,241	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:35,799	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:36,576	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:36,585	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2959983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104/trial-7b1b1_00003_3_alpha_d_ff=4,batch_size=16,d_core=256,d_model=32,dropout=0.0014,e_layers=4,learning_rate=0.0006,lradj=type1_2024-08-26_14-05-27/checkpoint_000000)[32m [repeated 7x across cluster][0m
2024-08-26 14:05:37,087	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:37,341	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:37,590	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:38,097	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:38,579	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2959981)[0m Epoch: 1, Steps: 133 | Train Loss: 0.3081862 Vali Loss: 0.2324577 Best vali loss: 0.2324577
[36m(_train_fn pid=2959981)[0m Updating learning rate to 0.001033943943090576
[36m(_train_fn pid=2959981)[0m saving checkpoint...
[36m(_train_fn pid=2959981)[0m Validation loss decreased (0.2325 --> 0.2257).  Saving model state dict ...
[36m(_train_fn pid=2959981)[0m Epoch: 2 cost time: 0.582524299621582
[36m(_train_fn pid=2959981)[0m Epoch: 2, Steps: 133 | Train Loss: 0.3532383 Vali Loss: 0.2257176 Best vali loss: 0.2257176
[36m(_train_fn pid=2959981)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2959981)[0m EarlyStopping counter: 2 out of 3

Trial trial-7b1b1_00001 completed after 5 iterations at 2024-08-26 14:05:33. Total running time: 6s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             0.75345 â”‚
â”‚ time_total_s                                 4.59866 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22572 â”‚
â”‚ train_loss                                   0.23478 â”‚
â”‚ valid_loss                                   0.24836 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2959981)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2959981)[0m Early stopping
[36m(_train_fn pid=2959982)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2959980)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2959982)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2959982)[0m {'batch_size': 8, 'learning_rate': 0.0014768824422182547, 'd_model': 256, 'd_core': 128, 'e_layers': 4, 'dropout': 0.007454707978090664, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=2959982)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2959982)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2959982)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2959982)[0m max_epoch 8[32m [repeated 3x across cluster][0m

Trial trial-7b1b1_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00365 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00027 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2960933)[0m {'batch_size': 128, 'learning_rate': 0.00026990171129911793, 'd_model': 128, 'd_core': 64, 'e_layers': 3, 'dropout': 0.003646582996398323, 'lradj': 'type1', 'd_ff': 256}
[36m(_train_fn pid=2959983)[0m 	iters: 500, epoch: 1 | loss: 0.3105939[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2959983)[0m 	speed: 0.0087s/iter; left time: 32.4379s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2959980)[0m Updating learning rate to 0.00020740251485476345[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2959980)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2959980)[0m Validation loss decreased (inf --> 0.2169).  Saving model state dict ...
[36m(_train_fn pid=2959980)[0m Epoch: 3 cost time: 1.2765166759490967[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2959980)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3248712 Vali Loss: 0.2174043 Best vali loss: 0.2168560[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2959983)[0m Validation loss decreased (inf --> 0.2260).  Saving model state dict ...
[36m(_train_fn pid=2960933)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2210161 Best vali loss: 0.2210161
[36m(_train_fn pid=2960933)[0m Validation loss decreased (0.2210 --> 0.2172).  Saving model state dict ...
[36m(_train_fn pid=2960933)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2172351 Best vali loss: 0.2172351

Trial trial-7b1b1_00000 completed after 4 iterations at 2024-08-26 14:05:37. Total running time: 9s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                              1.5388 â”‚
â”‚ time_total_s                                 8.05012 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21686 â”‚
â”‚ train_loss                                   0.35322 â”‚
â”‚ valid_loss                                   0.22428 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2959980)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2960933)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2187470 Best vali loss: 0.2172351
[36m(_train_fn pid=2960933)[0m Validation loss decreased (0.2172 --> 0.2167).  Saving model state dict ...
[36m(_train_fn pid=2960933)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2167089 Best vali loss: 0.2167089
[36m(_train_fn pid=2960933)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2168452 Best vali loss: 0.2167089

Trial trial-7b1b1_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.01111 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00017 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2961459)[0m {'batch_size': 8, 'learning_rate': 0.00017012284690088523, 'd_model': 128, 'd_core': 256, 'e_layers': 2, 'dropout': 0.011112586142189606, 'lradj': 'cosine', 'd_ff': 256}
2024-08-26 14:05:39,054	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:39,546	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:41,003	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:41,082	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2959983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104/trial-7b1b1_00003_3_alpha_d_ff=4,batch_size=16,d_core=256,d_model=32,dropout=0.0014,e_layers=4,learning_rate=0.0006,lradj=type1_2024-08-26_14-05-27/checkpoint_000002)[32m [repeated 11x across cluster][0m
2024-08-26 14:05:48,056	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:48,832	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:52,690	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2959982)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104/trial-7b1b1_00002_2_alpha_d_ff=2,batch_size=8,d_core=128,d_model=256,dropout=0.0075,e_layers=4,learning_rate=0.0015,lradj=type1_2024-08-26_14-05-27/checkpoint_000001)[32m [repeated 3x across cluster][0m
2024-08-26 14:05:53,014	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:55,960	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:05:56,772	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2959980)[0m Early stopping
[36m(_train_fn pid=2960933)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2171626 Best vali loss: 0.2167089
[36m(_train_fn pid=2961459)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2961459)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m

Trial trial-7b1b1_00004 completed after 7 iterations at 2024-08-26 14:05:39. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             0.48986 â”‚
â”‚ time_total_s                                 4.17322 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.21671 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.21707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2960933)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2170743 Best vali loss: 0.2167089
[36m(_train_fn pid=2960933)[0m Early stopping
[36m(_train_fn pid=2961459)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2961459)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2961459)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2961459)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2961459)[0m 	iters: 200, epoch: 1 | loss: 0.2728263[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2961459)[0m 	speed: 0.0048s/iter; left time: 39.2917s[32m [repeated 15x across cluster][0m

Trial trial-7b1b1_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00746 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00012 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2961893)[0m {'batch_size': 8, 'learning_rate': 0.00012008730323502788, 'd_model': 128, 'd_core': 128, 'e_layers': 1, 'dropout': 0.007463716154408458, 'lradj': 'type1', 'd_ff': 384}
[36m(_train_fn pid=2959983)[0m Updating learning rate to 0.00029821843216117967[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2959983)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2959983)[0m Epoch: 2 cost time: 3.8602817058563232[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2959983)[0m Epoch: 2, Steps: 529 | Train Loss: 0.2351134 Vali Loss: 0.2208094 Best vali loss: 0.2208094[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2959982)[0m Validation loss decreased (inf --> 0.2723).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2959983)[0m Validation loss decreased (0.2260 --> 0.2208).  Saving model state dict ...
[36m(_train_fn pid=2960933)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2961893)[0m configuration
[36m(_train_fn pid=2961893)[0m Use GPU: cuda:0
[36m(_train_fn pid=2961893)[0m train 8449
[36m(_train_fn pid=2961893)[0m val 2785
[36m(_train_fn pid=2961893)[0m start_epoch 0
[36m(_train_fn pid=2961893)[0m max_epoch 8
[36m(_train_fn pid=2959982)[0m 	iters: 500, epoch: 2 | loss: 1.7633867[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2959982)[0m 	speed: 0.0098s/iter; left time: 67.7924s[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2959983)[0m Updating learning rate to 0.00014910921608058983
[36m(_train_fn pid=2959983)[0m saving checkpoint...
[36m(_train_fn pid=2959983)[0m Epoch: 3 cost time: 5.123131036758423
[36m(_train_fn pid=2959983)[0m Epoch: 3, Steps: 529 | Train Loss: 0.2942416 Vali Loss: 0.2258291 Best vali loss: 0.2208094
[36m(_train_fn pid=2959983)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2961459)[0m Validation loss decreased (inf --> 0.2203).  Saving model state dict ...
[36m(_train_fn pid=2959983)[0m 	iters: 400, epoch: 4 | loss: 0.3139997[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2959983)[0m 	speed: 0.0098s/iter; left time: 22.1210s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2961893)[0m Updating learning rate to 0.00012008730323502788[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2961893)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2961893)[0m Epoch: 1 cost time: 5.803069591522217[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2961893)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.3520032 Vali Loss: 0.2185933 Best vali loss: 0.2185933[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2959982)[0m Validation loss decreased (0.2723 --> 0.2716).  Saving model state dict ...
[36m(_train_fn pid=2959983)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2961893)[0m Validation loss decreased (inf --> 0.2186).  Saving model state dict ...
[36m(_train_fn pid=2959982)[0m 	iters: 400, epoch: 3 | loss: 0.4962221[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2959982)[0m 	speed: 0.0091s/iter; left time: 54.0683s[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2961459)[0m Updating learning rate to 0.000145208932789631[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2961459)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2961459)[0m Epoch: 2 cost time: 7.149302244186401[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2961459)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.3857633 Vali Loss: 0.2243478 Best vali loss: 0.2202952[32m [repeated 4x across cluster][0m

Trial status: 3 TERMINATED | 4 RUNNING
Current time: 2024-08-26 14:05:57. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 7b1b1_00004 with best_valid_loss=0.21670885473436255 and params={'batch_size': 128, 'learning_rate': 0.00026990171129911793, 'd_model': 128, 'd_core': 64, 'e_layers': 3, 'dropout': 0.003646582996398323, 'lradj': 'type1', 'd_ff': 256}
[36m(_train_fn pid=2959983)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104/trial-7b1b1_00003_3_alpha_d_ff=4,batch_size=16,d_core=256,d_model=32,dropout=0.0014,e_layers=4,learning_rate=0.0006,lradj=type1_2024-08-26_14-05-27/checkpoint_000004)[32m [repeated 4x across cluster][0m
2024-08-26 14:05:58,933	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:01,992	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:03,260	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:04,176	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2961459)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104/trial-7b1b1_00005_5_alpha_d_ff=2,batch_size=8,d_core=256,d_model=128,dropout=0.0111,e_layers=2,learning_rate=0.0002,lradj=cosine_2024-08-26_14-05-37/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 14:06:04,512	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:07,160	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-7b1b1_00002   RUNNING                 8       0.00147688          256              2        128            4   0.00745471   type1          2           23.4209        0.701892       0.271616            0.271616 â”‚
â”‚ trial-7b1b1_00003   RUNNING                16       0.000596437          32              4        256            4   0.00144806   type1          4           23.7757        0.337188       0.221802            0.220809 â”‚
â”‚ trial-7b1b1_00005   RUNNING                 8       0.000170123         128              2        256            2   0.0111126    cosine         2           17.8209        0.385763       0.224348            0.220295 â”‚
â”‚ trial-7b1b1_00006   RUNNING                 8       0.000120087         128              3        128            1   0.00746372   type1          2           14.8645        0.392027       0.221566            0.218593 â”‚
â”‚ trial-7b1b1_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            8.05012       0.353216       0.224278            0.216856 â”‚
â”‚ trial-7b1b1_00001   TERMINATED             64       0.00121134          512              2         64            1   0.00220905   cosine         5            4.59866       0.23478        0.248358            0.225718 â”‚
â”‚ trial-7b1b1_00004   TERMINATED            128       0.000269902         128              2         64            3   0.00364658   type1          7            4.17322     nan              0.217074            0.216709 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2961459)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m

Trial trial-7b1b1_00003 completed after 5 iterations at 2024-08-26 14:05:58. Total running time: 31s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.91586 â”‚
â”‚ time_total_s                                29.69153 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22081 â”‚
â”‚ train_loss                                   0.27765 â”‚
â”‚ valid_loss                                   0.22469 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2959983)[0m Early stopping

Trial trial-7b1b1_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00529 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00169 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2962785)[0m configuration
[36m(_train_fn pid=2962785)[0m {'batch_size': 16, 'learning_rate': 0.0016949504822745052, 'd_model': 512, 'd_core': 64, 'e_layers': 2, 'dropout': 0.005293800402162942, 'lradj': 'cosine', 'd_ff': 2048}
[36m(_train_fn pid=2962785)[0m Use GPU: cuda:0
[36m(_train_fn pid=2962785)[0m train 8449
[36m(_train_fn pid=2962785)[0m val 2785
[36m(_train_fn pid=2962785)[0m start_epoch 0
[36m(_train_fn pid=2962785)[0m max_epoch 8
[36m(_train_fn pid=2959982)[0m 	iters: 1000, epoch: 3 | loss: 0.6195095[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2959982)[0m 	speed: 0.0073s/iter; left time: 39.2051s[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2961893)[0m Validation loss decreased (0.2186 --> 0.2159).  Saving model state dict ...
[36m(_train_fn pid=2961893)[0m Updating learning rate to 3.002182580875697e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2961893)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2961893)[0m Epoch: 3 cost time: 4.912349462509155[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2961893)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.2702783 Vali Loss: 0.2159336 Best vali loss: 0.2159336[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2959983)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2961459)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2962785)[0m Validation loss decreased (inf --> 0.2673).  Saving model state dict ...
[36m(_train_fn pid=2962785)[0m 	iters: 400, epoch: 2 | loss: 0.5302635[32m [repeated 30x across cluster][0m
[36m(_train_fn pid=2962785)[0m 	speed: 0.0055s/iter; left time: 18.0818s[32m [repeated 30x across cluster][0m
[36m(_train_fn pid=2959982)[0m Validation loss decreased (0.2716 --> 0.2698).  Saving model state dict ...
[36m(_train_fn pid=2961893)[0m Updating learning rate to 1.5010912904378486e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2961893)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2961893)[0m Epoch: 4 cost time: 4.016038179397583[32m [repeated 4x across cluster][0m
2024-08-26 14:06:07,846	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:11,167	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2962785)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104/trial-7b1b1_00007_7_alpha_d_ff=4,batch_size=16,d_core=64,d_model=512,dropout=0.0053,e_layers=2,learning_rate=0.0017,lradj=cosine_2024-08-26_14-05-58/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 14:06:11,268	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:12,092	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:12,184	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:14,489	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:17,228	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2961893)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104/trial-7b1b1_00006_6_alpha_d_ff=3,batch_size=8,d_core=128,d_model=128,dropout=0.0075,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_14-05-39/checkpoint_000005)[32m [repeated 5x across cluster][0m
2024-08-26 14:06:17,779	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:20,902	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:21,082	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:21,142	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:21,290	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:22,380	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2963864)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104/trial-7b1b1_00009_9_alpha_d_ff=1,batch_size=32,d_core=64,d_model=64,dropout=0.0027,e_layers=2,learning_rate=0.0003,lradj=type1_2024-08-26_14-06-17/checkpoint_000001)[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2961893)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3531800 Vali Loss: 0.2169020 Best vali loss: 0.2159336[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2962785)[0m Validation loss decreased (0.2673 --> 0.2549).  Saving model state dict ...
[36m(_train_fn pid=2961893)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2962785)[0m Validation loss decreased (0.2549 --> 0.2470).  Saving model state dict ...

Trial trial-7b1b1_00005 completed after 4 iterations at 2024-08-26 14:06:11. Total running time: 43s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             7.08921 â”‚
â”‚ time_total_s                                32.32017 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                               0.2203 â”‚
â”‚ train_loss                                   0.40676 â”‚
â”‚ valid_loss                                   0.22944 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2961459)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2961459)[0m Early stopping
[36m(_train_fn pid=2962785)[0m 	iters: 100, epoch: 4 | loss: 1.3241085[32m [repeated 29x across cluster][0m
[36m(_train_fn pid=2962785)[0m 	speed: 0.0113s/iter; left time: 28.8954s[32m [repeated 29x across cluster][0m
[36m(_train_fn pid=2959982)[0m Updating learning rate to 0.00018461030527728184[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2959982)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2959982)[0m Epoch: 4 cost time: 8.093706369400024[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2959982)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.5312626 Vali Loss: 0.2702166 Best vali loss: 0.2697571[32m [repeated 5x across cluster][0m

Trial trial-7b1b1_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00091 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                       0.0006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2963486)[0m configuration
[36m(_train_fn pid=2963486)[0m {'batch_size': 8, 'learning_rate': 0.0005960629059802318, 'd_model': 128, 'd_core': 512, 'e_layers': 3, 'dropout': 0.0009073884752400232, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2963486)[0m Use GPU: cuda:0
[36m(_train_fn pid=2963486)[0m train 8449
[36m(_train_fn pid=2963486)[0m val 2785
[36m(_train_fn pid=2963486)[0m start_epoch 0
[36m(_train_fn pid=2963486)[0m max_epoch 8
[36m(_train_fn pid=2962785)[0m Validation loss decreased (0.2470 --> 0.2394).  Saving model state dict ...
[36m(_train_fn pid=2959982)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m

Trial trial-7b1b1_00006 completed after 6 iterations at 2024-08-26 14:06:17. Total running time: 49s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             5.13317 â”‚
â”‚ time_total_s                                36.12237 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21593 â”‚
â”‚ train_loss                                   0.25938 â”‚
â”‚ valid_loss                                   0.21655 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2961893)[0m Early stopping
[36m(_train_fn pid=2963486)[0m 	iters: 500, epoch: 1 | loss: 0.3510230[32m [repeated 29x across cluster][0m
[36m(_train_fn pid=2963486)[0m 	speed: 0.0061s/iter; left time: 48.8830s[32m [repeated 29x across cluster][0m
[36m(_train_fn pid=2962785)[0m Updating learning rate to 0.0005231605070144167[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2962785)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2962785)[0m Epoch: 5 cost time: 2.921654224395752[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2962785)[0m Epoch: 5, Steps: 529 | Train Loss: 0.3927920 Vali Loss: 0.2501915 Best vali loss: 0.2393532[32m [repeated 3x across cluster][0m

Trial trial-7b1b1_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00275 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00031 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2963864)[0m configuration
[36m(_train_fn pid=2963864)[0m {'batch_size': 32, 'learning_rate': 0.0003137643360513956, 'd_model': 64, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0027495421398313906, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=2963864)[0m Use GPU: cuda:0
[36m(_train_fn pid=2963864)[0m train 8449
[36m(_train_fn pid=2963864)[0m val 2785
[36m(_train_fn pid=2963864)[0m start_epoch 0
[36m(_train_fn pid=2963864)[0m max_epoch 8
[36m(_train_fn pid=2963864)[0m Validation loss decreased (inf --> 0.2217).  Saving model state dict ...
[36m(_train_fn pid=2962785)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2962785)[0m 	iters: 200, epoch: 7 | loss: 0.2685125[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2962785)[0m 	speed: 0.0054s/iter; left time: 4.6767s[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2963864)[0m Validation loss decreased (0.2217 --> 0.2190).  Saving model state dict ...
[36m(_train_fn pid=2963864)[0m Updating learning rate to 0.0001568821680256978[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2963864)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2963864)[0m Epoch: 2 cost time: 1.2016329765319824[32m [repeated 5x across cluster][0m
2024-08-26 14:06:23,946	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:24,428	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:25,477	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:27,046	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:28,611	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2963864)[0m Epoch: 2, Steps: 265 | Train Loss: 0.3140503 Vali Loss: 0.2190128 Best vali loss: 0.2190128[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2963864)[0m Validation loss decreased (0.2190 --> 0.2166).  Saving model state dict ...

Trial trial-7b1b1_00007 completed after 7 iterations at 2024-08-26 14:06:24. Total running time: 56s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             3.28316 â”‚
â”‚ time_total_s                                23.93014 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.23935 â”‚
â”‚ train_loss                                   0.28161 â”‚
â”‚ valid_loss                                    0.2472 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2962785)[0m Early stopping
[36m(_train_fn pid=2963486)[0m Validation loss decreased (inf --> 0.2208).  Saving model state dict ...
[36m(_train_fn pid=2963864)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2963864)[0m 	iters: 100, epoch: 6 | loss: 0.3571074[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2963864)[0m 	speed: 0.0109s/iter; left time: 7.5845s[32m [repeated 23x across cluster][0m

Trial status: 7 TERMINATED | 3 RUNNING
Current time: 2024-08-26 14:06:27. Total running time: 1min 0s
Logical resource usage: 3.0/32 CPUs, 0.75/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 7b1b1_00006 with best_valid_loss=0.2159335885083226 and params={'batch_size': 8, 'learning_rate': 0.00012008730323502788, 'd_model': 128, 'd_core': 128, 'e_layers': 1, 'dropout': 0.007463716154408458, 'lradj': 'type1', 'd_ff': 384}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-7b1b1_00002   RUNNING                 8       0.00147688          256              2        128            4   0.00745471    type1          5           51.802         0.561192       0.270988            0.269757 â”‚
â”‚ trial-7b1b1_00008   RUNNING                 8       0.000596063         128              1        512            3   0.000907388   cosine         1            8.47573       0.431162       0.220752            0.220752 â”‚
â”‚ trial-7b1b1_00009   RUNNING                32       0.000313764          64              1         64            2   0.00274954    type1          5            8.25058       0.236271       0.216932            0.216632 â”‚
â”‚ trial-7b1b1_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         4            8.05012       0.353216       0.224278            0.216856 â”‚
â”‚ trial-7b1b1_00001   TERMINATED             64       0.00121134          512              2         64            1   0.00220905    cosine         5            4.59866       0.23478        0.248358            0.225718 â”‚
â”‚ trial-7b1b1_00003   TERMINATED             16       0.000596437          32              4        256            4   0.00144806    type1          5           29.6915        0.277647       0.224686            0.220809 â”‚
â”‚ trial-7b1b1_00004   TERMINATED            128       0.000269902         128              2         64            3   0.00364658    type1          7            4.17322     nan              0.217074            0.216709 â”‚
â”‚ trial-7b1b1_00005   TERMINATED              8       0.000170123         128              2        256            2   0.0111126     cosine         4           32.3202        0.406762       0.229436            0.220295 â”‚
â”‚ trial-7b1b1_00006   TERMINATED              8       0.000120087         128              3        128            1   0.00746372    type1          6           36.1224        0.259382       0.216554            0.215934 â”‚
â”‚ trial-7b1b1_00007   TERMINATED             16       0.00169495          512              4         64            2   0.0052938     cosine         7           23.9301        0.281609       0.247198            0.239353 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2963864)[0m Updating learning rate to 1.9610271003212225e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2963864)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2963864)[0m Epoch: 5 cost time: 1.3051199913024902[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2963864)[0m Epoch: 5, Steps: 265 | Train Loss: 0.2362708 Vali Loss: 0.2169320 Best vali loss: 0.2166320[32m [repeated 4x across cluster][0m

Trial trial-7b1b1_00009 completed after 6 iterations at 2024-08-26 14:06:28. Total running time: 1min 0s
[36m(_train_fn pid=2963864)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104/trial-7b1b1_00009_9_alpha_d_ff=1,batch_size=32,d_core=64,d_model=64,dropout=0.0027,e_layers=2,learning_rate=0.0003,lradj=type1_2024-08-26_14-06-17/checkpoint_000005)[32m [repeated 5x across cluster][0m
2024-08-26 14:06:29,688	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:30,436	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:34,739	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2963486)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104/trial-7b1b1_00008_8_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0009,e_layers=3,learning_rate=0.0006,lradj=cosine_2024-08-26_14-06-11/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 14:06:39,587	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:39,598	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104' in 0.0021s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                               1.562 â”‚
â”‚ time_total_s                                 9.81258 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21663 â”‚
â”‚ train_loss                                   0.42807 â”‚
â”‚ valid_loss                                   0.21744 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2963864)[0m Early stopping

Trial trial-7b1b1_00002 completed after 6 iterations at 2024-08-26 14:06:30. Total running time: 1min 2s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             9.35212 â”‚
â”‚ time_total_s                                 61.1541 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.26976 â”‚
â”‚ train_loss                                   0.43943 â”‚
â”‚ valid_loss                                   0.27097 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2959982)[0m Early stopping
[36m(_train_fn pid=2959982)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2963486)[0m 	iters: 700, epoch: 3 | loss: 0.1875009[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2963486)[0m 	speed: 0.0041s/iter; left time: 22.9808s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2959982)[0m Updating learning rate to 4.615257631932046e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2959982)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2959982)[0m Epoch: 6 cost time: 8.409582138061523[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2959982)[0m Epoch: 6, Steps: 1057 | Train Loss: 0.4394313 Vali Loss: 0.2709723 Best vali loss: 0.2697571[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2963486)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2963486)[0m 	iters: 800, epoch: 4 | loss: 0.2085151[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2963486)[0m 	speed: 0.0040s/iter; left time: 17.9925s[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2963486)[0m Updating learning rate to 0.0004120831523731283
[36m(_train_fn pid=2963486)[0m saving checkpoint...
[36m(_train_fn pid=2963486)[0m Epoch: 3 cost time: 4.466991662979126
[36m(_train_fn pid=2963486)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.2841005 Vali Loss: 0.2248180 Best vali loss: 0.2207522

Trial trial-7b1b1_00008 completed after 4 iterations at 2024-08-26 14:06:39. Total running time: 1min 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b1b1_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             4.84565 â”‚
â”‚ time_total_s                                26.76406 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22075 â”‚
â”‚ train_loss                                   0.26234 â”‚
â”‚ valid_loss                                   0.24927 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:06:39. Total running time: 1min 11s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 7b1b1_00006 with best_valid_loss=0.2159335885083226 and params={'batch_size': 8, 'learning_rate': 0.00012008730323502788, 'd_model': 128, 'd_core': 128, 'e_layers': 1, 'dropout': 0.007463716154408458, 'lradj': 'type1', 'd_ff': 384}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-7b1b1_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         4            8.05012       0.353216       0.224278            0.216856 â”‚
â”‚ trial-7b1b1_00001   TERMINATED             64       0.00121134          512              2         64            1   0.00220905    cosine         5            4.59866       0.23478        0.248358            0.225718 â”‚
â”‚ trial-7b1b1_00002   TERMINATED              8       0.00147688          256              2        128            4   0.00745471    type1          6           61.1541        0.439431       0.270972            0.269757 â”‚
â”‚ trial-7b1b1_00003   TERMINATED             16       0.000596437          32              4        256            4   0.00144806    type1          5           29.6915        0.277647       0.224686            0.220809 â”‚
â”‚ trial-7b1b1_00004   TERMINATED            128       0.000269902         128              2         64            3   0.00364658    type1          7            4.17322     nan              0.217074            0.216709 â”‚
â”‚ trial-7b1b1_00005   TERMINATED              8       0.000170123         128              2        256            2   0.0111126     cosine         4           32.3202        0.406762       0.229436            0.220295 â”‚
â”‚ trial-7b1b1_00006   TERMINATED              8       0.000120087         128              3        128            1   0.00746372    type1          6           36.1224        0.259382       0.216554            0.215934 â”‚
â”‚ trial-7b1b1_00007   TERMINATED             16       0.00169495          512              4         64            2   0.0052938     cosine         7           23.9301        0.281609       0.247198            0.239353 â”‚
â”‚ trial-7b1b1_00008   TERMINATED              8       0.000596063         128              1        512            3   0.000907388   cosine         4           26.7641        0.262338       0.249275            0.220752 â”‚
â”‚ trial-7b1b1_00009   TERMINATED             32       0.000313764          64              1         64            2   0.00274954    type1          6            9.81258       0.428073       0.217437            0.216632 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 0.00012008730323502788, 'd_model': 128, 'd_core': 128, 'e_layers': 1, 'dropout': 0.007463716154408458, 'lradj': 'type1', 'd_ff': 384}
[36m(_train_fn pid=2963486)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed104/trial-7b1b1_00008_8_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0009,e_layers=3,learning_rate=0.0006,lradj=cosine_2024-08-26_14-06-11/checkpoint_000003)
[36m(_train_fn pid=2963486)[0m Updating learning rate to 0.0002980314529901159
[36m(_train_fn pid=2963486)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2963486)[0m saving checkpoint...
[36m(_train_fn pid=2963486)[0m Epoch: 4 cost time: 4.276971101760864
[36m(_train_fn pid=2963486)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.2623375 Vali Loss: 0.2492745 Best vali loss: 0.2207522
[36m(_train_fn pid=2963486)[0m Early stopping
[36m(_train_fn pid=2963486)[0m 	iters: 1000, epoch: 4 | loss: 0.3272193[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2963486)[0m 	speed: 0.0040s/iter; left time: 16.9975s[32m [repeated 2x across cluster][0m


Time taken (4 parallel trials): 76 seconds


2024-08-26 14:06:43,733	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:06:44,083	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:06:44,089	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:06:44,096	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:06:48,256	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2967239)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed105/trial-a8a47_00000_0_2024-08-26_14-06-44/checkpoint_000000)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed105   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator      â”‚
â”‚ Scheduler                        FIFOScheduler              â”‚
â”‚ Number of trials                 10                         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed105
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-06-43_215230_2965018/artifacts/2024-08-26_14-06-44/ETTh2_96_96_test_seed105/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:06:44. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-a8a47_00000   PENDING              32        0.0003             128              1         64            2   0            cosine  â”‚
â”‚ trial-a8a47_00001   PENDING              16        0.00480717         256              3         32            3   0.00234066   type1   â”‚
â”‚ trial-a8a47_00002   PENDING              16        0.00292218          64              1        128            3   0.00173261   type1   â”‚
â”‚ trial-a8a47_00003   PENDING              16        0.00146663         128              1         32            1   0.00193135   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-a8a47_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00173 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00292 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2967241)[0m configuration
[36m(_train_fn pid=2967241)[0m {'batch_size': 16, 'learning_rate': 0.002922184622864955, 'd_model': 64, 'd_core': 128, 'e_layers': 3, 'dropout': 0.0017326117520531535, 'lradj': 'type1', 'd_ff': 64}

Trial trial-a8a47_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-a8a47_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00234 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00481 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-a8a47_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00193 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00147 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2967242)[0m {'batch_size': 16, 'learning_rate': 0.0014666333252057943, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.0019313511099346109, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2967242)[0m Use GPU: cuda:0
[36m(_train_fn pid=2967242)[0m train 8449
[36m(_train_fn pid=2967242)[0m val 2785
[36m(_train_fn pid=2967242)[0m start_epoch 0
[36m(_train_fn pid=2967242)[0m max_epoch 8
[36m(_train_fn pid=2967239)[0m 	iters: 100, epoch: 1 | loss: 0.3167691
[36m(_train_fn pid=2967239)[0m 	speed: 0.0113s/iter; left time: 22.7926s
[36m(_train_fn pid=2967239)[0m Updating learning rate to 0.000288581929876693
[36m(_train_fn pid=2967239)[0m saving checkpoint...
[36m(_train_fn pid=2967239)[0m Validation loss decreased (inf --> 0.2204).  Saving model state dict ...
[36m(_train_fn pid=2967239)[0m Epoch: 1 cost time: 1.7912628650665283
2024-08-26 14:06:50,100	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2967239)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed105/trial-a8a47_00000_0_2024-08-26_14-06-44/checkpoint_000001)
2024-08-26 14:06:50,214	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:51,534	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:51,571	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:51,712	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:53,565	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2967239)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed105/trial-a8a47_00000_0_2024-08-26_14-06-44/checkpoint_000003)[32m [repeated 5x across cluster][0m
2024-08-26 14:06:53,763	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:55,440	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:56,439	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:56,448	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:57,102	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:06:59,671	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2968443)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed105/trial-a8a47_00004_4_alpha_d_ff=1,batch_size=16,d_core=128,d_model=128,dropout=0.0067,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_14-06-55/checkpoint_000000)[32m [repeated 6x across cluster][0m
2024-08-26 14:07:00,113	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:00,597	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:00,606	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2967239)[0m Epoch: 1, Steps: 265 | Train Loss: 0.3192727 Vali Loss: 0.2203734 Best vali loss: 0.2203734
[36m(_train_fn pid=2967239)[0m Updating learning rate to 0.00025606601717798207
[36m(_train_fn pid=2967239)[0m saving checkpoint...
[36m(_train_fn pid=2967239)[0m Validation loss decreased (0.2204 --> 0.2171).  Saving model state dict ...
[36m(_train_fn pid=2967239)[0m Epoch: 2 cost time: 1.5459773540496826
[36m(_train_fn pid=2967239)[0m Epoch: 2, Steps: 265 | Train Loss: 0.4532978 Vali Loss: 0.2171488 Best vali loss: 0.2171488
[36m(_train_fn pid=2967240)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2967240)[0m {'batch_size': 16, 'learning_rate': 0.004807170825485761, 'd_model': 256, 'd_core': 32, 'e_layers': 3, 'dropout': 0.0023406602108730656, 'lradj': 'type1', 'd_ff': 768}
[36m(_train_fn pid=2967239)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2967241)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2967241)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2967241)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2967241)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2967241)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2967239)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2967242)[0m 	iters: 400, epoch: 2 | loss: 0.2435019[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=2967242)[0m 	speed: 0.0056s/iter; left time: 18.4101s[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=2967239)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2967239)[0m Updating learning rate to 0.00015[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2967239)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2967241)[0m Validation loss decreased (inf --> 0.2294).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2967239)[0m Epoch: 4 cost time: 1.5662853717803955[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2967239)[0m Epoch: 4, Steps: 265 | Train Loss: 0.3228141 Vali Loss: 0.2217469 Best vali loss: 0.2171488[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2967239)[0m Early stopping

Trial trial-a8a47_00000 completed after 5 iterations at 2024-08-26 14:06:55. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.87193 â”‚
â”‚ time_total_s                                 9.72359 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21715 â”‚
â”‚ train_loss                                   0.28179 â”‚
â”‚ valid_loss                                   0.22578 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-a8a47_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00673 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00041 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2968443)[0m configuration
[36m(_train_fn pid=2968443)[0m {'batch_size': 16, 'learning_rate': 0.0004080499753514232, 'd_model': 128, 'd_core': 128, 'e_layers': 2, 'dropout': 0.006733084263154886, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2968443)[0m Use GPU: cuda:0
[36m(_train_fn pid=2967241)[0m EarlyStopping counter: 1 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2968443)[0m train 8449
[36m(_train_fn pid=2968443)[0m val 2785
[36m(_train_fn pid=2968443)[0m start_epoch 0
[36m(_train_fn pid=2968443)[0m max_epoch 8
[36m(_train_fn pid=2967241)[0m 	iters: 100, epoch: 3 | loss: 0.2120229[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2967241)[0m 	speed: 0.0150s/iter; left time: 45.9955s[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2967242)[0m Updating learning rate to 0.0010139448000582865[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2967242)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2967242)[0m Epoch: 3 cost time: 2.5078587532043457[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2967242)[0m Epoch: 3, Steps: 529 | Train Loss: 0.3591055 Vali Loss: 0.2275886 Best vali loss: 0.2185425[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2968443)[0m Validation loss decreased (inf --> 0.2188).  Saving model state dict ...

Trial trial-a8a47_00003 completed after 4 iterations at 2024-08-26 14:07:00. Total running time: 16s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                              3.0083 â”‚
â”‚ time_total_s                                14.39472 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21854 â”‚
â”‚ train_loss                                   0.36269 â”‚
â”‚ valid_loss                                   0.24794 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2967240)[0m Validation loss decreased (0.2719 --> 0.2709).  Saving model state dict ...
[36m(_train_fn pid=2967242)[0m Early stopping

Trial trial-a8a47_00005 started with configuration:
2024-08-26 14:07:01,627	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:03,478	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:03,775	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:05,010	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2968907)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed105/trial-a8a47_00005_5_alpha_d_ff=1,batch_size=64,d_core=512,d_model=256,dropout=0.0021,e_layers=4,learning_rate=0.0008,lradj=cosine_2024-08-26_14-07-00/checkpoint_000001)[32m [repeated 7x across cluster][0m
2024-08-26 14:07:05,053	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:05,102	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:05,398	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:06,396	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:07,841	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:09,680	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:10,015	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:10,029	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2967240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed105/trial-a8a47_00001_1_alpha_d_ff=3,batch_size=16,d_core=32,d_model=256,dropout=0.0023,e_layers=3,learning_rate=0.0048,lradj=type1_2024-08-26_14-06-44/checkpoint_000004)[32m [repeated 7x across cluster][0m
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00207 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00081 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2968907)[0m {'batch_size': 64, 'learning_rate': 0.000813671667441634, 'd_model': 256, 'd_core': 512, 'e_layers': 4, 'dropout': 0.002073237293402548, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2968907)[0m configuration
[36m(_train_fn pid=2968907)[0m Use GPU: cuda:0
[36m(_train_fn pid=2968443)[0m EarlyStopping counter: 1 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2968907)[0m train 8449
[36m(_train_fn pid=2968907)[0m val 2785
[36m(_train_fn pid=2968907)[0m start_epoch 0
[36m(_train_fn pid=2968907)[0m max_epoch 8
[36m(_train_fn pid=2968443)[0m 	iters: 300, epoch: 3 | loss: 0.2519626[32m [repeated 32x across cluster][0m
[36m(_train_fn pid=2968443)[0m 	speed: 0.0028s/iter; left time: 8.1489s[32m [repeated 32x across cluster][0m
[36m(_train_fn pid=2968907)[0m Updating learning rate to 0.000782703133587645[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2968907)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2968907)[0m Epoch: 1 cost time: 1.5188462734222412[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2968907)[0m Epoch: 1, Steps: 133 | Train Loss: 0.6411344 Vali Loss: 0.2224192 Best vali loss: 0.2224192[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2968907)[0m Validation loss decreased (inf --> 0.2224).  Saving model state dict ...
[36m(_train_fn pid=2967240)[0m Validation loss decreased (0.2709 --> 0.2707).  Saving model state dict ...

Trial trial-a8a47_00004 completed after 4 iterations at 2024-08-26 14:07:05. Total running time: 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.91712 â”‚
â”‚ time_total_s                                 8.39763 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21884 â”‚
â”‚ train_loss                                   0.29994 â”‚
â”‚ valid_loss                                   0.22511 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2968443)[0m Early stopping
[36m(_train_fn pid=2967241)[0m Validation loss decreased (0.2294 --> 0.2258).  Saving model state dict ...

Trial trial-a8a47_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00451 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                       0.0014 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2969646)[0m configuration
[36m(_train_fn pid=2969646)[0m {'batch_size': 32, 'learning_rate': 0.0014020717222324281, 'd_model': 64, 'd_core': 64, 'e_layers': 3, 'dropout': 0.004513789459348224, 'lradj': 'cosine', 'd_ff': 64}
[36m(_train_fn pid=2969646)[0m Use GPU: cuda:0
[36m(_train_fn pid=2969646)[0m train 8449
[36m(_train_fn pid=2969646)[0m val 2785
[36m(_train_fn pid=2968907)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2969646)[0m start_epoch 0
[36m(_train_fn pid=2969646)[0m max_epoch 8
[36m(_train_fn pid=2967240)[0m 	iters: 300, epoch: 5 | loss: 0.3660699[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2967240)[0m 	speed: 0.0088s/iter; left time: 16.0582s[32m [repeated 20x across cluster][0m

Trial trial-a8a47_00005 completed after 4 iterations at 2024-08-26 14:07:07. Total running time: 23s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                              1.4428 â”‚
â”‚ time_total_s                                  6.2032 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22242 â”‚
â”‚ train_loss                                   0.25393 â”‚
â”‚ valid_loss                                   0.23698 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2968907)[0m Updating learning rate to 0.000406835833720817[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2968907)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2968907)[0m Epoch: 4 cost time: 1.2433958053588867[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2968907)[0m Epoch: 4, Steps: 133 | Train Loss: 0.2539323 Vali Loss: 0.2369775 Best vali loss: 0.2224192[32m [repeated 6x across cluster][0m

Trial trial-a8a47_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00454 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00113 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2969871)[0m {'batch_size': 64, 'learning_rate': 0.001133470493569194, 'd_model': 64, 'd_core': 256, 'e_layers': 4, 'dropout': 0.004542753624023843, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2969646)[0m Validation loss decreased (inf --> 0.2299).  Saving model state dict ...
2024-08-26 14:07:10,871	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:11,603	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:11,729	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:12,645	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:13,555	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:13,686	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:14,437	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:14,481	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2968907)[0m Early stopping
[36m(_train_fn pid=2969646)[0m Validation loss decreased (0.2299 --> 0.2252).  Saving model state dict ...
[36m(_train_fn pid=2969871)[0m configuration
[36m(_train_fn pid=2969871)[0m Use GPU: cuda:0
[36m(_train_fn pid=2969871)[0m train 8449
[36m(_train_fn pid=2969871)[0m val 2785
[36m(_train_fn pid=2969871)[0m EarlyStopping counter: 2 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2969871)[0m start_epoch 0
[36m(_train_fn pid=2969871)[0m max_epoch 8
[36m(_train_fn pid=2967240)[0m 	iters: 400, epoch: 6 | loss: 0.5946949[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2967240)[0m 	speed: 0.0071s/iter; left time: 8.4617s[32m [repeated 20x across cluster][0m

Trial trial-a8a47_00007 completed after 4 iterations at 2024-08-26 14:07:13. Total running time: 29s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.90755 â”‚
â”‚ time_total_s                                 4.19653 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22677 â”‚
â”‚ train_loss                                   0.34347 â”‚
â”‚ valid_loss                                   0.23055 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2969871)[0m Early stopping
[36m(_train_fn pid=2969646)[0m Updating learning rate to 0.0009693106706591833[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2969646)[0m saving checkpoint...[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2969646)[0m Epoch: 3 cost time: 1.7719485759735107[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2969646)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3483345 Vali Loss: 0.2347457 Best vali loss: 0.2252059[32m [repeated 9x across cluster][0m

Trial status: 5 TERMINATED | 3 RUNNING | 1 PENDING
Current time: 2024-08-26 14:07:14. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: a8a47_00000 with best_valid_loss=0.2171487552259298 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-a8a47_00001   RUNNING                16       0.00480717          256              3         32            3   0.00234066   type1          5           24.3056        0.440393       0.270735            0.270697 â”‚
â”‚ trial-a8a47_00002   RUNNING                16       0.00292218           64              1        128            3   0.00173261   type1          5           24.3411        0.323443       0.235349            0.225826 â”‚
â”‚ trial-a8a47_00006   RUNNING                32       0.00140207           64              1         64            3   0.00451379   cosine         3            6.75817       0.348334       0.234746            0.225206 â”‚
â”‚ trial-a8a47_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5            9.72359       0.281792       0.225776            0.217149 â”‚
â”‚ trial-a8a47_00003   TERMINATED             16       0.00146663          128              1         32            1   0.00193135   cosine         4           14.3947        0.362694       0.247942            0.218543 â”‚
â”‚ trial-a8a47_00004   TERMINATED             16       0.00040805          128              1        128            2   0.00673308   type1          4            8.39763       0.299938       0.22511             0.218836 â”‚
â”‚ trial-a8a47_00005   TERMINATED             64       0.000813672         256              1        512            4   0.00207324   cosine         4            6.2032        0.253932       0.236977            0.222419 â”‚
â”‚ trial-a8a47_00007   TERMINATED             64       0.00113347           64              2        256            4   0.00454275   type1          4            4.19653       0.343473       0.230549            0.226769 â”‚
â”‚ trial-a8a47_00008   PENDING               128       0.00023712          256              1        256            2   0.00161947   cosine                                                                                â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2969871)[0m Validation loss decreased (inf --> 0.2268).  Saving model state dict ...

Trial trial-a8a47_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00162 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00024 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2970588)[0m configuration
2024-08-26 14:07:15,568	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2969646)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed105/trial-a8a47_00006_6_alpha_d_ff=1,batch_size=32,d_core=64,d_model=64,dropout=0.0045,e_layers=3,learning_rate=0.0014,lradj=cosine_2024-08-26_14-07-05/checkpoint_000003)[32m [repeated 10x across cluster][0m
2024-08-26 14:07:16,418	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:17,030	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:17,642	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:18,282	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:18,392	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:18,887	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:19,397	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:19,658	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:19,701	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:21,958	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2970588)[0m {'batch_size': 128, 'learning_rate': 0.00023711976209091006, 'd_model': 256, 'd_core': 256, 'e_layers': 2, 'dropout': 0.0016194670808432976, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2970588)[0m Use GPU: cuda:0
[36m(_train_fn pid=2970588)[0m train 8449
[36m(_train_fn pid=2970588)[0m val 2785
[36m(_train_fn pid=2970588)[0m start_epoch 0
[36m(_train_fn pid=2970588)[0m max_epoch 8
[36m(_train_fn pid=2970588)[0m Validation loss decreased (inf --> 0.2187).  Saving model state dict ...
[36m(_train_fn pid=2970588)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2187058 Best vali loss: 0.2187058
[36m(_train_fn pid=2970588)[0m Validation loss decreased (0.2187 --> 0.2152).  Saving model state dict ...
[36m(_train_fn pid=2970588)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2152016 Best vali loss: 0.2152016
[36m(_train_fn pid=2970588)[0m Validation loss decreased (0.2152 --> 0.2144).  Saving model state dict ...
[36m(_train_fn pid=2970588)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2143745 Best vali loss: 0.2143745
[36m(_train_fn pid=2969646)[0m EarlyStopping counter: 2 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2967240)[0m 	iters: 400, epoch: 7 | loss: 0.3977602[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2967240)[0m 	speed: 0.0108s/iter; left time: 7.1228s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2970588)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2207828 Best vali loss: 0.2143745

Trial trial-a8a47_00006 completed after 5 iterations at 2024-08-26 14:07:18. Total running time: 34s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             2.82065 â”‚
â”‚ time_total_s                                11.45877 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22521 â”‚
â”‚ train_loss                                   0.31012 â”‚
â”‚ valid_loss                                   0.23666 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2969646)[0m Early stopping
[36m(_train_fn pid=2970588)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2236794 Best vali loss: 0.2143745

Trial trial-a8a47_00008 completed after 6 iterations at 2024-08-26 14:07:19. Total running time: 35s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             0.50749 â”‚
â”‚ time_total_s                                 4.32265 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21437 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.22627 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2970588)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2262661 Best vali loss: 0.2143745
[36m(_train_fn pid=2970588)[0m Early stopping
[36m(_train_fn pid=2970588)[0m Updating learning rate to 3.472538518154336e-05[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2970588)[0m saving checkpoint...[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2970588)[0m Epoch: 6 cost time: 0.3998146057128906[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2969646)[0m Epoch: 5, Steps: 265 | Train Loss: 0.3101174 Vali Loss: 0.2366640 Best vali loss: 0.2252059[32m [repeated 4x across cluster][0m

Trial trial-a8a47_00001 completed after 7 iterations at 2024-08-26 14:07:19. Total running time: 35s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             5.21842 â”‚
â”‚ time_total_s                                33.92897 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                               0.2707 â”‚
â”‚ train_loss                                   0.64642 â”‚
â”‚ valid_loss                                    0.2708 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-a8a47_00002 completed after 7 iterations at 2024-08-26 14:07:19. Total running time: 35s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             5.21746 â”‚
â”‚ time_total_s                                34.02256 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.22583 â”‚
â”‚ train_loss                                   0.21049 â”‚
â”‚ valid_loss                                   0.24378 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-a8a47_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00809 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2971290)[0m {'batch_size': 32, 'learning_rate': 0.00010769052362778539, 'd_model': 128, 'd_core': 128, 'e_layers': 3, 'dropout': 0.008085624680094562, 'lradj': 'type1', 'd_ff': 384}
[36m(_train_fn pid=2971290)[0m configuration
[36m(_train_fn pid=2971290)[0m Use GPU: cuda:0
[36m(_train_fn pid=2971290)[0m train 8449
[36m(_train_fn pid=2971290)[0m val 2785
[36m(_train_fn pid=2971290)[0m start_epoch 0
[36m(_train_fn pid=2971290)[0m max_epoch 8
[36m(_train_fn pid=2971290)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed105/trial-a8a47_00009_9_alpha_d_ff=3,batch_size=32,d_core=128,d_model=128,dropout=0.0081,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_14-07-18/checkpoint_000000)[32m [repeated 10x across cluster][0m
2024-08-26 14:07:23,201	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:24,419	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:25,656	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:26,885	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:28,119	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:28,129	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed105' in 0.0086s.
[36m(_train_fn pid=2971290)[0m Validation loss decreased (inf --> 0.2218).  Saving model state dict ...
[36m(_train_fn pid=2967241)[0m EarlyStopping counter: 3 out of 3[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2971290)[0m Validation loss decreased (0.2218 --> 0.2205).  Saving model state dict ...
[36m(_train_fn pid=2971290)[0m 	iters: 200, epoch: 2 | loss: 0.5567987[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2971290)[0m 	speed: 0.0037s/iter; left time: 6.1834s[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2971290)[0m Validation loss decreased (0.2205 --> 0.2151).  Saving model state dict ...
[36m(_train_fn pid=2967241)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2971290)[0m Updating learning rate to 2.6922630906946346e-05[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2971290)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2971290)[0m Epoch: 3 cost time: 1.0136473178863525[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2971290)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3344735 Vali Loss: 0.2151255 Best vali loss: 0.2151255[32m [repeated 5x across cluster][0m

Trial trial-a8a47_00009 completed after 6 iterations at 2024-08-26 14:07:28. Total running time: 44s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a8a47_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.23224 â”‚
â”‚ time_total_s                                 8.15422 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21513 â”‚
â”‚ train_loss                                   0.52722 â”‚
â”‚ valid_loss                                   0.21547 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:07:28. Total running time: 44s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: a8a47_00008 with best_valid_loss=0.21437448271729145 and params={'batch_size': 128, 'learning_rate': 0.00023711976209091006, 'd_model': 256, 'd_core': 256, 'e_layers': 2, 'dropout': 0.0016194670808432976, 'lradj': 'cosine', 'd_ff': 256}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-a8a47_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5            9.72359       0.281792       0.225776            0.217149 â”‚
â”‚ trial-a8a47_00001   TERMINATED             16       0.00480717          256              3         32            3   0.00234066   type1          7           33.929         0.646418       0.270801            0.270697 â”‚
â”‚ trial-a8a47_00002   TERMINATED             16       0.00292218           64              1        128            3   0.00173261   type1          7           34.0226        0.210495       0.24378             0.225826 â”‚
â”‚ trial-a8a47_00003   TERMINATED             16       0.00146663          128              1         32            1   0.00193135   cosine         4           14.3947        0.362694       0.247942            0.218543 â”‚
â”‚ trial-a8a47_00004   TERMINATED             16       0.00040805          128              1        128            2   0.00673308   type1          4            8.39763       0.299938       0.22511             0.218836 â”‚
â”‚ trial-a8a47_00005   TERMINATED             64       0.000813672         256              1        512            4   0.00207324   cosine         4            6.2032        0.253932       0.236977            0.222419 â”‚
â”‚ trial-a8a47_00006   TERMINATED             32       0.00140207           64              1         64            3   0.00451379   cosine         5           11.4588        0.310117       0.236664            0.225206 â”‚
â”‚ trial-a8a47_00007   TERMINATED             64       0.00113347           64              2        256            4   0.00454275   type1          4            4.19653       0.343473       0.230549            0.226769 â”‚
â”‚ trial-a8a47_00008   TERMINATED            128       0.00023712          256              1        256            2   0.00161947   cosine         6            4.32265     nan              0.226266            0.214374 â”‚
â”‚ trial-a8a47_00009   TERMINATED             32       0.000107691         128              3        128            3   0.00808562   type1          6            8.15422       0.527216       0.215475            0.215125 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 128, 'learning_rate': 0.00023711976209091006, 'd_model': 256, 'd_core': 256, 'e_layers': 2, 'dropout': 0.0016194670808432976, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2971290)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed105/trial-a8a47_00009_9_alpha_d_ff=3,batch_size=32,d_core=128,d_model=128,dropout=0.0081,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_14-07-18/checkpoint_000005)[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2971290)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2971290)[0m 	iters: 200, epoch: 6 | loss: 0.2220740[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2971290)[0m 	speed: 0.0038s/iter; left time: 2.2682s[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2971290)[0m Early stopping
[36m(_train_fn pid=2971290)[0m Updating learning rate to 3.3653288633682933e-06[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2971290)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2971290)[0m Epoch: 6 cost time: 1.047762393951416[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2971290)[0m Epoch: 6, Steps: 265 | Train Loss: 0.5272159 Vali Loss: 0.2154749 Best vali loss: 0.2151255[32m [repeated 3x across cluster][0m


Time taken (4 parallel trials): 49 seconds


2024-08-26 14:07:32,091	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:07:32,458	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:07:32,463	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:07:32,470	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:07:35,383	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2974271)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106/trial-c5847_00003_3_alpha_d_ff=1,batch_size=128,d_core=512,d_model=256,dropout=0.0018,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_14-07-32/checkpoint_000000)
2024-08-26 14:07:35,904	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed106   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator      â”‚
â”‚ Scheduler                        FIFOScheduler              â”‚
â”‚ Number of trials                 10                         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-07-31_659200_2972041/artifacts/2024-08-26_14-07-32/ETTh2_96_96_test_seed106/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:07:32. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-c5847_00000   PENDING              32       0.0003              128              1         64            2   0             cosine  â”‚
â”‚ trial-c5847_00001   PENDING              32       6.02241e-05         512              1         32            4   0.0108984     cosine  â”‚
â”‚ trial-c5847_00002   PENDING              16       0.000223965          32              2         32            3   0.000929108   cosine  â”‚
â”‚ trial-c5847_00003   PENDING             128       6.85696e-05         256              1        512            1   0.00177935    cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c5847_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00178 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00007 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c5847_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                             0.0109 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c5847_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00093 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00022 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c5847_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2974270)[0m configuration
[36m(_train_fn pid=2974270)[0m {'batch_size': 16, 'learning_rate': 0.00022396510954416913, 'd_model': 32, 'd_core': 32, 'e_layers': 3, 'dropout': 0.0009291083829585979, 'lradj': 'cosine', 'd_ff': 64}
[36m(_train_fn pid=2974269)[0m Use GPU: cuda:0
[36m(_train_fn pid=2974270)[0m train 8449
[36m(_train_fn pid=2974270)[0m val 2785
[36m(_train_fn pid=2974270)[0m start_epoch 0
[36m(_train_fn pid=2974270)[0m max_epoch 8
[36m(_train_fn pid=2974271)[0m Updating learning rate to 6.595979607119256e-05
[36m(_train_fn pid=2974271)[0m saving checkpoint...
[36m(_train_fn pid=2974271)[0m Validation loss decreased (inf --> 0.2422).  Saving model state dict ...
[36m(_train_fn pid=2974271)[0m Epoch: 1 cost time: 0.6678931713104248
[36m(_train_fn pid=2974271)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2421844 Best vali loss: 0.2421844
[36m(_train_fn pid=2974268)[0m 	iters: 100, epoch: 1 | loss: 0.2975417
[36m(_train_fn pid=2974268)[0m 	speed: 0.0131s/iter; left time: 26.4417s
[36m(_train_fn pid=2974271)[0m Updating learning rate to 5.852778890569856e-05
[36m(_train_fn pid=2974271)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106/trial-c5847_00003_3_alpha_d_ff=1,batch_size=128,d_core=512,d_model=256,dropout=0.0018,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_14-07-32/checkpoint_000001)
2024-08-26 14:07:36,438	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2974271)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106/trial-c5847_00003_3_alpha_d_ff=1,batch_size=128,d_core=512,d_model=256,dropout=0.0018,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_14-07-32/checkpoint_000002)
2024-08-26 14:07:36,946	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2974271)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106/trial-c5847_00003_3_alpha_d_ff=1,batch_size=128,d_core=512,d_model=256,dropout=0.0018,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_14-07-32/checkpoint_000003)
2024-08-26 14:07:37,415	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:37,469	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:37,988	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:38,471	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:38,654	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:38,939	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:39,787	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:41,415	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:41,464	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2974269)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106/trial-c5847_00001_1_alpha_d_ff=1,batch_size=32,d_core=32,d_model=512,dropout=0.0109,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-07-32/checkpoint_000001)[32m [repeated 8x across cluster][0m
2024-08-26 14:07:41,671	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:41,682	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:42,201	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:42,734	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:43,246	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:43,774	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:43,900	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:44,131	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:44,302	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:44,863	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2974271)[0m saving checkpoint...
[36m(_train_fn pid=2974271)[0m Validation loss decreased (0.2422 --> 0.2288).  Saving model state dict ...
[36m(_train_fn pid=2974271)[0m Epoch: 2 cost time: 0.3726639747619629
[36m(_train_fn pid=2974271)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2288336 Best vali loss: 0.2288336
[36m(_train_fn pid=2974271)[0m Updating learning rate to 4.7405004153649034e-05
[36m(_train_fn pid=2974271)[0m saving checkpoint...
[36m(_train_fn pid=2974271)[0m Validation loss decreased (0.2288 --> 0.2228).  Saving model state dict ...
[36m(_train_fn pid=2974271)[0m Epoch: 3 cost time: 0.3885962963104248
[36m(_train_fn pid=2974271)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2227647 Best vali loss: 0.2227647
[36m(_train_fn pid=2974271)[0m Updating learning rate to 3.428478496524865e-05
[36m(_train_fn pid=2974271)[0m saving checkpoint...
[36m(_train_fn pid=2974271)[0m Validation loss decreased (0.2228 --> 0.2206).  Saving model state dict ...
[36m(_train_fn pid=2974271)[0m Epoch: 4 cost time: 0.36759519577026367
[36m(_train_fn pid=2974271)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2206115 Best vali loss: 0.2206115
[36m(_train_fn pid=2974268)[0m Epoch: 1, Steps: 265 | Train Loss: 0.4216875 Vali Loss: 0.2186006 Best vali loss: 0.2186006
[36m(_train_fn pid=2974271)[0m Validation loss decreased (0.2206 --> 0.2194).  Saving model state dict ...
[36m(_train_fn pid=2974271)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2194224 Best vali loss: 0.2194224
[36m(_train_fn pid=2974271)[0m Validation loss decreased (0.2194 --> 0.2187).  Saving model state dict ...
[36m(_train_fn pid=2974271)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2186788 Best vali loss: 0.2186788
[36m(_train_fn pid=2974271)[0m Validation loss decreased (0.2187 --> 0.2186).  Saving model state dict ...
[36m(_train_fn pid=2974271)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2185557 Best vali loss: 0.2185557

Trial trial-c5847_00003 completed after 8 iterations at 2024-08-26 14:07:38. Total running time: 6s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.46583 â”‚
â”‚ time_total_s                                 4.85711 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21845 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.21845 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2974271)[0m Validation loss decreased (0.2186 --> 0.2184).  Saving model state dict ...
[36m(_train_fn pid=2974271)[0m Epoch: 8, Steps: 67 | Train Loss: nan Vali Loss: 0.2184495 Best vali loss: 0.2184495
[36m(_train_fn pid=2974268)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2974268)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2974270)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2974268)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2974268)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2974268)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2974268)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2974268)[0m EarlyStopping counter: 1 out of 3

Trial trial-c5847_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                             0.0052 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00133 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2974268)[0m Updating learning rate to 0.00025606601717798207[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2974268)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2974269)[0m Validation loss decreased (inf --> 0.2149).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2974268)[0m Epoch: 2 cost time: 1.989976406097412[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2975514)[0m {'batch_size': 64, 'learning_rate': 0.0013338059216942008, 'd_model': 32, 'd_core': 64, 'e_layers': 1, 'dropout': 0.005195867918226476, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=2974269)[0m 	iters: 200, epoch: 2 | loss: 0.2213652[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2974269)[0m 	speed: 0.0088s/iter; left time: 14.6483s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2974269)[0m Validation loss decreased (0.2149 --> 0.2136).  Saving model state dict ...
[36m(_train_fn pid=2974268)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2975514)[0m Epoch: 2, Steps: 133 | Train Loss: 0.3871691 Vali Loss: 0.2186441 Best vali loss: 0.2186441[32m [repeated 7x across cluster][0m

Trial trial-c5847_00000 completed after 4 iterations at 2024-08-26 14:07:43. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.22534 â”‚
â”‚ time_total_s                                 9.82491 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                               0.2186 â”‚
â”‚ train_loss                                   0.35441 â”‚
â”‚ valid_loss                                   0.22359 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2974268)[0m Early stopping
[36m(_train_fn pid=2975514)[0m configuration
[36m(_train_fn pid=2975514)[0m Use GPU: cuda:0
[36m(_train_fn pid=2975514)[0m train 8449
[36m(_train_fn pid=2975514)[0m val 2785
[36m(_train_fn pid=2975514)[0m start_epoch 0
[36m(_train_fn pid=2975514)[0m max_epoch 8

Trial trial-c5847_00004 completed after 7 iterations at 2024-08-26 14:07:44. Total running time: 12s
2024-08-26 14:07:46,337	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:46,369	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:48,243	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2976577)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106/trial-c5847_00006_6_alpha_d_ff=1,batch_size=128,d_core=128,d_model=32,dropout=0.0012,e_layers=4,learning_rate=0.0012,lradj=type1_2024-08-26_14-07-44/checkpoint_000000)[32m [repeated 14x across cluster][0m
2024-08-26 14:07:49,231	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:49,599	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:49,998	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:50,679	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             0.55846 â”‚
â”‚ time_total_s                                 4.38351 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.21862 â”‚
â”‚ train_loss                                   0.25297 â”‚
â”‚ valid_loss                                   0.21899 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c5847_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                              0.001 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00017 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2976370)[0m configuration
[36m(_train_fn pid=2976370)[0m {'batch_size': 8, 'learning_rate': 0.00016589924437186384, 'd_model': 32, 'd_core': 128, 'e_layers': 3, 'dropout': 0.0010000318239144878, 'lradj': 'cosine', 'd_ff': 32}
[36m(_train_fn pid=2976370)[0m Use GPU: cuda:0
[36m(_train_fn pid=2975514)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2975514)[0m Updating learning rate to 2.0840717526471887e-05[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2975514)[0m saving checkpoint...[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2975514)[0m Validation loss decreased (inf --> 0.2222).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2975514)[0m Epoch: 7 cost time: 0.4329643249511719[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2976370)[0m train 8449
[36m(_train_fn pid=2976370)[0m val 2785
[36m(_train_fn pid=2976370)[0m start_epoch 0
[36m(_train_fn pid=2976370)[0m max_epoch 8
[36m(_train_fn pid=2974269)[0m 	iters: 200, epoch: 4 | loss: 0.3722830[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2974269)[0m 	speed: 0.0073s/iter; left time: 8.2751s[32m [repeated 19x across cluster][0m

Trial trial-c5847_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00117 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00124 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2976577)[0m {'batch_size': 128, 'learning_rate': 0.0012411966914890851, 'd_model': 32, 'd_core': 128, 'e_layers': 4, 'dropout': 0.0011716482008300437, 'lradj': 'type1', 'd_ff': 32}
[36m(_train_fn pid=2974270)[0m Validation loss decreased (0.2315 --> 0.2213).  Saving model state dict ...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2974269)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4979749 Vali Loss: 0.2234474 Best vali loss: 0.2136302[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2976577)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2260572 Best vali loss: 0.2260572
[36m(_train_fn pid=2975514)[0m Early stopping
[36m(_train_fn pid=2976577)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2253059 Best vali loss: 0.2253059

Trial trial-c5847_00001 completed after 5 iterations at 2024-08-26 14:07:49. Total running time: 17s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             3.25979 â”‚
â”‚ time_total_s                                15.51875 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21363 â”‚
â”‚ train_loss                                    0.3313 â”‚
â”‚ valid_loss                                    0.2212 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2974269)[0m Early stopping
[36m(_train_fn pid=2976577)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2224486 Best vali loss: 0.2224486
[36m(_train_fn pid=2976577)[0m configuration
[36m(_train_fn pid=2976577)[0m Use GPU: cuda:0
[36m(_train_fn pid=2974269)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2976577)[0m Updating learning rate to 0.0003102991728722713[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2976577)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2976577)[0m Validation loss decreased (inf --> 0.2261).  Saving model state dict ...
[36m(_train_fn pid=2976577)[0m Epoch: 3 cost time: 0.6256892681121826[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2976577)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2251833 Best vali loss: 0.2224486
[36m(_train_fn pid=2976577)[0m train 8449
[36m(_train_fn pid=2976577)[0m val 2785

Trial trial-c5847_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                             0.0118 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00456 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2977086)[0m configuration
2024-08-26 14:07:51,359	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:51,776	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:52,033	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:52,281	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:52,722	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:53,133	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:53,589	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2977086)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106/trial-c5847_00007_7_alpha_d_ff=2,batch_size=128,d_core=32,d_model=64,dropout=0.0118,e_layers=2,learning_rate=0.0046,lradj=cosine_2024-08-26_14-07-49/checkpoint_000003)[32m [repeated 11x across cluster][0m
2024-08-26 14:07:55,389	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:55,634	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:57,087	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:58,265	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:07:59,417	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2977845)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106/trial-c5847_00009_9_alpha_d_ff=4,batch_size=64,d_core=256,d_model=128,dropout=0.0084,e_layers=2,learning_rate=0.0012,lradj=cosine_2024-08-26_14-07-53/checkpoint_000002)[32m [repeated 5x across cluster][0m
2024-08-26 14:08:00,387	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2977086)[0m {'batch_size': 128, 'learning_rate': 0.004564362655988426, 'd_model': 64, 'd_core': 32, 'e_layers': 2, 'dropout': 0.011795716212699692, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2977086)[0m Use GPU: cuda:0
[36m(_train_fn pid=2976577)[0m start_epoch 0
[36m(_train_fn pid=2976577)[0m max_epoch 8
[36m(_train_fn pid=2974270)[0m 	iters: 500, epoch: 3 | loss: 0.8576124[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2974270)[0m 	speed: 0.0067s/iter; left time: 17.9812s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2977086)[0m train 8449
[36m(_train_fn pid=2977086)[0m val 2785
[36m(_train_fn pid=2976577)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2245791 Best vali loss: 0.2224486
[36m(_train_fn pid=2977086)[0m start_epoch 0
[36m(_train_fn pid=2977086)[0m max_epoch 8
[36m(_train_fn pid=2976577)[0m Validation loss decreased (0.2253 --> 0.2224).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-c5847_00006 completed after 6 iterations at 2024-08-26 14:07:52. Total running time: 19s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             0.67264 â”‚
â”‚ time_total_s                                 5.61521 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22245 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.22349 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2976577)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2234928 Best vali loss: 0.2224486
[36m(_train_fn pid=2977086)[0m Validation loss decreased (inf --> 0.2274).  Saving model state dict ...
[36m(_train_fn pid=2974270)[0m Epoch: 3, Steps: 529 | Train Loss: 0.5692083 Vali Loss: 0.2189756 Best vali loss: 0.2189756[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2977086)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2361307 Best vali loss: 0.2274020[32m [repeated 3x across cluster][0m

Trial trial-c5847_00007 completed after 4 iterations at 2024-08-26 14:07:53. Total running time: 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.45215 â”‚
â”‚ time_total_s                                 2.44864 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                               0.2274 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.23224 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c5847_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00166 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00294 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2977525)[0m {'batch_size': 16, 'learning_rate': 0.0029395053706728564, 'd_model': 512, 'd_core': 64, 'e_layers': 3, 'dropout': 0.001661302197238129, 'lradj': 'type1', 'd_ff': 2048}

Trial trial-c5847_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00836 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00116 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2977086)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2974270)[0m EarlyStopping counter: 1 out of 3[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2974270)[0m Updating learning rate to 0.00011198255477208457[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2974270)[0m saving checkpoint...[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2974270)[0m Epoch: 4 cost time: 2.954930067062378[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2977845)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2977845)[0m {'batch_size': 64, 'learning_rate': 0.0011629387904400692, 'd_model': 128, 'd_core': 256, 'e_layers': 2, 'dropout': 0.008356938282916013, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=2977845)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2976370)[0m 	iters: 100, epoch: 2 | loss: 0.5869440[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2976370)[0m 	speed: 0.0271s/iter; left time: 197.6395s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2977845)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2977845)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2977845)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2977845)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2974270)[0m Validation loss decreased (0.2213 --> 0.2190).  Saving model state dict ...
[36m(_train_fn pid=2977845)[0m Validation loss decreased (inf --> 0.2214).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2977845)[0m Epoch: 2, Steps: 133 | Train Loss: 0.4186889 Vali Loss: 0.2246135 Best vali loss: 0.2214234[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2977086)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2322404 Best vali loss: 0.2274020

Trial trial-c5847_00009 completed after 4 iterations at 2024-08-26 14:08:00. Total running time: 27s
2024-08-26 14:08:00,431	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:01,857	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:05,390	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2977525)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106/trial-c5847_00008_8_alpha_d_ff=4,batch_size=16,d_core=64,d_model=512,dropout=0.0017,e_layers=3,learning_rate=0.0029,lradj=type1_2024-08-26_14-07-52/checkpoint_000001)[32m [repeated 4x across cluster][0m
2024-08-26 14:08:06,391	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:06,923	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.96652 â”‚
â”‚ time_total_s                                 5.25939 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22142 â”‚
â”‚ train_loss                                   0.35735 â”‚
â”‚ valid_loss                                   0.22421 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2977845)[0m Early stopping
[36m(_train_fn pid=2977845)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2977845)[0m Updating learning rate to 0.0005814693952200346[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2977845)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2977845)[0m Epoch: 4 cost time: 0.7999546527862549[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2977525)[0m 	iters: 100, epoch: 2 | loss: 0.2223169[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2977525)[0m 	speed: 0.0217s/iter; left time: 78.3424s[32m [repeated 18x across cluster][0m

Trial status: 7 TERMINATED | 3 RUNNING
Current time: 2024-08-26 14:08:02. Total running time: 30s
Logical resource usage: 3.0/32 CPUs, 0.75/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: c5847_00001 with best_valid_loss=0.21363018874473588 and params={'batch_size': 32, 'learning_rate': 6.022409883251087e-05, 'd_model': 512, 'd_core': 32, 'e_layers': 4, 'dropout': 0.010898384841071022, 'lradj': 'cosine', 'd_ff': 512}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-c5847_00002   RUNNING                16       0.000223965          32              2         32            3   0.000929108   cosine         5           27.7822        0.352957       0.218983            0.218976 â”‚
â”‚ trial-c5847_00005   RUNNING                 8       0.000165899          32              1        128            3   0.00100003    cosine         1            9.93061       0.621954       0.228711            0.228711 â”‚
â”‚ trial-c5847_00008   RUNNING                16       0.00293951          512              4         64            3   0.0016613     type1          1            6.81681       0.451419       0.275261            0.275261 â”‚
â”‚ trial-c5847_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         4            9.82491       0.354414       0.223587            0.218601 â”‚
â”‚ trial-c5847_00001   TERMINATED             32       6.02241e-05         512              1         32            4   0.0108984     cosine         5           15.5187        0.331305       0.221205            0.21363  â”‚
â”‚ trial-c5847_00003   TERMINATED            128       6.85696e-05         256              1        512            1   0.00177935    cosine         8            4.85711     nan              0.218449            0.218449 â”‚
â”‚ trial-c5847_00004   TERMINATED             64       0.00133381           32              2         64            1   0.00519587    type1          7            4.38351       0.252968       0.218993            0.218616 â”‚
â”‚ trial-c5847_00006   TERMINATED            128       0.0012412            32              1        128            4   0.00117165    type1          6            5.61521     nan              0.223493            0.222449 â”‚
â”‚ trial-c5847_00007   TERMINATED            128       0.00456436           64              2         32            2   0.0117957     cosine         4            2.44864     nan              0.23224             0.227402 â”‚
â”‚ trial-c5847_00009   TERMINATED             64       0.00116294          128              4        256            2   0.00835694    cosine         4            5.25939       0.357347       0.224206            0.221423 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2977525)[0m Validation loss decreased (inf --> 0.2753).  Saving model state dict ...
[36m(_train_fn pid=2974270)[0m Epoch: 5, Steps: 529 | Train Loss: 0.3529566 Vali Loss: 0.2189831 Best vali loss: 0.2189756[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2977525)[0m Validation loss decreased (0.2753 --> 0.2654).  Saving model state dict ...
[36m(_train_fn pid=2974270)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2976370)[0m Updating learning rate to 0.00014160386253046648[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2976370)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2976370)[0m Epoch: 2 cost time: 9.510820388793945[32m [repeated 3x across cluster][0m

Trial trial-c5847_00002 completed after 6 iterations at 2024-08-26 14:08:06. Total running time: 34s
2024-08-26 14:08:09,564	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:13,451	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2977525)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106/trial-c5847_00008_8_alpha_d_ff=4,batch_size=16,d_core=64,d_model=512,dropout=0.0017,e_layers=3,learning_rate=0.0029,lradj=type1_2024-08-26_14-07-52/checkpoint_000003)[32m [repeated 4x across cluster][0m
2024-08-26 14:08:14,532	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:17,114	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:19,985	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2976370)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106/trial-c5847_00005_5_alpha_d_ff=1,batch_size=8,d_core=128,d_model=32,dropout=0.0010,e_layers=3,learning_rate=0.0002,lradj=cosine_2024-08-26_14-07-43/checkpoint_000003)[32m [repeated 3x across cluster][0m
2024-08-26 14:08:24,465	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:24,469	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106' in 0.0020s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             5.06333 â”‚
â”‚ time_total_s                                32.84557 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21898 â”‚
â”‚ train_loss                                   0.59246 â”‚
â”‚ valid_loss                                   0.21957 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2974270)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2974270)[0m Early stopping
[36m(_train_fn pid=2977525)[0m 	iters: 200, epoch: 3 | loss: 0.8519412[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2977525)[0m 	speed: 0.0072s/iter; left time: 21.2782s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2974270)[0m Epoch: 6, Steps: 529 | Train Loss: 0.5924589 Vali Loss: 0.2195739 Best vali loss: 0.2189756[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2976370)[0m Validation loss decreased (0.2287 --> 0.2204).  Saving model state dict ...
[36m(_train_fn pid=2977525)[0m Updating learning rate to 0.0007348763426682141[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2977525)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2977525)[0m Epoch: 3 cost time: 3.6239542961120605[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2977525)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2976370)[0m 	iters: 900, epoch: 3 | loss: 0.5038896[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2976370)[0m 	speed: 0.0064s/iter; left time: 34.8720s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2977525)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2977525)[0m Epoch: 4, Steps: 529 | Train Loss: 0.5159002 Vali Loss: 0.2693802 Best vali loss: 0.2653960[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2976370)[0m Updating learning rate to 0.00011469306831743176[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2976370)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2976370)[0m Epoch: 3 cost time: 6.714756727218628[32m [repeated 2x across cluster][0m

Trial trial-c5847_00008 completed after 5 iterations at 2024-08-26 14:08:17. Total running time: 44s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             3.66068 â”‚
â”‚ time_total_s                                23.47947 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                               0.2654 â”‚
â”‚ train_loss                                    0.3251 â”‚
â”‚ valid_loss                                   0.27027 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2977525)[0m Early stopping
[36m(_train_fn pid=2976370)[0m 	iters: 500, epoch: 4 | loss: 0.3962470[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2976370)[0m 	speed: 0.0039s/iter; left time: 18.7216s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2977525)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2977525)[0m Epoch: 5, Steps: 529 | Train Loss: 0.3250962 Vali Loss: 0.2702727 Best vali loss: 0.2653960[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2976370)[0m Updating learning rate to 8.294962218593192e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2976370)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2976370)[0m Epoch: 4 cost time: 4.898963689804077[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2976370)[0m 	iters: 700, epoch: 5 | loss: 0.4353231[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2976370)[0m 	speed: 0.0037s/iter; left time: 13.0977s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2976370)[0m EarlyStopping counter: 2 out of 3

Trial trial-c5847_00005 completed after 5 iterations at 2024-08-26 14:08:24. Total running time: 51s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c5847_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             4.47727 â”‚
â”‚ time_total_s                                38.99775 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22039 â”‚
â”‚ train_loss                                   0.52498 â”‚
â”‚ valid_loss                                   0.22076 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:08:24. Total running time: 51s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: c5847_00001 with best_valid_loss=0.21363018874473588 and params={'batch_size': 32, 'learning_rate': 6.022409883251087e-05, 'd_model': 512, 'd_core': 32, 'e_layers': 4, 'dropout': 0.010898384841071022, 'lradj': 'cosine', 'd_ff': 512}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-c5847_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         4            9.82491       0.354414       0.223587            0.218601 â”‚
â”‚ trial-c5847_00001   TERMINATED             32       6.02241e-05         512              1         32            4   0.0108984     cosine         5           15.5187        0.331305       0.221205            0.21363  â”‚
â”‚ trial-c5847_00002   TERMINATED             16       0.000223965          32              2         32            3   0.000929108   cosine         6           32.8456        0.592459       0.219574            0.218976 â”‚
â”‚ trial-c5847_00003   TERMINATED            128       6.85696e-05         256              1        512            1   0.00177935    cosine         8            4.85711     nan              0.218449            0.218449 â”‚
â”‚ trial-c5847_00004   TERMINATED             64       0.00133381           32              2         64            1   0.00519587    type1          7            4.38351       0.252968       0.218993            0.218616 â”‚
â”‚ trial-c5847_00005   TERMINATED              8       0.000165899          32              1        128            3   0.00100003    cosine         5           38.9977        0.524977       0.220763            0.220389 â”‚
â”‚ trial-c5847_00006   TERMINATED            128       0.0012412            32              1        128            4   0.00117165    type1          6            5.61521     nan              0.223493            0.222449 â”‚
â”‚ trial-c5847_00007   TERMINATED            128       0.00456436           64              2         32            2   0.0117957     cosine         4            2.44864     nan              0.23224             0.227402 â”‚
â”‚ trial-c5847_00008   TERMINATED             16       0.00293951          512              4         64            3   0.0016613     type1          5           23.4795        0.325096       0.270273            0.265396 â”‚
â”‚ trial-c5847_00009   TERMINATED             64       0.00116294          128              4        256            2   0.00835694    cosine         4            5.25939       0.357347       0.224206            0.221423 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 32, 'learning_rate': 6.022409883251087e-05, 'd_model': 512, 'd_core': 32, 'e_layers': 4, 'dropout': 0.010898384841071022, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=2976370)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed106/trial-c5847_00005_5_alpha_d_ff=1,batch_size=8,d_core=128,d_model=32,dropout=0.0010,e_layers=3,learning_rate=0.0002,lradj=cosine_2024-08-26_14-07-43/checkpoint_000004)
[36m(_train_fn pid=2976370)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2976370)[0m Early stopping
[36m(_train_fn pid=2976370)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.5249767 Vali Loss: 0.2207628 Best vali loss: 0.2203894[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2976370)[0m Updating learning rate to 5.12061760544321e-05
[36m(_train_fn pid=2976370)[0m saving checkpoint...
[36m(_train_fn pid=2976370)[0m Epoch: 5 cost time: 3.9401605129241943
[36m(_train_fn pid=2976370)[0m 	iters: 1000, epoch: 5 | loss: 0.3028269[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2976370)[0m 	speed: 0.0037s/iter; left time: 11.9723s[32m [repeated 3x across cluster][0m


Time taken (4 parallel trials): 56 seconds


2024-08-26 14:08:28,544	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:08:28,901	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:08:28,906	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:08:28,913	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:08:33,112	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed107   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator      â”‚
â”‚ Scheduler                        FIFOScheduler              â”‚
â”‚ Number of trials                 10                         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-08-28_114705_2979245/artifacts/2024-08-26_14-08-28/ETTh2_96_96_test_seed107/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:08:29. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e72ad_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-e72ad_00001   PENDING               8       0.000451177         256              4        128            2   0.0030252    type1   â”‚
â”‚ trial-e72ad_00002   PENDING              32       0.000338982          32              1        512            4   0.0073023    cosine  â”‚
â”‚ trial-e72ad_00003   PENDING              16       0.000138336          64              3         32            3   0.00552133   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-e72ad_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-e72ad_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                             0.0073 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00034 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-e72ad_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00303 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00045 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-e72ad_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00552 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00014 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2981466)[0m configuration
[36m(_train_fn pid=2981466)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2981466)[0m Use GPU: cuda:0
[36m(_train_fn pid=2981467)[0m {'batch_size': 8, 'learning_rate': 0.00045117719822431287, 'd_model': 256, 'd_core': 128, 'e_layers': 2, 'dropout': 0.003025203453712267, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2981466)[0m train 8449
[36m(_train_fn pid=2981466)[0m val 2785
[36m(_train_fn pid=2981466)[0m start_epoch 0
[36m(_train_fn pid=2981466)[0m max_epoch 8
[36m(_train_fn pid=2981466)[0m 	iters: 100, epoch: 1 | loss: 0.7488903
[36m(_train_fn pid=2981466)[0m 	speed: 0.0117s/iter; left time: 23.5497s
[36m(_train_fn pid=2981466)[0m Updating learning rate to 0.000288581929876693
[36m(_train_fn pid=2981466)[0m saving checkpoint...
[36m(_train_fn pid=2981466)[0m Validation loss decreased (inf --> 0.2186).  Saving model state dict ...
[36m(_train_fn pid=2981466)[0m Epoch: 1 cost time: 1.829813003540039
[36m(_train_fn pid=2981466)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00000_0_2024-08-26_14-08-28/checkpoint_000000)
2024-08-26 14:08:33,942	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:34,893	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:36,630	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:36,724	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:36,838	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:38,579	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2981466)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00000_0_2024-08-26_14-08-28/checkpoint_000003)[32m [repeated 6x across cluster][0m
2024-08-26 14:08:39,179	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:39,693	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:41,397	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:41,624	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:43,676	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2981468)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00002_2_alpha_d_ff=1,batch_size=32,d_core=512,d_model=32,dropout=0.0073,e_layers=4,learning_rate=0.0003,lradj=cosine_2024-08-26_14-08-28/checkpoint_000004)[32m [repeated 5x across cluster][0m
2024-08-26 14:08:45,906	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:45,934	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:46,331	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:46,638	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:48,117	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2981466)[0m Epoch: 1, Steps: 265 | Train Loss: 0.5037695 Vali Loss: 0.2185988 Best vali loss: 0.2185988
[36m(_train_fn pid=2981466)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2981469)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2981469)[0m {'batch_size': 16, 'learning_rate': 0.0001383358995732941, 'd_model': 64, 'd_core': 32, 'e_layers': 3, 'dropout': 0.0055213332460338676, 'lradj': 'cosine', 'd_ff': 192}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2981469)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2981469)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2981469)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2981469)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2981469)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2981466)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2981468)[0m Validation loss decreased (0.2318 --> 0.2235).  Saving model state dict ...
[36m(_train_fn pid=2981467)[0m 	iters: 800, epoch: 1 | loss: 0.3646987[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2981467)[0m 	speed: 0.0055s/iter; left time: 41.9889s[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2981469)[0m Updating learning rate to 0.00013307080290029868[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2981469)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2981469)[0m Validation loss decreased (inf --> 0.2241).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2981469)[0m Epoch: 1 cost time: 5.026185989379883[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2981469)[0m Epoch: 1, Steps: 529 | Train Loss: 0.4138050 Vali Loss: 0.2240912 Best vali loss: 0.2240912[32m [repeated 5x across cluster][0m

Trial trial-e72ad_00000 completed after 4 iterations at 2024-08-26 14:08:38. Total running time: 9s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.85316 â”‚
â”‚ time_total_s                                 8.09624 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                               0.2186 â”‚
â”‚ train_loss                                   0.36386 â”‚
â”‚ valid_loss                                   0.22452 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2981466)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2981466)[0m Early stopping
[36m(_train_fn pid=2981468)[0m Validation loss decreased (0.2235 --> 0.2235).  Saving model state dict ...

Trial trial-e72ad_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00967 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00018 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2982541)[0m configuration
[36m(_train_fn pid=2982541)[0m {'batch_size': 8, 'learning_rate': 0.00018152490846912284, 'd_model': 512, 'd_core': 64, 'e_layers': 3, 'dropout': 0.00966973829782565, 'lradj': 'cosine', 'd_ff': 1536}
[36m(_train_fn pid=2982541)[0m Use GPU: cuda:0
[36m(_train_fn pid=2982541)[0m train 8449
[36m(_train_fn pid=2982541)[0m val 2785
[36m(_train_fn pid=2982541)[0m start_epoch 0
[36m(_train_fn pid=2982541)[0m max_epoch 8
[36m(_train_fn pid=2981468)[0m Validation loss decreased (0.2235 --> 0.2216).  Saving model state dict ...
[36m(_train_fn pid=2981469)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2981468)[0m 	iters: 100, epoch: 5 | loss: 0.3101581[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2981468)[0m 	speed: 0.0155s/iter; left time: 14.8891s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2981469)[0m Updating learning rate to 0.00011807707612155578[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2981469)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2981467)[0m Validation loss decreased (inf --> 0.2286).  Saving model state dict ...
[36m(_train_fn pid=2981469)[0m Epoch: 2 cost time: 4.092251300811768[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2981469)[0m Epoch: 2, Steps: 529 | Train Loss: 0.3735623 Vali Loss: 0.2279254 Best vali loss: 0.2240912[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2981469)[0m Validation loss decreased (0.2241 --> 0.2184).  Saving model state dict ...
[36m(_train_fn pid=2982541)[0m Validation loss decreased (inf --> 0.2307).  Saving model state dict ...
[36m(_train_fn pid=2981468)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2981468)[0m 	iters: 200, epoch: 7 | loss: 0.2025528[32m [repeated 27x across cluster][0m
[36m(_train_fn pid=2981468)[0m 	speed: 0.0072s/iter; left time: 2.3756s[32m [repeated 27x across cluster][0m

Trial trial-e72ad_00002 completed after 7 iterations at 2024-08-26 14:08:48. Total running time: 19s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             2.20954 â”‚
â”‚ time_total_s                                17.62587 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.22158 â”‚
â”‚ train_loss                                   0.25927 â”‚
â”‚ valid_loss                                   0.22279 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2981468)[0m Early stopping
[36m(_train_fn pid=2981468)[0m Updating learning rate to 1.2901739712943915e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2981468)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2981468)[0m Epoch: 7 cost time: 1.9021971225738525[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2981468)[0m Epoch: 7, Steps: 265 | Train Loss: 0.2592685 Vali Loss: 0.2227865 Best vali loss: 0.2215814[32m [repeated 6x across cluster][0m

Trial trial-e72ad_00005 started with configuration:
[36m(_train_fn pid=2981469)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00003_3_alpha_d_ff=3,batch_size=16,d_core=32,d_model=64,dropout=0.0055,e_layers=3,learning_rate=0.0001,lradj=cosine_2024-08-26_14-08-28/checkpoint_000003)[32m [repeated 6x across cluster][0m
2024-08-26 14:08:49,748	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:52,384	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:52,704	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:53,568	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:53,735	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:57,325	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2983346)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00005_5_alpha_d_ff=4,batch_size=16,d_core=128,d_model=64,dropout=0.0057,e_layers=2,learning_rate=0.0036,lradj=cosine_2024-08-26_14-08-48/checkpoint_000001)[32m [repeated 5x across cluster][0m
2024-08-26 14:08:57,975	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:58,094	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:08:59,364	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:00,731	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:02,055	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00565 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00357 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2983346)[0m configuration
[36m(_train_fn pid=2983346)[0m {'batch_size': 16, 'learning_rate': 0.00356706015415723, 'd_model': 64, 'd_core': 128, 'e_layers': 2, 'dropout': 0.005653916193390973, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2983346)[0m Use GPU: cuda:0
[36m(_train_fn pid=2983346)[0m train 8449
[36m(_train_fn pid=2983346)[0m val 2785
[36m(_train_fn pid=2983346)[0m start_epoch 0
[36m(_train_fn pid=2983346)[0m max_epoch 8
[36m(_train_fn pid=2981469)[0m Validation loss decreased (0.2184 --> 0.2179).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2981468)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2983346)[0m 	iters: 400, epoch: 1 | loss: 0.5059136[32m [repeated 30x across cluster][0m
[36m(_train_fn pid=2983346)[0m 	speed: 0.0049s/iter; left time: 18.8190s[32m [repeated 30x across cluster][0m
[36m(_train_fn pid=2981467)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2981469)[0m Updating learning rate to 4.269852135263678e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2981469)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2981469)[0m Epoch: 5 cost time: 3.209191083908081[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2981469)[0m Epoch: 5, Steps: 529 | Train Loss: 0.4203514 Vali Loss: 0.2191800 Best vali loss: 0.2178721[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2983346)[0m Validation loss decreased (inf --> 0.2614).  Saving model state dict ...
[36m(_train_fn pid=2982541)[0m Validation loss decreased (0.2307 --> 0.2253).  Saving model state dict ...
[36m(_train_fn pid=2983346)[0m Validation loss decreased (0.2614 --> 0.2545).  Saving model state dict ...
[36m(_train_fn pid=2983346)[0m 	iters: 100, epoch: 3 | loss: 0.3155025[32m [repeated 32x across cluster][0m
[36m(_train_fn pid=2983346)[0m 	speed: 0.0137s/iter; left time: 42.0775s[32m [repeated 32x across cluster][0m
[36m(_train_fn pid=2981469)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2981469)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2982541)[0m Updating learning rate to 0.00012549574175092277[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2982541)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2982541)[0m Epoch: 3 cost time: 5.055794715881348[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2982541)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.3328694 Vali Loss: 0.2433765 Best vali loss: 0.2252631[32m [repeated 4x across cluster][0m

Trial status: 2 TERMINATED | 4 RUNNING
Current time: 2024-08-26 14:08:59. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e72ad_00003 with best_valid_loss=0.2178720507630326 and params={'batch_size': 16, 'learning_rate': 0.0001383358995732941, 'd_model': 64, 'd_core': 32, 'e_layers': 3, 'dropout': 0.0055213332460338676, 'lradj': 'cosine', 'd_ff': 192}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e72ad_00001   RUNNING                 8       0.000451177         256              4        128            2   0.0030252    type1          3           22.2173        0.415528       0.232548            0.224095 â”‚
â”‚ trial-e72ad_00003   RUNNING                16       0.000138336          64              3         32            3   0.00552133   cosine         6           27.4685        0.474243       0.218845            0.217872 â”‚
â”‚ trial-e72ad_00004   RUNNING                 8       0.000181525         512              3         64            3   0.00966974   cosine         3           17.9379        0.332869       0.243376            0.225263 â”‚
â”‚ trial-e72ad_00005   RUNNING                16       0.00356706           64              4        128            2   0.00565392   cosine         2            7.68503       0.563704       0.254477            0.254477 â”‚
â”‚ trial-e72ad_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            8.09624       0.363865       0.224524            0.218599 â”‚
â”‚ trial-e72ad_00002   TERMINATED             32       0.000338982          32              1        512            4   0.0073023    cosine         7           17.6259        0.259268       0.222786            0.221581 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2983346)[0m Validation loss decreased (0.2545 --> 0.2372).  Saving model state dict ...

Trial trial-e72ad_00003 completed after 7 iterations at 2024-08-26 14:09:02. Total running time: 33s
2024-08-26 14:09:03,822	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:03,858	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2982541)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=512,dropout=0.0097,e_layers=3,learning_rate=0.0002,lradj=cosine_2024-08-26_14-08-38/checkpoint_000003)[32m [repeated 6x across cluster][0m
2024-08-26 14:09:05,024	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:05,736	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:06,378	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:06,444	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:07,110	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:07,495	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             4.07753 â”‚
â”‚ time_total_s                                31.54602 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.21787 â”‚
â”‚ train_loss                                   0.38705 â”‚
â”‚ valid_loss                                   0.21926 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2981469)[0m Early stopping
[36m(_train_fn pid=2981467)[0m 	iters: 700, epoch: 5 | loss: 0.3027510[32m [repeated 30x across cluster][0m
[36m(_train_fn pid=2981467)[0m 	speed: 0.0044s/iter; left time: 15.5880s[32m [repeated 30x across cluster][0m
[36m(_train_fn pid=2981469)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m

Trial trial-e72ad_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00381 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00291 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2984272)[0m configuration
[36m(_train_fn pid=2984272)[0m {'batch_size': 128, 'learning_rate': 0.002914821435755178, 'd_model': 512, 'd_core': 32, 'e_layers': 2, 'dropout': 0.0038085190166750643, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2984272)[0m Use GPU: cuda:0
[36m(_train_fn pid=2984272)[0m train 8449
[36m(_train_fn pid=2984272)[0m val 2785
[36m(_train_fn pid=2981469)[0m Updating learning rate to 5.265096672995418e-06[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2981469)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2981469)[0m Epoch: 7 cost time: 3.4162700176239014[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2981469)[0m Epoch: 7, Steps: 529 | Train Loss: 0.3870487 Vali Loss: 0.2192648 Best vali loss: 0.2178721[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2983346)[0m Validation loss decreased (0.2372 --> 0.2369).  Saving model state dict ...
[36m(_train_fn pid=2984272)[0m start_epoch 0
[36m(_train_fn pid=2984272)[0m max_epoch 8
[36m(_train_fn pid=2984272)[0m Validation loss decreased (inf --> 0.2479).  Saving model state dict ...
[36m(_train_fn pid=2984272)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2479490 Best vali loss: 0.2479490
[36m(_train_fn pid=2984272)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2483087 Best vali loss: 0.2479490

Trial trial-e72ad_00001 completed after 5 iterations at 2024-08-26 14:09:06. Total running time: 37s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             7.01121 â”‚
â”‚ time_total_s                                35.88577 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                               0.2241 â”‚
â”‚ train_loss                                   0.31952 â”‚
â”‚ valid_loss                                   0.24741 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2984272)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2494228 Best vali loss: 0.2479490

Trial trial-e72ad_00006 completed after 4 iterations at 2024-08-26 14:09:07. Total running time: 38s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                              0.6645 â”‚
â”‚ time_total_s                                 3.54419 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.24795 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.26856 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2981467)[0m Early stopping
[36m(_train_fn pid=2984272)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2685595 Best vali loss: 0.2479490
[36m(_train_fn pid=2984272)[0m Early stopping

Trial trial-e72ad_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00007 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              4 â”‚
â”‚ batch_size                            128 â”‚
â”‚ d_core                                 32 â”‚
â”‚ d_model                                32 â”‚
â”‚ dropout                             0.002 â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0004 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2983346)[0m 	iters: 100, epoch: 6 | loss: 0.2846771[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2983346)[0m 	speed: 0.0075s/iter; left time: 11.1406s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2983346)[0m EarlyStopping counter: 1 out of 3[32m [repeated 6x across cluster][0m

Trial trial-e72ad_00008 started with configuration:
2024-08-26 14:09:09,078	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2984756)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00007_7_alpha_d_ff=4,batch_size=128,d_core=32,d_model=32,dropout=0.0020,e_layers=2,learning_rate=0.0004,lradj=cosine_2024-08-26_14-09-06/checkpoint_000000)[32m [repeated 8x across cluster][0m
2024-08-26 14:09:09,549	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:09,606	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:09,971	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:10,058	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:10,608	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:11,115	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:11,645	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:12,207	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:12,763	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:13,769	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00116 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00493 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2984756)[0m configuration
[36m(_train_fn pid=2984756)[0m {'batch_size': 128, 'learning_rate': 0.0003992043493372306, 'd_model': 32, 'd_core': 32, 'e_layers': 2, 'dropout': 0.0019993570942256106, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2984756)[0m Use GPU: cuda:0
[36m(_train_fn pid=2984894)[0m configuration
[36m(_train_fn pid=2984894)[0m {'batch_size': 8, 'learning_rate': 0.004926921659347528, 'd_model': 128, 'd_core': 128, 'e_layers': 4, 'dropout': 0.001159367583795647, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=2984894)[0m Use GPU: cuda:0
[36m(_train_fn pid=2984894)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2984894)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2983346)[0m Updating learning rate to 0.0011010026654557975[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2983346)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2983346)[0m Epoch: 5 cost time: 3.377786874771118[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2983346)[0m Epoch: 5, Steps: 529 | Train Loss: 0.4519841 Vali Loss: 0.2417764 Best vali loss: 0.2368810[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2984756)[0m start_epoch 0
[36m(_train_fn pid=2984756)[0m max_epoch 8
[36m(_train_fn pid=2984894)[0m start_epoch 0
[36m(_train_fn pid=2984894)[0m max_epoch 8
[36m(_train_fn pid=2984756)[0m Validation loss decreased (0.2476 --> 0.2331).  Saving model state dict ...

Trial trial-e72ad_00004 completed after 5 iterations at 2024-08-26 14:09:09. Total running time: 40s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.74491 â”‚
â”‚ time_total_s                                29.44453 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22526 â”‚
â”‚ train_loss                                   0.24343 â”‚
â”‚ valid_loss                                   0.25731 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2984756)[0m Validation loss decreased (0.2331 --> 0.2260).  Saving model state dict ...
[36m(_train_fn pid=2984756)[0m Validation loss decreased (inf --> 0.2476).  Saving model state dict ...
[36m(_train_fn pid=2984756)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2260160 Best vali loss: 0.2260160[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2984756)[0m Validation loss decreased (0.2260 --> 0.2215).  Saving model state dict ...

Trial trial-e72ad_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00264 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00012 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2984756)[0m Validation loss decreased (0.2215 --> 0.2205).  Saving model state dict ...
[36m(_train_fn pid=2984756)[0m Validation loss decreased (0.2205 --> 0.2196).  Saving model state dict ...
[36m(_train_fn pid=2982541)[0m Early stopping
[36m(_train_fn pid=2984756)[0m Validation loss decreased (0.2196 --> 0.2195).  Saving model state dict ...
[36m(_train_fn pid=2984756)[0m Validation loss decreased (0.2195 --> 0.2194).  Saving model state dict ...

Trial trial-e72ad_00007 completed after 8 iterations at 2024-08-26 14:09:12. Total running time: 43s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.55442 â”‚
â”‚ time_total_s                                 4.82685 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21937 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.21937 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2984894)[0m 	iters: 500, epoch: 1 | loss: 0.1218617[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2984894)[0m 	speed: 0.0086s/iter; left time: 68.5254s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2983346)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m

Trial trial-e72ad_00005 completed after 7 iterations at 2024-08-26 14:09:13. Total running time: 44s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                              3.7966 â”‚
â”‚ time_total_s                                  24.115 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.23688 â”‚
â”‚ train_loss                                   0.31085 â”‚
â”‚ valid_loss                                   0.25377 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2983346)[0m Early stopping
[36m(_train_fn pid=2985354)[0m configuration
2024-08-26 14:09:16,480	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2985354)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00009_9_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0026,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_14-09-09/checkpoint_000000)[32m [repeated 11x across cluster][0m
2024-08-26 14:09:17,833	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:20,576	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:24,958	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2985354)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00009_9_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0026,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_14-09-09/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 14:09:25,512	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:29,135	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:09:33,013	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2984894)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00008_8_alpha_d_ff=4,batch_size=8,d_core=128,d_model=128,dropout=0.0012,e_layers=4,learning_rate=0.0049,lradj=cosine_2024-08-26_14-09-07/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 14:09:33,129	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2984894)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00008_8_alpha_d_ff=4,batch_size=8,d_core=128,d_model=128,dropout=0.0012,e_layers=4,learning_rate=0.0049,lradj=cosine_2024-08-26_14-09-07/checkpoint_000003)[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2984894)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00008_8_alpha_d_ff=4,batch_size=8,d_core=128,d_model=128,dropout=0.0012,e_layers=4,learning_rate=0.0049,lradj=cosine_2024-08-26_14-09-07/checkpoint_000004)
[36m(_train_fn pid=2985354)[0m {'batch_size': 8, 'learning_rate': 0.00011556228694028946, 'd_model': 256, 'd_core': 64, 'e_layers': 1, 'dropout': 0.0026360657982210695, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2985354)[0m Use GPU: cuda:0
[36m(_train_fn pid=2985354)[0m train 8449
[36m(_train_fn pid=2985354)[0m val 2785
[36m(_train_fn pid=2983346)[0m Updating learning rate to 0.00013576314324740497[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2983346)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2983346)[0m Epoch: 7 cost time: 3.1120517253875732[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2983346)[0m Epoch: 7, Steps: 529 | Train Loss: 0.3108543 Vali Loss: 0.2537723 Best vali loss: 0.2368810[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2985354)[0m start_epoch 0
[36m(_train_fn pid=2985354)[0m max_epoch 8
[36m(_train_fn pid=2984756)[0m Epoch: 8, Steps: 67 | Train Loss: nan Vali Loss: 0.2193665 Best vali loss: 0.2193665[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2985354)[0m Validation loss decreased (inf --> 0.2162).  Saving model state dict ...
[36m(_train_fn pid=2985354)[0m 	iters: 600, epoch: 2 | loss: 0.2044568[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2985354)[0m 	speed: 0.0042s/iter; left time: 28.2284s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2983346)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2984894)[0m Updating learning rate to 0.004739401869352628[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2984894)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2984894)[0m Epoch: 1 cost time: 7.760369539260864[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2984894)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.4635576 Vali Loss: 0.2728725 Best vali loss: 0.2728725[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2985354)[0m Validation loss decreased (0.2162 --> 0.2150).  Saving model state dict ...
[36m(_train_fn pid=2984894)[0m Validation loss decreased (inf --> 0.2729).  Saving model state dict ...
[36m(_train_fn pid=2984894)[0m 	iters: 900, epoch: 2 | loss: 0.4402474[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2984894)[0m 	speed: 0.0069s/iter; left time: 44.7700s[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2985354)[0m Updating learning rate to 9.863858184259686e-05
[36m(_train_fn pid=2985354)[0m saving checkpoint...
[36m(_train_fn pid=2985354)[0m Epoch: 2 cost time: 3.313586950302124
[36m(_train_fn pid=2985354)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.4028402 Vali Loss: 0.2150376 Best vali loss: 0.2150376
[36m(_train_fn pid=2985354)[0m Updating learning rate to 7.989302977927941e-05
[36m(_train_fn pid=2985354)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2985354)[0m saving checkpoint...
[36m(_train_fn pid=2985354)[0m Epoch: 3 cost time: 3.637970209121704
[36m(_train_fn pid=2985354)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.2621347 Vali Loss: 0.2152798 Best vali loss: 0.2150376
[36m(_train_fn pid=2984894)[0m Validation loss decreased (0.2729 --> 0.2712).  Saving model state dict ...
[36m(_train_fn pid=2984894)[0m 	iters: 500, epoch: 3 | loss: 0.5218909[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2984894)[0m 	speed: 0.0057s/iter; left time: 33.4184s[32m [repeated 18x across cluster][0m

Trial status: 8 TERMINATED | 2 RUNNING
Current time: 2024-08-26 14:09:29. Total running time: 1min 0s
Logical resource usage: 2.0/32 CPUs, 0.5/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e72ad_00009 with best_valid_loss=0.21503758430480957 and params={'batch_size': 8, 'learning_rate': 0.00011556228694028946, 'd_model': 256, 'd_core': 64, 'e_layers': 1, 'dropout': 0.0026360657982210695, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e72ad_00008   RUNNING                 8       0.00492692          128              4        128            4   0.00115937   cosine         2           16.8659        0.401149       0.271161            0.271161 â”‚
â”‚ trial-e72ad_00009   RUNNING                 8       0.000115562         256              4         64            1   0.00263607   cosine         3           13.8314        0.262135       0.21528             0.215038 â”‚
â”‚ trial-e72ad_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            8.09624       0.363865       0.224524            0.218599 â”‚
â”‚ trial-e72ad_00001   TERMINATED              8       0.000451177         256              4        128            2   0.0030252    type1          5           35.8858        0.319517       0.247409            0.224095 â”‚
â”‚ trial-e72ad_00002   TERMINATED             32       0.000338982          32              1        512            4   0.0073023    cosine         7           17.6259        0.259268       0.222786            0.221581 â”‚
â”‚ trial-e72ad_00003   TERMINATED             16       0.000138336          64              3         32            3   0.00552133   cosine         7           31.546         0.387049       0.219265            0.217872 â”‚
â”‚ trial-e72ad_00004   TERMINATED              8       0.000181525         512              3         64            3   0.00966974   cosine         5           29.4445        0.243435       0.257313            0.225263 â”‚
â”‚ trial-e72ad_00005   TERMINATED             16       0.00356706           64              4        128            2   0.00565392   cosine         7           24.115         0.310854       0.253772            0.236881 â”‚
â”‚ trial-e72ad_00006   TERMINATED            128       0.00291482          512              2         32            2   0.00380852   cosine         4            3.54419     nan              0.26856             0.247949 â”‚
â”‚ trial-e72ad_00007   TERMINATED            128       0.000399204          32              4         32            2   0.00199936   cosine         8            4.82685     nan              0.219367            0.219367 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2985354)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2985354)[0m Updating learning rate to 5.778114347014473e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2985354)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2985354)[0m Epoch: 4 cost time: 3.37632155418396[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2985354)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3293200 Vali Loss: 0.2207958 Best vali loss: 0.2150376[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2984894)[0m Validation loss decreased (0.2712 --> 0.2702).  Saving model state dict ...

Trial trial-e72ad_00009 completed after 5 iterations at 2024-08-26 14:09:33. Total running time: 1min 4s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             3.99106 â”‚
â”‚ time_total_s                                21.99732 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21504 â”‚
â”‚ train_loss                                   0.37063 â”‚
â”‚ valid_loss                                   0.22123 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2985354)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2985354)[0m Early stopping
[36m(_train_fn pid=2984894)[0m 	iters: 200, epoch: 4 | loss: 0.3617152[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2984894)[0m 	speed: 0.0044s/iter; left time: 22.3813s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2985354)[0m Updating learning rate to 3.566925716101005e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2985354)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2985354)[0m Epoch: 5 cost time: 3.3217318058013916[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2985354)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.3706330 Vali Loss: 0.2212307 Best vali loss: 0.2150376[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2984894)[0m Validation loss decreased (0.2702 --> 0.2700).  Saving model state dict ...
[36m(_train_fn pid=2984894)[0m 	iters: 200, epoch: 5 | loss: 0.2475650[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2984894)[0m 	speed: 0.0048s/iter; left time: 19.4351s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2984894)[0m Updating learning rate to 0.002463460829673764
[36m(_train_fn pid=2984894)[0m saving checkpoint...
[36m(_train_fn pid=2984894)[0m Epoch: 4 cost time: 4.783553838729858
[36m(_train_fn pid=2984894)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.5424313 Vali Loss: 0.2700375 Best vali loss: 0.2700375
[36m(_train_fn pid=2984894)[0m Updating learning rate to 0.0015207351838772564
[36m(_train_fn pid=2984894)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2984894)[0m saving checkpoint...
[36m(_train_fn pid=2984894)[0m Epoch: 5 cost time: 5.147263765335083
[36m(_train_fn pid=2984894)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.6391898 Vali Loss: 0.2715733 Best vali loss: 0.2700375
[36m(_train_fn pid=2984894)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00008_8_alpha_d_ff=4,batch_size=8,d_core=128,d_model=128,dropout=0.0012,e_layers=4,learning_rate=0.0049,lradj=cosine_2024-08-26_14-09-07/checkpoint_000005)
2024-08-26 14:09:55,160	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107' in 0.0110s.
[36m(_train_fn pid=2984894)[0m 	iters: 100, epoch: 6 | loss: 0.3950013[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2984894)[0m 	speed: 0.0139s/iter; left time: 42.8226s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2984894)[0m Updating learning rate to 0.0007215309718240071
[36m(_train_fn pid=2984894)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2984894)[0m saving checkpoint...
[36m(_train_fn pid=2984894)[0m Epoch: 6 cost time: 4.766677618026733
[36m(_train_fn pid=2984894)[0m Epoch: 6, Steps: 1057 | Train Loss: 0.4025295 Vali Loss: 0.2702179 Best vali loss: 0.2700375
[36m(_train_fn pid=2984894)[0m 	iters: 100, epoch: 7 | loss: 0.2570160[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2984894)[0m 	speed: 0.0146s/iter; left time: 29.4577s[32m [repeated 10x across cluster][0m

Trial trial-e72ad_00008 completed after 7 iterations at 2024-08-26 14:09:55. Total running time: 1min 26s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-e72ad_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             5.46399 â”‚
â”‚ time_total_s                                46.48705 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.27004 â”‚
â”‚ train_loss                                   0.47782 â”‚
â”‚ valid_loss                                   0.27076 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:09:55. Total running time: 1min 26s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: e72ad_00009 with best_valid_loss=0.21503758430480957 and params={'batch_size': 8, 'learning_rate': 0.00011556228694028946, 'd_model': 256, 'd_core': 64, 'e_layers': 1, 'dropout': 0.0026360657982210695, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-e72ad_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            8.09624       0.363865       0.224524            0.218599 â”‚
â”‚ trial-e72ad_00001   TERMINATED              8       0.000451177         256              4        128            2   0.0030252    type1          5           35.8858        0.319517       0.247409            0.224095 â”‚
â”‚ trial-e72ad_00002   TERMINATED             32       0.000338982          32              1        512            4   0.0073023    cosine         7           17.6259        0.259268       0.222786            0.221581 â”‚
â”‚ trial-e72ad_00003   TERMINATED             16       0.000138336          64              3         32            3   0.00552133   cosine         7           31.546         0.387049       0.219265            0.217872 â”‚
â”‚ trial-e72ad_00004   TERMINATED              8       0.000181525         512              3         64            3   0.00966974   cosine         5           29.4445        0.243435       0.257313            0.225263 â”‚
â”‚ trial-e72ad_00005   TERMINATED             16       0.00356706           64              4        128            2   0.00565392   cosine         7           24.115         0.310854       0.253772            0.236881 â”‚
â”‚ trial-e72ad_00006   TERMINATED            128       0.00291482          512              2         32            2   0.00380852   cosine         4            3.54419     nan              0.26856             0.247949 â”‚
â”‚ trial-e72ad_00007   TERMINATED            128       0.000399204          32              4         32            2   0.00199936   cosine         8            4.82685     nan              0.219367            0.219367 â”‚
â”‚ trial-e72ad_00008   TERMINATED              8       0.00492692          128              4        128            4   0.00115937   cosine         7           46.487         0.477823       0.270763            0.270037 â”‚
â”‚ trial-e72ad_00009   TERMINATED              8       0.000115562         256              4         64            1   0.00263607   cosine         5           21.9973        0.370633       0.221231            0.215038 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 0.00011556228694028946, 'd_model': 256, 'd_core': 64, 'e_layers': 1, 'dropout': 0.0026360657982210695, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2984894)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed107/trial-e72ad_00008_8_alpha_d_ff=4,batch_size=8,d_core=128,d_model=128,dropout=0.0012,e_layers=4,learning_rate=0.0049,lradj=cosine_2024-08-26_14-09-07/checkpoint_000006)
[36m(_train_fn pid=2984894)[0m Updating learning rate to 0.00018751978999490034
[36m(_train_fn pid=2984894)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2984894)[0m saving checkpoint...
[36m(_train_fn pid=2984894)[0m Epoch: 7 cost time: 4.82248854637146
[36m(_train_fn pid=2984894)[0m Epoch: 7, Steps: 1057 | Train Loss: 0.4778231 Vali Loss: 0.2707629 Best vali loss: 0.2700375
[36m(_train_fn pid=2984894)[0m Early stopping
[36m(_train_fn pid=2984894)[0m 	iters: 1000, epoch: 7 | loss: 0.4510747[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2984894)[0m 	speed: 0.0045s/iter; left time: 5.0229s[32m [repeated 9x across cluster][0m


Time taken (4 parallel trials): 91 seconds


2024-08-26 14:09:59,003	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:09:59,364	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:09:59,369	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:09:59,376	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:10:02,228	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2989128)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed108/trial-1d15f_00002_2_alpha_d_ff=1,batch_size=128,d_core=512,d_model=32,dropout=0.0043,e_layers=2,learning_rate=0.0004,lradj=cosine_2024-08-26_14-09-59/checkpoint_000000)
2024-08-26 14:10:02,700	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:02,791	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed108   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator      â”‚
â”‚ Scheduler                        FIFOScheduler              â”‚
â”‚ Number of trials                 10                         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed108
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-09-58_574919_2986902/artifacts/2024-08-26_14-09-59/ETTh2_96_96_test_seed108/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:09:59. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-1d15f_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-1d15f_00001   PENDING               8       0.000919433         512              1         32            1   0.00153278   cosine  â”‚
â”‚ trial-1d15f_00002   PENDING             128       0.000448659          32              1        512            2   0.00426765   cosine  â”‚
â”‚ trial-1d15f_00003   PENDING              64       0.000643464         128              2        128            2   0.0119699    cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-1d15f_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00427 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00045 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-1d15f_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00153 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00092 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-1d15f_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.01197 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00064 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2989127)[0m configuration
[36m(_train_fn pid=2989127)[0m {'batch_size': 8, 'learning_rate': 0.0009194334818886371, 'd_model': 512, 'd_core': 32, 'e_layers': 1, 'dropout': 0.001532780456097398, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=2989127)[0m Use GPU: cuda:0

Trial trial-1d15f_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2989127)[0m train 8449
[36m(_train_fn pid=2989127)[0m val 2785
[36m(_train_fn pid=2989127)[0m start_epoch 0
[36m(_train_fn pid=2989127)[0m max_epoch 8
[36m(_train_fn pid=2989127)[0m 	iters: 100, epoch: 1 | loss: 1.0262333
[36m(_train_fn pid=2989127)[0m 	speed: 0.0114s/iter; left time: 95.5654s
[36m(_train_fn pid=2989128)[0m Updating learning rate to 0.0004315825912682643
[36m(_train_fn pid=2989128)[0m saving checkpoint...
[36m(_train_fn pid=2989128)[0m Validation loss decreased (inf --> 0.2488).  Saving model state dict ...
[36m(_train_fn pid=2989128)[0m Epoch: 1 cost time: 0.7160446643829346
[36m(_train_fn pid=2989128)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2487858 Best vali loss: 0.2487858
[36m(_train_fn pid=2989129)[0m Epoch: 1, Steps: 133 | Train Loss: 0.4010598 Vali Loss: 0.2187376 Best vali loss: 0.2187376
2024-08-26 14:10:03,405	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:03,597	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:03,637	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:03,967	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:04,570	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:04,626	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:05,169	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:05,457	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:05,586	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:05,716	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:06,746	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:07,275	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2989127)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed108/trial-1d15f_00001_1_alpha_d_ff=1,batch_size=8,d_core=32,d_model=512,dropout=0.0015,e_layers=1,learning_rate=0.0009,lradj=cosine_2024-08-26_14-09-59/checkpoint_000000)[32m [repeated 14x across cluster][0m
2024-08-26 14:10:08,028	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:08,222	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:08,580	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:08,934	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:09,274	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2989128)[0m Validation loss decreased (0.2488 --> 0.2312).  Saving model state dict ...
[36m(_train_fn pid=2989128)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2312428 Best vali loss: 0.2312428
[36m(_train_fn pid=2989128)[0m Validation loss decreased (0.2312 --> 0.2273).  Saving model state dict ...
[36m(_train_fn pid=2989128)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2272635 Best vali loss: 0.2272635
[36m(_train_fn pid=2989129)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2989129)[0m Epoch: 2, Steps: 133 | Train Loss: 0.4308465 Vali Loss: 0.2240091 Best vali loss: 0.2187376
[36m(_train_fn pid=2989128)[0m Validation loss decreased (0.2273 --> 0.2238).  Saving model state dict ...
[36m(_train_fn pid=2989128)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2238056 Best vali loss: 0.2238056
[36m(_train_fn pid=2989128)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2273142 Best vali loss: 0.2238056
[36m(_train_fn pid=2989128)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2242238 Best vali loss: 0.2238056

Trial trial-1d15f_00003 completed after 4 iterations at 2024-08-26 14:10:05. Total running time: 6s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.95713 â”‚
â”‚ time_total_s                                 4.61628 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21874 â”‚
â”‚ train_loss                                   0.33876 â”‚
â”‚ valid_loss                                   0.23798 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2989129)[0m Early stopping

Trial trial-1d15f_00002 completed after 7 iterations at 2024-08-26 14:10:05. Total running time: 6s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             0.54418 â”‚
â”‚ time_total_s                                 4.78828 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.22381 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.22406 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2989128)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2240578 Best vali loss: 0.2238056
[36m(_train_fn pid=2989126)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2989126)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2989126)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2989126)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2989126)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2989126)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2989126)[0m max_epoch 8[32m [repeated 3x across cluster][0m

Trial trial-1d15f_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00799 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00025 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2990526)[0m {'batch_size': 128, 'learning_rate': 0.000245325947538042, 'd_model': 32, 'd_core': 64, 'e_layers': 1, 'dropout': 0.007986157314845681, 'lradj': 'type1', 'd_ff': 128}

Trial trial-1d15f_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00218 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00079 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2989126)[0m 	iters: 100, epoch: 4 | loss: 0.4652805[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2989126)[0m 	speed: 0.0086s/iter; left time: 10.5380s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2989127)[0m Updating learning rate to 0.0008844396286555678[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2989127)[0m saving checkpoint...[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2989127)[0m Validation loss decreased (inf --> 0.2341).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2989127)[0m Epoch: 1 cost time: 5.192423582077026[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2989127)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.4241041 Vali Loss: 0.2341093 Best vali loss: 0.2341093[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2989126)[0m Validation loss decreased (0.2188 --> 0.2186).  Saving model state dict ...
[36m(_train_fn pid=2990526)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2671540 Best vali loss: 0.2671540
[36m(_train_fn pid=2990526)[0m Validation loss decreased (0.2672 --> 0.2482).  Saving model state dict ...
[36m(_train_fn pid=2990526)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2482055 Best vali loss: 0.2482055
[36m(_train_fn pid=2989126)[0m EarlyStopping counter: 2 out of 3[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2990526)[0m Validation loss decreased (0.2482 --> 0.2422).  Saving model state dict ...
[36m(_train_fn pid=2990526)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2422064 Best vali loss: 0.2422064
2024-08-26 14:10:09,549	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:09,673	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:10,029	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:10,411	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:10,775	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:11,030	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:11,892	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:13,559	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2991345)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed108/trial-1d15f_00006_6_alpha_d_ff=3,batch_size=64,d_core=512,d_model=256,dropout=0.0018,e_layers=4,learning_rate=0.0010,lradj=type1_2024-08-26_14-10-09/checkpoint_000000)[32m [repeated 13x across cluster][0m
2024-08-26 14:10:16,153	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:16,178	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:17,182	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2990526)[0m Validation loss decreased (0.2422 --> 0.2395).  Saving model state dict ...
[36m(_train_fn pid=2990526)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2394966 Best vali loss: 0.2394966

Trial trial-1d15f_00000 completed after 5 iterations at 2024-08-26 14:10:09. Total running time: 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                              1.5181 â”‚
â”‚ time_total_s                                 8.53219 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21859 â”‚
â”‚ train_loss                                   0.34844 â”‚
â”‚ valid_loss                                   0.22317 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2990526)[0m Validation loss decreased (0.2395 --> 0.2385).  Saving model state dict ...
[36m(_train_fn pid=2990526)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2385220 Best vali loss: 0.2385220
[36m(_train_fn pid=2990526)[0m Validation loss decreased (0.2385 --> 0.2380).  Saving model state dict ...
[36m(_train_fn pid=2990526)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2380063 Best vali loss: 0.2380063
[36m(_train_fn pid=2990526)[0m Validation loss decreased (0.2380 --> 0.2378).  Saving model state dict ...
[36m(_train_fn pid=2990526)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2377671 Best vali loss: 0.2377671
[36m(_train_fn pid=2989126)[0m Early stopping[32m [repeated 2x across cluster][0m

Trial trial-1d15f_00004 completed after 8 iterations at 2024-08-26 14:10:10. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.36155 â”‚
â”‚ time_total_s                                 3.58433 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.23766 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.23766 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2990526)[0m Validation loss decreased (0.2378 --> 0.2377).  Saving model state dict ...
[36m(_train_fn pid=2990526)[0m Epoch: 8, Steps: 67 | Train Loss: nan Vali Loss: 0.2376646 Best vali loss: 0.2376646
[36m(_train_fn pid=2990564)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2990564)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m

Trial trial-1d15f_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00179 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00095 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2990564)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2990564)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2990564)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2990564)[0m max_epoch 8[32m [repeated 2x across cluster][0m

Trial trial-1d15f_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00149 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00025 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2991691)[0m {'batch_size': 64, 'learning_rate': 0.0002524635711548319, 'd_model': 512, 'd_core': 64, 'e_layers': 4, 'dropout': 0.0014860295676103485, 'lradj': 'cosine', 'd_ff': 1536}
[36m(_train_fn pid=2991345)[0m {'batch_size': 64, 'learning_rate': 0.0009505054535325733, 'd_model': 256, 'd_core': 512, 'e_layers': 4, 'dropout': 0.001785247473809928, 'lradj': 'type1', 'd_ff': 768}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2990564)[0m 	iters: 200, epoch: 2 | loss: 0.2974465[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2990564)[0m 	speed: 0.0037s/iter; left time: 13.0220s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2989127)[0m Updating learning rate to 0.0007847855658910255[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2989127)[0m saving checkpoint...[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2990564)[0m Validation loss decreased (inf --> 0.2254).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2989127)[0m Epoch: 2 cost time: 3.79874324798584[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2989127)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.3455030 Vali Loss: 0.2473343 Best vali loss: 0.2341093[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2989127)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2991691)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2991691)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2991691)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2991691)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2990564)[0m Validation loss decreased (0.2254 --> 0.2196).  Saving model state dict ...
[36m(_train_fn pid=2991691)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2991691)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2991345)[0m 	iters: 100, epoch: 3 | loss: 0.5036935[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2991345)[0m 	speed: 0.0252s/iter; left time: 17.5864s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2990564)[0m Updating learning rate to 0.0003973583392530707[32m [repeated 4x across cluster][0m
2024-08-26 14:10:18,743	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2991345)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed108/trial-1d15f_00006_6_alpha_d_ff=3,batch_size=64,d_core=512,d_model=256,dropout=0.0018,e_layers=4,learning_rate=0.0010,lradj=type1_2024-08-26_14-10-09/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 14:10:19,076	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:21,353	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:21,994	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:23,816	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2991345)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed108/trial-1d15f_00006_6_alpha_d_ff=3,batch_size=64,d_core=512,d_model=256,dropout=0.0018,e_layers=4,learning_rate=0.0010,lradj=type1_2024-08-26_14-10-09/checkpoint_000004)[32m [repeated 4x across cluster][0m
2024-08-26 14:10:24,763	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:25,294	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:25,569	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:25,610	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:27,838	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:28,343	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:28,482	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:28,621	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:29,126	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2992916)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed108/trial-1d15f_00009_9_alpha_d_ff=3,batch_size=64,d_core=32,d_model=64,dropout=0.0027,e_layers=1,learning_rate=0.0009,lradj=type1_2024-08-26_14-10-25/checkpoint_000001)[32m [repeated 9x across cluster][0m
2024-08-26 14:10:29,389	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2990564)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2991691)[0m Validation loss decreased (inf --> 0.2174).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2990564)[0m Epoch: 2 cost time: 4.579909324645996[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2990564)[0m Epoch: 2, Steps: 529 | Train Loss: 0.6065072 Vali Loss: 0.2195585 Best vali loss: 0.2195585[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2991345)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2991691)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2991345)[0m Validation loss decreased (0.2267 --> 0.2193).  Saving model state dict ...
[36m(_train_fn pid=2990564)[0m 	iters: 400, epoch: 3 | loss: 0.3568721[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2990564)[0m 	speed: 0.0144s/iter; left time: 39.9385s[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2991691)[0m Updating learning rate to 0.00017453859855575554[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2991691)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2991691)[0m Epoch: 3 cost time: 2.552549362182617[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2991345)[0m Epoch: 5, Steps: 133 | Train Loss: 0.2580744 Vali Loss: 0.2269725 Best vali loss: 0.2192809[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2991345)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m

Trial trial-1d15f_00007 completed after 4 iterations at 2024-08-26 14:10:24. Total running time: 25s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.76526 â”‚
â”‚ time_total_s                                12.37004 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21739 â”‚
â”‚ train_loss                                    0.3717 â”‚
â”‚ valid_loss                                   0.22227 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2991691)[0m Early stopping

Trial trial-1d15f_00006 completed after 6 iterations at 2024-08-26 14:10:25. Total running time: 26s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.75134 â”‚
â”‚ time_total_s                                14.42664 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21928 â”‚
â”‚ train_loss                                    0.2104 â”‚
â”‚ valid_loss                                   0.22866 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2990564)[0m Validation loss decreased (0.2196 --> 0.2176).  Saving model state dict ...

Trial trial-1d15f_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00108 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00368 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2992747)[0m configuration
[36m(_train_fn pid=2992747)[0m {'batch_size': 64, 'learning_rate': 0.003681176969727583, 'd_model': 64, 'd_core': 512, 'e_layers': 3, 'dropout': 0.001083374307691118, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2992747)[0m Use GPU: cuda:0
[36m(_train_fn pid=2992747)[0m train 8449
[36m(_train_fn pid=2992747)[0m val 2785
[36m(_train_fn pid=2992747)[0m start_epoch 0
[36m(_train_fn pid=2992747)[0m max_epoch 8

Trial trial-1d15f_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00267 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00088 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2992916)[0m {'batch_size': 64, 'learning_rate': 0.0008803706236158859, 'd_model': 64, 'd_core': 32, 'e_layers': 1, 'dropout': 0.002668319917736213, 'lradj': 'type1', 'd_ff': 192}
[36m(_train_fn pid=2992747)[0m Validation loss decreased (inf --> 0.2287).  Saving model state dict ...
[36m(_train_fn pid=2989127)[0m 	iters: 900, epoch: 4 | loss: 0.3146228[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2989127)[0m 	speed: 0.0035s/iter; left time: 15.5122s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2992747)[0m Updating learning rate to 0.0035410705138054087[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2992747)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2992747)[0m Epoch: 1 cost time: 0.9242887496948242[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2992747)[0m Epoch: 2, Steps: 133 | Train Loss: 0.4990625 Vali Loss: 0.2338639 Best vali loss: 0.2287098[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2992747)[0m EarlyStopping counter: 1 out of 3[32m [repeated 5x across cluster][0m

Trial status: 6 TERMINATED | 4 RUNNING
Current time: 2024-08-26 14:10:29. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 1d15f_00009 with best_valid_loss=0.21555732149945128 and params={'batch_size': 64, 'learning_rate': 0.0008803706236158859, 'd_model': 64, 'd_core': 32, 'e_layers': 1, 'dropout': 0.002668319917736213, 'lradj': 'type1', 'd_ff': 192}
2024-08-26 14:10:29,709	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:29,851	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:30,162	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:30,321	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:30,882	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:30,904	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:30,961	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:31,749	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-1d15f_00001   RUNNING                 8       0.000919433         512              1         32            1   0.00153278   cosine         3           24.3656        0.333088       0.242596            0.234109 â”‚
â”‚ trial-1d15f_00005   RUNNING                16       0.000794717          32              2         32            2   0.00217838   type1          4           21.084         0.273614       0.21999             0.217598 â”‚
â”‚ trial-1d15f_00008   RUNNING                64       0.00368118           64              4        512            3   0.00108337   cosine         3            3.09451       0.355913       0.226542            0.226542 â”‚
â”‚ trial-1d15f_00009   RUNNING                64       0.000880371          64              3         32            1   0.00266832   type1          2            1.99151       0.363737       0.215557            0.215557 â”‚
â”‚ trial-1d15f_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5            8.53219       0.348437       0.223167            0.218593 â”‚
â”‚ trial-1d15f_00002   TERMINATED            128       0.000448659          32              1        512            2   0.00426765   cosine         7            4.78828     nan              0.224058            0.223806 â”‚
â”‚ trial-1d15f_00003   TERMINATED             64       0.000643464         128              2        128            2   0.0119699    cosine         4            4.61628       0.33876        0.237981            0.218738 â”‚
â”‚ trial-1d15f_00004   TERMINATED            128       0.000245326          32              4         64            1   0.00798616   type1          8            3.58433     nan              0.237665            0.237665 â”‚
â”‚ trial-1d15f_00006   TERMINATED             64       0.000950505         256              3        512            4   0.00178525   type1          6           14.4266        0.2104         0.228661            0.219281 â”‚
â”‚ trial-1d15f_00007   TERMINATED             64       0.000252464         512              3         64            4   0.00148603   cosine         4           12.37          0.371703       0.222273            0.217387 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-1d15f_00001 completed after 4 iterations at 2024-08-26 14:10:29. Total running time: 30s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             4.55433 â”‚
â”‚ time_total_s                                28.91995 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.23411 â”‚
â”‚ train_loss                                   0.25209 â”‚
â”‚ valid_loss                                   0.25456 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2989127)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2992747)[0m Validation loss decreased (0.2287 --> 0.2265).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-1d15f_00009 completed after 5 iterations at 2024-08-26 14:10:30. Total running time: 31s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             0.55878 â”‚
â”‚ time_total_s                                  3.7405 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21556 â”‚
â”‚ train_loss                                   0.43667 â”‚
â”‚ valid_loss                                   0.21715 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2992916)[0m configuration
[36m(_train_fn pid=2992916)[0m Use GPU: cuda:0
[36m(_train_fn pid=2992916)[0m train 8449
[36m(_train_fn pid=2992916)[0m val 2785

Trial trial-1d15f_00008 completed after 6 iterations at 2024-08-26 14:10:31. Total running time: 32s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             0.78584 â”‚
â”‚ time_total_s                                 5.44708 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22654 â”‚
â”‚ train_loss                                   0.34969 â”‚
â”‚ valid_loss                                   0.23238 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2992916)[0m start_epoch 0
2024-08-26 14:10:32,773	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:32,782	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed108' in 0.0088s.
[36m(_train_fn pid=2992916)[0m max_epoch 8

Trial trial-1d15f_00005 completed after 6 iterations at 2024-08-26 14:10:32. Total running time: 33s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-1d15f_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.86565 â”‚
â”‚ time_total_s                                25.50879 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                               0.2176 â”‚
â”‚ train_loss                                   0.38735 â”‚
â”‚ valid_loss                                   0.22133 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:10:32. Total running time: 33s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 1d15f_00009 with best_valid_loss=0.21555732149945128 and params={'batch_size': 64, 'learning_rate': 0.0008803706236158859, 'd_model': 64, 'd_core': 32, 'e_layers': 1, 'dropout': 0.002668319917736213, 'lradj': 'type1', 'd_ff': 192}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-1d15f_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5            8.53219       0.348437       0.223167            0.218593 â”‚
â”‚ trial-1d15f_00001   TERMINATED              8       0.000919433         512              1         32            1   0.00153278   cosine         4           28.9199        0.252086       0.254559            0.234109 â”‚
â”‚ trial-1d15f_00002   TERMINATED            128       0.000448659          32              1        512            2   0.00426765   cosine         7            4.78828     nan              0.224058            0.223806 â”‚
â”‚ trial-1d15f_00003   TERMINATED             64       0.000643464         128              2        128            2   0.0119699    cosine         4            4.61628       0.33876        0.237981            0.218738 â”‚
â”‚ trial-1d15f_00004   TERMINATED            128       0.000245326          32              4         64            1   0.00798616   type1          8            3.58433     nan              0.237665            0.237665 â”‚
â”‚ trial-1d15f_00005   TERMINATED             16       0.000794717          32              2         32            2   0.00217838   type1          6           25.5088        0.387355       0.221325            0.217598 â”‚
â”‚ trial-1d15f_00006   TERMINATED             64       0.000950505         256              3        512            4   0.00178525   type1          6           14.4266        0.2104         0.228661            0.219281 â”‚
â”‚ trial-1d15f_00007   TERMINATED             64       0.000252464         512              3         64            4   0.00148603   cosine         4           12.37          0.371703       0.222273            0.217387 â”‚
â”‚ trial-1d15f_00008   TERMINATED             64       0.00368118           64              4        512            3   0.00108337   cosine         6            5.44708       0.34969        0.232381            0.226542 â”‚
â”‚ trial-1d15f_00009   TERMINATED             64       0.000880371          64              3         32            1   0.00266832   type1          5            3.7405        0.436666       0.217147            0.215557 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 64, 'learning_rate': 0.0008803706236158859, 'd_model': 64, 'd_core': 32, 'e_layers': 1, 'dropout': 0.002668319917736213, 'lradj': 'type1', 'd_ff': 192}
[36m(_train_fn pid=2990564)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed108/trial-1d15f_00005_5_alpha_d_ff=2,batch_size=16,d_core=32,d_model=32,dropout=0.0022,e_layers=2,learning_rate=0.0008,lradj=type1_2024-08-26_14-10-05/checkpoint_000005)[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2992916)[0m Validation loss decreased (inf --> 0.2193).  Saving model state dict ...
[36m(_train_fn pid=2990564)[0m 	iters: 500, epoch: 6 | loss: 0.3231266[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2990564)[0m 	speed: 0.0027s/iter; left time: 2.9896s[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2990564)[0m Updating learning rate to 2.4834896203316917e-05[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2990564)[0m saving checkpoint...[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2990564)[0m Epoch: 6 cost time: 1.595931053161621[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2990564)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3873548 Vali Loss: 0.2213252 Best vali loss: 0.2175979[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2990564)[0m EarlyStopping counter: 3 out of 3[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2990564)[0m Early stopping[32m [repeated 3x across cluster][0m


Time taken (4 parallel trials): 38 seconds


2024-08-26 14:10:36,734	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:10:37,093	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:10:37,098	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:10:37,105	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:10:41,208	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed109   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator      â”‚
â”‚ Scheduler                        FIFOScheduler              â”‚
â”‚ Number of trials                 10                         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-10-36_302097_2994096/artifacts/2024-08-26_14-10-37/ETTh2_96_96_test_seed109/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:10:37. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-3392c_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-3392c_00001   PENDING              32       0.00240873          128              2        256            1   0.00405384   cosine  â”‚
â”‚ trial-3392c_00002   PENDING               8       0.000100428          32              2        128            3   0.0119808    type1   â”‚
â”‚ trial-3392c_00003   PENDING              32       0.000549219         512              2        256            4   0.00626912   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-3392c_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.01198 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                       0.0001 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-3392c_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00627 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00055 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-3392c_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00405 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00241 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-3392c_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2996325)[0m configuration
[36m(_train_fn pid=2996325)[0m {'batch_size': 32, 'learning_rate': 0.002408727636281429, 'd_model': 128, 'd_core': 256, 'e_layers': 1, 'dropout': 0.004053837564575444, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=2996325)[0m Use GPU: cuda:0
[36m(_train_fn pid=2996326)[0m {'batch_size': 8, 'learning_rate': 0.00010042822313033324, 'd_model': 32, 'd_core': 128, 'e_layers': 3, 'dropout': 0.011980822208128055, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=2996325)[0m train 8449
[36m(_train_fn pid=2996325)[0m val 2785
[36m(_train_fn pid=2996325)[0m start_epoch 0
[36m(_train_fn pid=2996325)[0m max_epoch 8
[36m(_train_fn pid=2996325)[0m 	iters: 100, epoch: 1 | loss: 0.8891114
[36m(_train_fn pid=2996325)[0m 	speed: 0.0108s/iter; left time: 21.7842s
[36m(_train_fn pid=2996325)[0m Updating learning rate to 0.002317050899418066
[36m(_train_fn pid=2996325)[0m saving checkpoint...
[36m(_train_fn pid=2996325)[0m Validation loss decreased (inf --> 0.2302).  Saving model state dict ...
[36m(_train_fn pid=2996325)[0m Epoch: 1 cost time: 1.6703004837036133
[36m(_train_fn pid=2996325)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109/trial-3392c_00001_1_alpha_d_ff=2,batch_size=32,d_core=256,d_model=128,dropout=0.0041,e_layers=1,learning_rate=0.0024,lradj=cosine_2024-08-26_14-10-37/checkpoint_000000)
2024-08-26 14:10:41,944	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:42,956	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:42,960	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:44,307	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:44,785	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2996327)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109/trial-3392c_00003_3_alpha_d_ff=2,batch_size=32,d_core=256,d_model=512,dropout=0.0063,e_layers=4,learning_rate=0.0005,lradj=cosine_2024-08-26_14-10-37/checkpoint_000001)[32m [repeated 6x across cluster][0m
2024-08-26 14:10:46,441	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:46,510	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:46,654	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:48,516	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:49,090	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:49,667	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:51,195	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:51,523	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2997588)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109/trial-3392c_00004_4_alpha_d_ff=3,batch_size=128,d_core=32,d_model=256,dropout=0.0046,e_layers=3,learning_rate=0.0041,lradj=cosine_2024-08-26_14-10-48/checkpoint_000000)[32m [repeated 7x across cluster][0m
2024-08-26 14:10:52,246	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:52,495	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:52,585	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:52,933	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:52,993	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2996325)[0m Epoch: 1, Steps: 265 | Train Loss: 0.8015679 Vali Loss: 0.2301766 Best vali loss: 0.2301766
[36m(_train_fn pid=2996325)[0m Validation loss decreased (0.2302 --> 0.2239).  Saving model state dict ...
[36m(_train_fn pid=2996326)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2996324)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2996326)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2996326)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2996326)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2996326)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2996326)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2996325)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2996324)[0m 	iters: 100, epoch: 3 | loss: 0.5316429[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2996324)[0m 	speed: 0.0171s/iter; left time: 25.4217s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2996327)[0m Updating learning rate to 0.00046878815035163255[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2996327)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2996327)[0m Validation loss decreased (inf --> 0.2262).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2996325)[0m Epoch: 3 cost time: 1.4435296058654785[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2996325)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3152778 Vali Loss: 0.2281668 Best vali loss: 0.2239397[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2996324)[0m Validation loss decreased (0.2185 --> 0.2180).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-3392c_00001 completed after 5 iterations at 2024-08-26 14:10:48. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                              2.0027 â”‚
â”‚ time_total_s                                  9.8275 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22394 â”‚
â”‚ train_loss                                   0.37067 â”‚
â”‚ valid_loss                                   0.24645 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2996325)[0m Early stopping

Trial trial-3392c_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00456 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00415 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2997588)[0m configuration
[36m(_train_fn pid=2997588)[0m {'batch_size': 128, 'learning_rate': 0.004146292258317688, 'd_model': 256, 'd_core': 32, 'e_layers': 3, 'dropout': 0.004557957040848009, 'lradj': 'cosine', 'd_ff': 768}
[36m(_train_fn pid=2997588)[0m Use GPU: cuda:0
[36m(_train_fn pid=2996327)[0m EarlyStopping counter: 2 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2996326)[0m 	iters: 1000, epoch: 1 | loss: 0.4075944[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2996326)[0m 	speed: 0.0085s/iter; left time: 63.5359s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2997588)[0m train 8449
[36m(_train_fn pid=2997588)[0m val 2785
[36m(_train_fn pid=2997588)[0m start_epoch 0
[36m(_train_fn pid=2997588)[0m max_epoch 8
[36m(_train_fn pid=2996324)[0m Updating learning rate to 9.259748514523653e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2996324)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2996324)[0m Epoch: 5 cost time: 1.7004203796386719[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2996324)[0m Epoch: 5, Steps: 265 | Train Loss: 0.2744980 Vali Loss: 0.2210246 Best vali loss: 0.2180327[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2997588)[0m Validation loss decreased (inf --> 0.2620).  Saving model state dict ...
[36m(_train_fn pid=2997588)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2620183 Best vali loss: 0.2620183
[36m(_train_fn pid=2997588)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2640267 Best vali loss: 0.2620183

Trial trial-3392c_00003 completed after 4 iterations at 2024-08-26 14:10:52. Total running time: 15s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.82441 â”‚
â”‚ time_total_s                                13.80482 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22616 â”‚
â”‚ train_loss                                   0.35149 â”‚
â”‚ valid_loss                                   0.23701 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-3392c_00000 completed after 6 iterations at 2024-08-26 14:10:52. Total running time: 15s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.73552 â”‚
â”‚ time_total_s                                14.24097 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21803 â”‚
â”‚ train_loss                                   0.25919 â”‚
â”‚ valid_loss                                   0.22763 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
2024-08-26 14:10:53,718	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:54,433	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:55,147	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:55,236	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:55,777	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:56,307	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:56,831	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2998158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109/trial-3392c_00005_5_alpha_d_ff=1,batch_size=128,d_core=256,d_model=128,dropout=0.0088,e_layers=2,learning_rate=0.0001,lradj=type1_2024-08-26_14-10-52/checkpoint_000003)[32m [repeated 12x across cluster][0m
2024-08-26 14:10:57,346	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:57,924	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:58,486	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:58,959	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:10:59,004	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2997588)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2562731 Best vali loss: 0.2562731
[36m(_train_fn pid=2997588)[0m Validation loss decreased (0.2620 --> 0.2563).  Saving model state dict ...
[36m(_train_fn pid=2996324)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2997588)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2613502 Best vali loss: 0.2562731

Trial trial-3392c_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00882 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00008 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2998158)[0m {'batch_size': 128, 'learning_rate': 8.27207809147702e-05, 'd_model': 128, 'd_core': 256, 'e_layers': 2, 'dropout': 0.008816433385655693, 'lradj': 'type1', 'd_ff': 128}

Trial trial-3392c_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                             0.0069 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00018 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2997588)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2710700 Best vali loss: 0.2562731

Trial trial-3392c_00004 completed after 6 iterations at 2024-08-26 14:10:55. Total running time: 18s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             0.71093 â”‚
â”‚ time_total_s                                 5.08434 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.25627 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                    0.2672 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2997588)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2671999 Best vali loss: 0.2562731
[36m(_train_fn pid=2998332)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2998332)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2997588)[0m EarlyStopping counter: 3 out of 3[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2996326)[0m 	iters: 600, epoch: 2 | loss: 0.2610072[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2996326)[0m 	speed: 0.0042s/iter; left time: 28.2752s[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2998332)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2998332)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2998158)[0m Validation loss decreased (0.2526 --> 0.2328).  Saving model state dict ...
[36m(_train_fn pid=2998332)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2998332)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2998158)[0m Validation loss decreased (0.2328 --> 0.2279).  Saving model state dict ...

Trial trial-3392c_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00164 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00012 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2998158)[0m Updating learning rate to 2.068019522869255e-05[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2998158)[0m saving checkpoint...[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2998158)[0m Epoch: 3 cost time: 0.40741991996765137[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2996324)[0m Epoch: 6, Steps: 265 | Train Loss: 0.2591935 Vali Loss: 0.2276292 Best vali loss: 0.2180327[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2998158)[0m Validation loss decreased (inf --> 0.2526).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2998158)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2278848 Best vali loss: 0.2278848[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2998749)[0m {'batch_size': 16, 'learning_rate': 0.00011855433557204775, 'd_model': 64, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0016394728399937975, 'lradj': 'cosine', 'd_ff': 64}
[36m(_train_fn pid=2998158)[0m Validation loss decreased (0.2279 --> 0.2262).  Saving model state dict ...
[36m(_train_fn pid=2998158)[0m Validation loss decreased (0.2262 --> 0.2255).  Saving model state dict ...
[36m(_train_fn pid=2998158)[0m Validation loss decreased (0.2255 --> 0.2252).  Saving model state dict ...
[36m(_train_fn pid=2998158)[0m Validation loss decreased (0.2252 --> 0.2251).  Saving model state dict ...
[36m(_train_fn pid=2997588)[0m Early stopping

Trial trial-3392c_00005 completed after 8 iterations at 2024-08-26 14:10:59. Total running time: 21s
2024-08-26 14:10:59,306	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:00,379	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:02,147	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2998332)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109/trial-3392c_00006_6_alpha_d_ff=2,batch_size=16,d_core=512,d_model=32,dropout=0.0069,e_layers=2,learning_rate=0.0002,lradj=type1_2024-08-26_14-10-52/checkpoint_000001)[32m [repeated 8x across cluster][0m
2024-08-26 14:11:03,032	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:05,378	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:05,747	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:06,697	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:06,751	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.51572 â”‚
â”‚ time_total_s                                 4.93846 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22498 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.22498 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2998158)[0m Validation loss decreased (0.2251 --> 0.2250).  Saving model state dict ...
[36m(_train_fn pid=2998332)[0m {'batch_size': 16, 'learning_rate': 0.00018121454545587143, 'd_model': 32, 'd_core': 512, 'e_layers': 2, 'dropout': 0.006901727233588521, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=2998749)[0m configuration
[36m(_train_fn pid=2998749)[0m Use GPU: cuda:0
[36m(_train_fn pid=2998332)[0m 	iters: 200, epoch: 2 | loss: 0.3785357[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2998332)[0m 	speed: 0.0049s/iter; left time: 17.2026s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2998749)[0m train 8449
[36m(_train_fn pid=2998749)[0m val 2785

Trial trial-3392c_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00085 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00007 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2999492)[0m configuration
[36m(_train_fn pid=2999492)[0m Use GPU: cuda:0
[36m(_train_fn pid=2999492)[0m train 8449
[36m(_train_fn pid=2999492)[0m val 2785
[36m(_train_fn pid=2996326)[0m Validation loss decreased (0.2352 --> 0.2241).  Saving model state dict ...
[36m(_train_fn pid=2998749)[0m start_epoch 0
[36m(_train_fn pid=2998749)[0m max_epoch 8
[36m(_train_fn pid=2999492)[0m start_epoch 0
[36m(_train_fn pid=2999492)[0m max_epoch 8
[36m(_train_fn pid=2998749)[0m Updating learning rate to 0.00011404212984876872[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2998749)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2998749)[0m Epoch: 1 cost time: 2.6836438179016113[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2998749)[0m Epoch: 1, Steps: 529 | Train Loss: 0.8744800 Vali Loss: 0.2311863 Best vali loss: 0.2311863[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2998749)[0m Validation loss decreased (inf --> 0.2312).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2998158)[0m Epoch: 8, Steps: 67 | Train Loss: nan Vali Loss: 0.2249828 Best vali loss: 0.2249828[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2999492)[0m {'batch_size': 8, 'learning_rate': 7.215226706834495e-05, 'd_model': 128, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0008463264008062425, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=2998332)[0m Validation loss decreased (0.2331 --> 0.2229).  Saving model state dict ...
[36m(_train_fn pid=2999492)[0m 	iters: 900, epoch: 1 | loss: 0.1821757[32m [repeated 36x across cluster][0m
[36m(_train_fn pid=2999492)[0m 	speed: 0.0046s/iter; left time: 34.5757s[32m [repeated 36x across cluster][0m
[36m(_train_fn pid=2996326)[0m Updating learning rate to 2.510705578258331e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2996326)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2996326)[0m Epoch: 3 cost time: 6.264081239700317[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2996326)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4149772 Vali Loss: 0.2210033 Best vali loss: 0.2210033[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2999492)[0m Validation loss decreased (inf --> 0.2226).  Saving model state dict ...
[36m(_train_fn pid=2996326)[0m Validation loss decreased (0.2241 --> 0.2210).  Saving model state dict ...[32m [repeated 4x across cluster][0m

Trial status: 5 TERMINATED | 4 RUNNING
Current time: 2024-08-26 14:11:07. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 3392c_00000 with best_valid_loss=0.21803274913364937 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
2024-08-26 14:11:08,356	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2998749)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109/trial-3392c_00007_7_alpha_d_ff=1,batch_size=16,d_core=128,d_model=64,dropout=0.0016,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_14-10-55/checkpoint_000003)[32m [repeated 6x across cluster][0m
2024-08-26 14:11:08,482	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:11,017	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:11,762	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:12,182	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:13,523	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2998749)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109/trial-3392c_00007_7_alpha_d_ff=1,batch_size=16,d_core=128,d_model=64,dropout=0.0016,e_layers=1,learning_rate=0.0001,lradj=cosine_2024-08-26_14-10-55/checkpoint_000005)[32m [repeated 5x across cluster][0m
2024-08-26 14:11:14,524	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:15,052	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:15,725	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-3392c_00002   RUNNING                 8       0.000100428          32              2        128            3   0.0119808     type1          3           28.0754        0.414977       0.221003            0.221003 â”‚
â”‚ trial-3392c_00006   RUNNING                16       0.000181215          32              2        512            2   0.00690173    type1          3           10.8835        0.27605        0.220874            0.220874 â”‚
â”‚ trial-3392c_00007   RUNNING                16       0.000118554          64              1        128            1   0.00163947    cosine         3            9.05411       0.791429       0.218776            0.218776 â”‚
â”‚ trial-3392c_00008   RUNNING                 8       7.21523e-05         128              4        512            4   0.000846326   cosine         1            6.16614       0.326991       0.222649            0.222649 â”‚
â”‚ trial-3392c_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         6           14.241         0.259194       0.227629            0.218033 â”‚
â”‚ trial-3392c_00001   TERMINATED             32       0.00240873          128              2        256            1   0.00405384    cosine         5            9.8275        0.370667       0.246449            0.22394  â”‚
â”‚ trial-3392c_00003   TERMINATED             32       0.000549219         512              2        256            4   0.00626912    cosine         4           13.8048        0.351489       0.23701             0.226163 â”‚
â”‚ trial-3392c_00004   TERMINATED            128       0.00414629          256              3         32            3   0.00455796    cosine         6            5.08434     nan              0.2672              0.256273 â”‚
â”‚ trial-3392c_00005   TERMINATED            128       8.27208e-05         128              1        256            2   0.00881643    type1          8            4.93846     nan              0.224983            0.224983 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2998332)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2996326)[0m 	iters: 600, epoch: 4 | loss: 0.3973921[32m [repeated 33x across cluster][0m
[36m(_train_fn pid=2996326)[0m 	speed: 0.0065s/iter; left time: 30.3225s[32m [repeated 33x across cluster][0m
[36m(_train_fn pid=2998332)[0m Updating learning rate to 1.1325909090991965e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2998332)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2998332)[0m Epoch: 5 cost time: 2.6893560886383057[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2998332)[0m Epoch: 5, Steps: 529 | Train Loss: 0.4909165 Vali Loss: 0.2212757 Best vali loss: 0.2208742[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2999492)[0m Validation loss decreased (0.2226 --> 0.2198).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2998332)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m

Trial trial-3392c_00006 completed after 6 iterations at 2024-08-26 14:11:15. Total running time: 37s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             3.28755 â”‚
â”‚ time_total_s                                20.55018 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22087 â”‚
â”‚ train_loss                                   0.36728 â”‚
â”‚ valid_loss                                   0.22136 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2998332)[0m Early stopping
[36m(_train_fn pid=2996326)[0m 	iters: 200, epoch: 5 | loss: 0.1988463[32m [repeated 32x across cluster][0m
[36m(_train_fn pid=2996326)[0m 	speed: 0.0046s/iter; left time: 18.6064s[32m [repeated 32x across cluster][0m

Trial trial-3392c_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00209 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00014 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3000757)[0m configuration
[36m(_train_fn pid=3000757)[0m {'batch_size': 8, 'learning_rate': 0.00013597630874475632, 'd_model': 512, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0020880482546892865, 'lradj': 'type1', 'd_ff': 1536}
[36m(_train_fn pid=3000757)[0m Use GPU: cuda:0
[36m(_train_fn pid=3000757)[0m train 8449
[36m(_train_fn pid=3000757)[0m val 2785
[36m(_train_fn pid=3000757)[0m start_epoch 0
[36m(_train_fn pid=3000757)[0m max_epoch 8
[36m(_train_fn pid=2998749)[0m Updating learning rate to 4.512205723279032e-06[32m [repeated 5x across cluster][0m
2024-08-26 14:11:17,919	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:17,950	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:21,596	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2996326)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109/trial-3392c_00002_2_alpha_d_ff=2,batch_size=8,d_core=128,d_model=32,dropout=0.0120,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_14-10-37/checkpoint_000004)[32m [repeated 6x across cluster][0m
2024-08-26 14:11:23,313	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:23,967	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:28,751	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:28,763	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2996326)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109/trial-3392c_00002_2_alpha_d_ff=2,batch_size=8,d_core=128,d_model=32,dropout=0.0120,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_14-10-37/checkpoint_000005)[32m [repeated 3x across cluster][0m
2024-08-26 14:11:30,606	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2996326)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109/trial-3392c_00002_2_alpha_d_ff=2,batch_size=8,d_core=128,d_model=32,dropout=0.0120,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_14-10-37/checkpoint_000006)[32m [repeated 3x across cluster][0m
2024-08-26 14:11:36,991	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2998749)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2998749)[0m Epoch: 7 cost time: 1.7932238578796387[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2998749)[0m Epoch: 7, Steps: 529 | Train Loss: 0.3236303 Vali Loss: 0.2181302 Best vali loss: 0.2181302[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2998749)[0m Validation loss decreased (0.2183 --> 0.2181).  Saving model state dict ...[32m [repeated 3x across cluster][0m

Trial trial-3392c_00007 completed after 8 iterations at 2024-08-26 14:11:17. Total running time: 40s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             2.22242 â”‚
â”‚ time_total_s                                21.24444 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21794 â”‚
â”‚ train_loss                                   0.47454 â”‚
â”‚ valid_loss                                   0.21794 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2999492)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2999492)[0m 	iters: 700, epoch: 4 | loss: 0.3027871[32m [repeated 29x across cluster][0m
[36m(_train_fn pid=2999492)[0m 	speed: 0.0045s/iter; left time: 20.4568s[32m [repeated 29x across cluster][0m
[36m(_train_fn pid=2996326)[0m Updating learning rate to 6.276763945645828e-06[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2996326)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2996326)[0m Epoch: 5 cost time: 5.841036558151245[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2996326)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.2493316 Vali Loss: 0.2194182 Best vali loss: 0.2194182[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2996326)[0m Validation loss decreased (0.2200 --> 0.2194).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3000757)[0m Validation loss decreased (inf --> 0.2196).  Saving model state dict ...
[36m(_train_fn pid=2999492)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2996326)[0m 	iters: 900, epoch: 6 | loss: 1.1150689[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2996326)[0m 	speed: 0.0057s/iter; left time: 12.9215s[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=3000757)[0m Updating learning rate to 0.00013597630874475632[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3000757)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3000757)[0m Epoch: 1 cost time: 5.994083881378174[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3000757)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.3026002 Vali Loss: 0.2196360 Best vali loss: 0.2196360[32m [repeated 2x across cluster][0m

Trial trial-3392c_00008 completed after 5 iterations at 2024-08-26 14:11:28. Total running time: 51s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.43461 â”‚
â”‚ time_total_s                                28.20836 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21978 â”‚
â”‚ train_loss                                   0.43424 â”‚
â”‚ valid_loss                                   0.22857 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2996326)[0m Validation loss decreased (0.2194 --> 0.2192).  Saving model state dict ...
[36m(_train_fn pid=2999492)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2999492)[0m Early stopping
[36m(_train_fn pid=3000757)[0m 	iters: 200, epoch: 3 | loss: 0.4916064[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=3000757)[0m 	speed: 0.0057s/iter; left time: 34.7869s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=3000757)[0m Updating learning rate to 6.798815437237816e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3000757)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3000757)[0m Epoch: 2 cost time: 5.719341039657593[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3000757)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.4206071 Vali Loss: 0.2268381 Best vali loss: 0.2196360[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3000757)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2996326)[0m Validation loss decreased (0.2192 --> 0.2192).  Saving model state dict ...
[36m(_train_fn pid=2996326)[0m 	iters: 200, epoch: 8 | loss: 0.1334939[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2996326)[0m 	speed: 0.0049s/iter; left time: 4.2080s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3000757)[0m EarlyStopping counter: 2 out of 3

Trial status: 8 TERMINATED | 2 RUNNING
Current time: 2024-08-26 14:11:37. Total running time: 1min 0s
Logical resource usage: 2.0/32 CPUs, 0.5/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 3392c_00007 with best_valid_loss=0.21793961107623855 and params={'batch_size': 16, 'learning_rate': 0.00011855433557204775, 'd_model': 64, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0016394728399937975, 'lradj': 'cosine', 'd_ff': 64}
[36m(_train_fn pid=2996326)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109/trial-3392c_00002_2_alpha_d_ff=2,batch_size=8,d_core=128,d_model=32,dropout=0.0120,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_14-10-37/checkpoint_000007)[32m [repeated 2x across cluster][0m
2024-08-26 14:11:43,087	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:43,099	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109' in 0.0110s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-3392c_00002   RUNNING                 8       0.000100428          32              2        128            3   0.0119808     type1          7           57.1702        0.430271       0.219157            0.219157 â”‚
â”‚ trial-3392c_00009   RUNNING                 8       0.000135976         512              3         64            3   0.00208805    type1          3           20.4053        0.276238       0.22368             0.219636 â”‚
â”‚ trial-3392c_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         6           14.241         0.259194       0.227629            0.218033 â”‚
â”‚ trial-3392c_00001   TERMINATED             32       0.00240873          128              2        256            1   0.00405384    cosine         5            9.8275        0.370667       0.246449            0.22394  â”‚
â”‚ trial-3392c_00003   TERMINATED             32       0.000549219         512              2        256            4   0.00626912    cosine         4           13.8048        0.351489       0.23701             0.226163 â”‚
â”‚ trial-3392c_00004   TERMINATED            128       0.00414629          256              3         32            3   0.00455796    cosine         6            5.08434     nan              0.2672              0.256273 â”‚
â”‚ trial-3392c_00005   TERMINATED            128       8.27208e-05         128              1        256            2   0.00881643    type1          8            4.93846     nan              0.224983            0.224983 â”‚
â”‚ trial-3392c_00006   TERMINATED             16       0.000181215          32              2        512            2   0.00690173    type1          6           20.5502        0.367284       0.22136             0.220874 â”‚
â”‚ trial-3392c_00007   TERMINATED             16       0.000118554          64              1        128            1   0.00163947    cosine         8           21.2444        0.47454        0.21794             0.21794  â”‚
â”‚ trial-3392c_00008   TERMINATED              8       7.21523e-05         128              4        512            4   0.000846326   cosine         5           28.2084        0.43424        0.228572            0.21978  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3000757)[0m Updating learning rate to 3.399407718618908e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3000757)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3000757)[0m Epoch: 3 cost time: 5.477991342544556[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3000757)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.2762385 Vali Loss: 0.2236804 Best vali loss: 0.2196360[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3000757)[0m 	iters: 1000, epoch: 4 | loss: 0.1434212[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=3000757)[0m 	speed: 0.0042s/iter; left time: 17.8115s[32m [repeated 18x across cluster][0m

Trial trial-3392c_00002 completed after 8 iterations at 2024-08-26 14:11:42. Total running time: 1min 5s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                              6.8688 â”‚
â”‚ time_total_s                                64.03905 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21912 â”‚
â”‚ train_loss                                   0.23489 â”‚
â”‚ valid_loss                                   0.21912 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2996326)[0m Validation loss decreased (0.2192 --> 0.2191).  Saving model state dict ...

Trial trial-3392c_00009 completed after 4 iterations at 2024-08-26 14:11:43. Total running time: 1min 5s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3392c_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                              6.0927 â”‚
â”‚ time_total_s                                26.49799 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21964 â”‚
â”‚ train_loss                                   0.24634 â”‚
â”‚ valid_loss                                   0.23433 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:11:43. Total running time: 1min 5s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 3392c_00007 with best_valid_loss=0.21793961107623855 and params={'batch_size': 16, 'learning_rate': 0.00011855433557204775, 'd_model': 64, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0016394728399937975, 'lradj': 'cosine', 'd_ff': 64}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-3392c_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         6           14.241         0.259194       0.227629            0.218033 â”‚
â”‚ trial-3392c_00001   TERMINATED             32       0.00240873          128              2        256            1   0.00405384    cosine         5            9.8275        0.370667       0.246449            0.22394  â”‚
â”‚ trial-3392c_00002   TERMINATED              8       0.000100428          32              2        128            3   0.0119808     type1          8           64.039         0.234891       0.219117            0.219117 â”‚
â”‚ trial-3392c_00003   TERMINATED             32       0.000549219         512              2        256            4   0.00626912    cosine         4           13.8048        0.351489       0.23701             0.226163 â”‚
â”‚ trial-3392c_00004   TERMINATED            128       0.00414629          256              3         32            3   0.00455796    cosine         6            5.08434     nan              0.2672              0.256273 â”‚
â”‚ trial-3392c_00005   TERMINATED            128       8.27208e-05         128              1        256            2   0.00881643    type1          8            4.93846     nan              0.224983            0.224983 â”‚
â”‚ trial-3392c_00006   TERMINATED             16       0.000181215          32              2        512            2   0.00690173    type1          6           20.5502        0.367284       0.22136             0.220874 â”‚
â”‚ trial-3392c_00007   TERMINATED             16       0.000118554          64              1        128            1   0.00163947    cosine         8           21.2444        0.47454        0.21794             0.21794  â”‚
â”‚ trial-3392c_00008   TERMINATED              8       7.21523e-05         128              4        512            4   0.000846326   cosine         5           28.2084        0.43424        0.228572            0.21978  â”‚
â”‚ trial-3392c_00009   TERMINATED              8       0.000135976         512              3         64            3   0.00208805    type1          4           26.498         0.24634        0.234326            0.219636 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 16, 'learning_rate': 0.00011855433557204775, 'd_model': 64, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0016394728399937975, 'lradj': 'cosine', 'd_ff': 64}
[36m(_train_fn pid=3000757)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed109/trial-3392c_00009_9_alpha_d_ff=3,batch_size=8,d_core=64,d_model=512,dropout=0.0021,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_14-11-15/checkpoint_000003)
[36m(_train_fn pid=3000757)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3000757)[0m Early stopping
[36m(_train_fn pid=3000757)[0m Updating learning rate to 1.699703859309454e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3000757)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3000757)[0m Epoch: 4 cost time: 5.433501482009888[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3000757)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.2463401 Vali Loss: 0.2343263 Best vali loss: 0.2196360[32m [repeated 2x across cluster][0m


Time taken (4 parallel trials): 70 seconds


2024-08-26 14:11:47,211	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:11:47,619	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:11:47,624	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:11:47,631	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:11:52,594	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed1010   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator       â”‚
â”‚ Scheduler                        FIFOScheduler               â”‚
â”‚ Number of trials                 10                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-11-46_779977_3001784/artifacts/2024-08-26_14-11-47/ETTh2_96_96_test_seed1010/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:11:47. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-5d94c_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-5d94c_00001   PENDING              32       0.000529475         512              1        256            3   0.00153171   type1   â”‚
â”‚ trial-5d94c_00002   PENDING              16       6.86391e-05         512              3        256            4   0.00267454   cosine  â”‚
â”‚ trial-5d94c_00003   PENDING              16       0.000169235         128              4         32            4   0.00301574   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-5d94c_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00153 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00053 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-5d94c_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00302 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00017 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-5d94c_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00267 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00007 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-5d94c_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3004009)[0m configuration
[36m(_train_fn pid=3004009)[0m {'batch_size': 16, 'learning_rate': 6.863907390163623e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 4, 'dropout': 0.0026745414898141197, 'lradj': 'cosine', 'd_ff': 1536}
[36m(_train_fn pid=3004010)[0m {'batch_size': 16, 'learning_rate': 0.0001692352850701102, 'd_model': 128, 'd_core': 32, 'e_layers': 4, 'dropout': 0.003015742921468875, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=3004008)[0m Use GPU: cuda:0
[36m(_train_fn pid=3004009)[0m train 8449
[36m(_train_fn pid=3004009)[0m val 2785
[36m(_train_fn pid=3004009)[0m start_epoch 0
[36m(_train_fn pid=3004009)[0m max_epoch 8
[36m(_train_fn pid=3004007)[0m 	iters: 100, epoch: 1 | loss: 0.2794905
[36m(_train_fn pid=3004007)[0m 	speed: 0.0137s/iter; left time: 27.6420s
[36m(_train_fn pid=3004007)[0m Updating learning rate to 0.000288581929876693
[36m(_train_fn pid=3004007)[0m saving checkpoint...
[36m(_train_fn pid=3004007)[0m Validation loss decreased (inf --> 0.2165).  Saving model state dict ...
[36m(_train_fn pid=3004007)[0m Epoch: 1 cost time: 2.4744303226470947
[36m(_train_fn pid=3004007)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00000_0_2024-08-26_14-11-47/checkpoint_000000)
2024-08-26 14:11:53,475	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:55,256	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:57,000	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:57,916	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3004007)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00000_0_2024-08-26_14-11-47/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 14:11:59,569	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:11:59,599	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:00,093	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:00,304	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:03,037	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3005219)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00004_4_alpha_d_ff=2,batch_size=128,d_core=256,d_model=256,dropout=0.0023,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_14-12-00/checkpoint_000000)[32m [repeated 5x across cluster][0m
2024-08-26 14:12:03,151	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:03,539	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:04,024	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:04,504	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:04,985	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:06,310	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3004007)[0m Epoch: 1, Steps: 265 | Train Loss: 0.2792686 Vali Loss: 0.2165236 Best vali loss: 0.2165236
[36m(_train_fn pid=3004008)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=3004007)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=3004008)[0m {'batch_size': 32, 'learning_rate': 0.0005294751313984894, 'd_model': 512, 'd_core': 256, 'e_layers': 3, 'dropout': 0.001531706665548935, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=3004010)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3004008)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3004008)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3004008)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3004008)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3004007)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3004007)[0m 	iters: 100, epoch: 3 | loss: 0.3728422[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3004007)[0m 	speed: 0.0191s/iter; left time: 28.5468s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3004008)[0m Validation loss decreased (0.2436 --> 0.2283).  Saving model state dict ...
[36m(_train_fn pid=3004008)[0m Updating learning rate to 0.0002647375656992447[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3004008)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3004008)[0m Validation loss decreased (inf --> 0.2436).  Saving model state dict ...
[36m(_train_fn pid=3004008)[0m Epoch: 2 cost time: 3.062568426132202[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3004008)[0m Epoch: 2, Steps: 265 | Train Loss: 0.2653616 Vali Loss: 0.2283125 Best vali loss: 0.2283125[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3004007)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3004009)[0m Validation loss decreased (inf --> 0.2145).  Saving model state dict ...

Trial trial-5d94c_00000 completed after 4 iterations at 2024-08-26 14:12:00. Total running time: 12s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.38513 â”‚
â”‚ time_total_s                                11.09398 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21652 â”‚
â”‚ train_loss                                    0.3788 â”‚
â”‚ valid_loss                                   0.22347 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3004007)[0m Early stopping
[36m(_train_fn pid=3004007)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m

Trial trial-5d94c_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00227 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00038 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3005219)[0m configuration
[36m(_train_fn pid=3005219)[0m {'batch_size': 128, 'learning_rate': 0.00037570545144950375, 'd_model': 256, 'd_core': 256, 'e_layers': 2, 'dropout': 0.002265025765037468, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=3005219)[0m Use GPU: cuda:0
[36m(_train_fn pid=3004008)[0m 	iters: 100, epoch: 4 | loss: 0.2012864[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3004008)[0m 	speed: 0.0203s/iter; left time: 24.8973s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3005219)[0m train 8449
[36m(_train_fn pid=3005219)[0m val 2785
[36m(_train_fn pid=3005219)[0m start_epoch 0
[36m(_train_fn pid=3005219)[0m max_epoch 8
[36m(_train_fn pid=3005219)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2194445 Best vali loss: 0.2194445
[36m(_train_fn pid=3005219)[0m Updating learning rate to 0.00037570545144950375[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3005219)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3005219)[0m Epoch: 1 cost time: 0.6291217803955078[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3004007)[0m Epoch: 4, Steps: 265 | Train Loss: 0.3787992 Vali Loss: 0.2234730 Best vali loss: 0.2165236[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3005219)[0m Validation loss decreased (0.2194 --> 0.2190).  Saving model state dict ...
[36m(_train_fn pid=3005219)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2189560 Best vali loss: 0.2189560
[36m(_train_fn pid=3005219)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2205999 Best vali loss: 0.2189560
[36m(_train_fn pid=3005219)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2231732 Best vali loss: 0.2189560

Trial trial-5d94c_00004 completed after 5 iterations at 2024-08-26 14:12:04. Total running time: 17s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             0.47863 â”‚
â”‚ time_total_s                                 3.14637 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21896 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.22167 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3005219)[0m Validation loss decreased (inf --> 0.2194).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3005219)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2216749 Best vali loss: 0.2189560

Trial trial-5d94c_00001 completed after 5 iterations at 2024-08-26 14:12:06. Total running time: 18s
2024-08-26 14:12:07,186	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:07,209	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:09,348	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3005753)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00005_5_alpha_d_ff=4,batch_size=128,d_core=128,d_model=512,dropout=0.0022,e_layers=4,learning_rate=0.0024,lradj=type1_2024-08-26_14-12-04/checkpoint_000000)[32m [repeated 9x across cluster][0m
2024-08-26 14:12:11,315	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:12,235	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:13,349	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:15,319	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3005753)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00005_5_alpha_d_ff=4,batch_size=128,d_core=128,d_model=512,dropout=0.0022,e_layers=4,learning_rate=0.0024,lradj=type1_2024-08-26_14-12-04/checkpoint_000003)[32m [repeated 4x across cluster][0m
2024-08-26 14:12:16,041	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:17,342	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             3.15675 â”‚
â”‚ time_total_s                                17.11985 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22831 â”‚
â”‚ train_loss                                   0.27443 â”‚
â”‚ valid_loss                                   0.24434 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3005219)[0m Early stopping
[36m(_train_fn pid=3005219)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3004008)[0m Early stopping

Trial trial-5d94c_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00225 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00243 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3005753)[0m configuration
[36m(_train_fn pid=3005753)[0m {'batch_size': 128, 'learning_rate': 0.002429224228101269, 'd_model': 512, 'd_core': 128, 'e_layers': 4, 'dropout': 0.0022461988374000543, 'lradj': 'type1', 'd_ff': 2048}
[36m(_train_fn pid=3005753)[0m Use GPU: cuda:0
[36m(_train_fn pid=3004010)[0m 	iters: 500, epoch: 2 | loss: 0.1662802[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=3004010)[0m 	speed: 0.0137s/iter; left time: 43.7495s[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=3005753)[0m train 8449
[36m(_train_fn pid=3005753)[0m val 2785

Trial trial-5d94c_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.01058 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00068 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3005852)[0m configuration
[36m(_train_fn pid=3005852)[0m {'batch_size': 8, 'learning_rate': 0.0006809438948267272, 'd_model': 256, 'd_core': 256, 'e_layers': 2, 'dropout': 0.010581433509769197, 'lradj': 'type1', 'd_ff': 256}
[36m(_train_fn pid=3005753)[0m start_epoch 0
[36m(_train_fn pid=3005753)[0m max_epoch 8
[36m(_train_fn pid=3005852)[0m Use GPU: cuda:0
[36m(_train_fn pid=3005852)[0m train 8449
[36m(_train_fn pid=3005852)[0m val 2785
[36m(_train_fn pid=3005852)[0m start_epoch 0
[36m(_train_fn pid=3005852)[0m max_epoch 8
[36m(_train_fn pid=3004010)[0m Updating learning rate to 8.46176425350551e-05[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3004010)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3004010)[0m Epoch: 2 cost time: 6.985631942749023[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3004010)[0m Epoch: 2, Steps: 529 | Train Loss: 0.2450733 Vali Loss: 0.2178196 Best vali loss: 0.2178196[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3004010)[0m Validation loss decreased (0.2193 --> 0.2178).  Saving model state dict ...
[36m(_train_fn pid=3005753)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2748683 Best vali loss: 0.2748683
[36m(_train_fn pid=3005753)[0m Validation loss decreased (inf --> 0.2749).  Saving model state dict ...
[36m(_train_fn pid=3005753)[0m Validation loss decreased (0.2749 --> 0.2739).  Saving model state dict ...
[36m(_train_fn pid=3005753)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2739117 Best vali loss: 0.2739117
[36m(_train_fn pid=3004009)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3005852)[0m Validation loss decreased (inf --> 0.2221).  Saving model state dict ...
[36m(_train_fn pid=3005852)[0m 	iters: 1000, epoch: 1 | loss: 0.2234188[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3005852)[0m 	speed: 0.0030s/iter; left time: 22.4474s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3005753)[0m Validation loss decreased (0.2739 --> 0.2735).  Saving model state dict ...
[36m(_train_fn pid=3005753)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2734731 Best vali loss: 0.2734731
[36m(_train_fn pid=3005753)[0m Updating learning rate to 0.0006073060570253173[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3005753)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3005753)[0m Epoch: 3 cost time: 1.7278780937194824[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3005852)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.2588186 Vali Loss: 0.2220573 Best vali loss: 0.2220573
[36m(_train_fn pid=3005753)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2793969 Best vali loss: 0.2734731
[36m(_train_fn pid=3005852)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.3629176 Vali Loss: 0.2250424 Best vali loss: 0.2220573
[36m(_train_fn pid=3005852)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3005753)[0m Validation loss decreased (0.2735 --> 0.2672).  Saving model state dict ...
[36m(_train_fn pid=3004010)[0m 	iters: 500, epoch: 3 | loss: 0.3432509[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=3004010)[0m 	speed: 0.0204s/iter; left time: 54.5546s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=3005753)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2671636 Best vali loss: 0.2671636

Trial status: 3 TERMINATED | 4 RUNNING
Current time: 2024-08-26 14:12:17. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 5d94c_00002 with best_valid_loss=0.21448706631386516 and params={'batch_size': 16, 'learning_rate': 6.863907390163623e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 4, 'dropout': 0.0026745414898141197, 'lradj': 'cosine', 'd_ff': 1536}
2024-08-26 14:12:18,904	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:18,960	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:19,052	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:19,927	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:20,931	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3005753)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00005_5_alpha_d_ff=4,batch_size=128,d_core=128,d_model=512,dropout=0.0022,e_layers=4,learning_rate=0.0024,lradj=type1_2024-08-26_14-12-04/checkpoint_000006)[32m [repeated 7x across cluster][0m
2024-08-26 14:12:22,831	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:23,670	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:25,957	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-5d94c_00002   RUNNING                16       6.86391e-05         512              3        256            4   0.00267454   cosine         2           17.9781        0.477902       0.218119            0.214487 â”‚
â”‚ trial-5d94c_00003   RUNNING                16       0.000169235         128              4         32            4   0.00301574   type1          2           18.0029        0.245073       0.21782             0.21782  â”‚
â”‚ trial-5d94c_00005   RUNNING               128       0.00242922          512              4        128            4   0.0022462    type1          5           10.7917      nan              0.267164            0.267164 â”‚
â”‚ trial-5d94c_00006   RUNNING                 8       0.000680944         256              1        256            2   0.0105814    type1          2            8.17585       0.362918       0.225042            0.222057 â”‚
â”‚ trial-5d94c_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           11.094         0.378799       0.223473            0.216524 â”‚
â”‚ trial-5d94c_00001   TERMINATED             32       0.000529475         512              1        256            3   0.00153171   type1          5           17.1199        0.274431       0.244337            0.228312 â”‚
â”‚ trial-5d94c_00004   TERMINATED            128       0.000375705         256              2        256            2   0.00226503   type1          5            3.14637     nan              0.221675            0.218956 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3005753)[0m Updating learning rate to 0.0001518265142563293[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3005753)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3005753)[0m Epoch: 5 cost time: 1.737962007522583[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3005753)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2708599 Best vali loss: 0.2671636
[36m(_train_fn pid=3005753)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2708722 Best vali loss: 0.2671636
[36m(_train_fn pid=3005852)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4317691 Vali Loss: 0.2208384 Best vali loss: 0.2208384[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3005753)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3005852)[0m Validation loss decreased (0.2221 --> 0.2208).  Saving model state dict ...
[36m(_train_fn pid=3005852)[0m 	iters: 800, epoch: 4 | loss: 0.1281753[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=3005852)[0m 	speed: 0.0031s/iter; left time: 13.7361s[32m [repeated 17x across cluster][0m

Trial trial-5d94c_00005 completed after 8 iterations at 2024-08-26 14:12:22. Total running time: 35s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.89724 â”‚
â”‚ time_total_s                                16.26676 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.26716 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.27091 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3005753)[0m Epoch: 8, Steps: 67 | Train Loss: nan Vali Loss: 0.2709137 Best vali loss: 0.2671636
[36m(_train_fn pid=3005753)[0m Early stopping
[36m(_train_fn pid=3005753)[0m Updating learning rate to 1.8978314282041164e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3005753)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3005753)[0m Epoch: 8 cost time: 1.6744122505187988[32m [repeated 6x across cluster][0m

Trial trial-5d94c_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00195 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00014 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3007123)[0m configuration
[36m(_train_fn pid=3007123)[0m {'batch_size': 128, 'learning_rate': 0.00013705137833352736, 'd_model': 64, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0019463778409547986, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=3007123)[0m Use GPU: cuda:0
[36m(_train_fn pid=3007123)[0m train 8449
[36m(_train_fn pid=3007123)[0m val 2785
[36m(_train_fn pid=3007123)[0m start_epoch 0
[36m(_train_fn pid=3007123)[0m max_epoch 8
[36m(_train_fn pid=3007123)[0m Validation loss decreased (inf --> 0.2627).  Saving model state dict ...
[36m(_train_fn pid=3007123)[0m Epoch: 1, Steps: 67 | Train Loss: nan Vali Loss: 0.2626835 Best vali loss: 0.2626835
2024-08-26 14:12:26,395	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:26,441	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3004010)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00003_3_alpha_d_ff=4,batch_size=16,d_core=32,d_model=128,dropout=0.0030,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_14-11-47/checkpoint_000003)[32m [repeated 4x across cluster][0m
2024-08-26 14:12:26,647	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:27,309	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:27,530	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:27,966	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:28,635	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:29,430	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:30,243	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:30,603	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:30,905	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:31,009	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:31,442	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3005852)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.2382407 Vali Loss: 0.2386992 Best vali loss: 0.2208384

Trial trial-5d94c_00002 completed after 4 iterations at 2024-08-26 14:12:26. Total running time: 38s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             7.48865 â”‚
â”‚ time_total_s                                 37.1816 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21449 â”‚
â”‚ train_loss                                   0.33448 â”‚
â”‚ valid_loss                                   0.21962 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3004009)[0m Epoch: 4, Steps: 529 | Train Loss: 0.3344819 Vali Loss: 0.2196207 Best vali loss: 0.2144871
[36m(_train_fn pid=3007123)[0m Validation loss decreased (0.2627 --> 0.2408).  Saving model state dict ...
[36m(_train_fn pid=3007123)[0m Epoch: 2, Steps: 67 | Train Loss: nan Vali Loss: 0.2407822 Best vali loss: 0.2407822
[36m(_train_fn pid=3004010)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3007123)[0m Validation loss decreased (0.2408 --> 0.2346).  Saving model state dict ...
[36m(_train_fn pid=3007123)[0m Epoch: 3, Steps: 67 | Train Loss: nan Vali Loss: 0.2346176 Best vali loss: 0.2346176
[36m(_train_fn pid=3004010)[0m 	iters: 100, epoch: 5 | loss: 0.3107133[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=3004010)[0m 	speed: 0.0170s/iter; left time: 34.3274s[32m [repeated 21x across cluster][0m

Trial trial-5d94c_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00174 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00024 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3004009)[0m Early stopping
[36m(_train_fn pid=3007123)[0m Validation loss decreased (0.2346 --> 0.2321).  Saving model state dict ...
[36m(_train_fn pid=3007123)[0m Epoch: 4, Steps: 67 | Train Loss: nan Vali Loss: 0.2321460 Best vali loss: 0.2321460
[36m(_train_fn pid=3007123)[0m Validation loss decreased (0.2321 --> 0.2313).  Saving model state dict ...
[36m(_train_fn pid=3007123)[0m Epoch: 5, Steps: 67 | Train Loss: nan Vali Loss: 0.2312605 Best vali loss: 0.2312605
[36m(_train_fn pid=3007123)[0m Updating learning rate to 8.56571114584546e-06[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3007123)[0m saving checkpoint...[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3007123)[0m Epoch: 5 cost time: 0.5365369319915771[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3007123)[0m Validation loss decreased (0.2313 --> 0.2307).  Saving model state dict ...
[36m(_train_fn pid=3007469)[0m configuration
[36m(_train_fn pid=3007469)[0m {'batch_size': 32, 'learning_rate': 0.00024229298373008601, 'd_model': 128, 'd_core': 128, 'e_layers': 3, 'dropout': 0.0017367303005084082, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=3007469)[0m Use GPU: cuda:0
[36m(_train_fn pid=3007123)[0m Epoch: 6, Steps: 67 | Train Loss: nan Vali Loss: 0.2307328 Best vali loss: 0.2307328
[36m(_train_fn pid=3007469)[0m train 8449
[36m(_train_fn pid=3007469)[0m val 2785
[36m(_train_fn pid=3007469)[0m start_epoch 0
[36m(_train_fn pid=3007469)[0m max_epoch 8
[36m(_train_fn pid=3007123)[0m Validation loss decreased (0.2307 --> 0.2306).  Saving model state dict ...
[36m(_train_fn pid=3007123)[0m Epoch: 7, Steps: 67 | Train Loss: nan Vali Loss: 0.2305804 Best vali loss: 0.2305804

Trial trial-5d94c_00007 completed after 8 iterations at 2024-08-26 14:12:30. Total running time: 43s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.65876 â”‚
â”‚ time_total_s                                 6.55725 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.23048 â”‚
â”‚ train_loss                                       nan â”‚
â”‚ valid_loss                                   0.23048 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3007123)[0m Validation loss decreased (0.2306 --> 0.2305).  Saving model state dict ...
[36m(_train_fn pid=3007123)[0m Epoch: 8, Steps: 67 | Train Loss: nan Vali Loss: 0.2304759 Best vali loss: 0.2304759

Trial trial-5d94c_00003 completed after 5 iterations at 2024-08-26 14:12:31. Total running time: 43s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             4.56521 â”‚
â”‚ time_total_s                                41.79429 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21782 â”‚
â”‚ train_loss                                   0.27426 â”‚
â”‚ valid_loss                                   0.22178 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3004010)[0m Early stopping
[36m(_train_fn pid=3007469)[0m Validation loss decreased (inf --> 0.2189).  Saving model state dict ...

Trial trial-5d94c_00006 completed after 6 iterations at 2024-08-26 14:12:31. Total running time: 43s
2024-08-26 14:12:32,016	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3007469)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00008_8_alpha_d_ff=1,batch_size=32,d_core=128,d_model=128,dropout=0.0017,e_layers=3,learning_rate=0.0002,lradj=type1_2024-08-26_14-12-26/checkpoint_000001)[32m [repeated 13x across cluster][0m
2024-08-26 14:12:33,425	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:35,072	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:12:38,882	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3008314)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00009_9_alpha_d_ff=2,batch_size=8,d_core=512,d_model=256,dropout=0.0018,e_layers=4,learning_rate=0.0040,lradj=cosine_2024-08-26_14-12-30/checkpoint_000000)[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3008314)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00009_9_alpha_d_ff=2,batch_size=8,d_core=512,d_model=256,dropout=0.0018,e_layers=4,learning_rate=0.0040,lradj=cosine_2024-08-26_14-12-30/checkpoint_000001)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             3.90974 â”‚
â”‚ time_total_s                                23.56562 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22084 â”‚
â”‚ train_loss                                   0.30557 â”‚
â”‚ valid_loss                                    0.2397 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3005852)[0m Epoch: 6, Steps: 1057 | Train Loss: 0.3055727 Vali Loss: 0.2397047 Best vali loss: 0.2208384[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3007469)[0m EarlyStopping counter: 1 out of 3[32m [repeated 4x across cluster][0m

Trial trial-5d94c_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                             0.0018 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00395 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3008314)[0m configuration
[36m(_train_fn pid=3008314)[0m {'batch_size': 8, 'learning_rate': 0.003954296620329546, 'd_model': 256, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0017983161657607273, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=3008314)[0m Use GPU: cuda:0
[36m(_train_fn pid=3008314)[0m train 8449
[36m(_train_fn pid=3008314)[0m val 2785
[36m(_train_fn pid=3007469)[0m 	iters: 200, epoch: 3 | loss: 0.3051564[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=3007469)[0m 	speed: 0.0042s/iter; left time: 5.8816s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=3008314)[0m start_epoch 0
[36m(_train_fn pid=3008314)[0m max_epoch 8
[36m(_train_fn pid=3007469)[0m Updating learning rate to 6.0573245932521504e-05[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3007469)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3007469)[0m Epoch: 3 cost time: 1.158029556274414[32m [repeated 8x across cluster][0m

Trial trial-5d94c_00008 completed after 4 iterations at 2024-08-26 14:12:35. Total running time: 47s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.64543 â”‚
â”‚ time_total_s                                  7.1123 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                               0.2189 â”‚
â”‚ train_loss                                   0.51699 â”‚
â”‚ valid_loss                                   0.22234 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3007469)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3007469)[0m Epoch: 4, Steps: 265 | Train Loss: 0.5169915 Vali Loss: 0.2223414 Best vali loss: 0.2188971[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3007469)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3008314)[0m 	iters: 1000, epoch: 1 | loss: 0.2359909[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3008314)[0m 	speed: 0.0043s/iter; left time: 32.1530s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3008314)[0m Validation loss decreased (inf --> 0.2680).  Saving model state dict ...
[36m(_train_fn pid=3008314)[0m Updating learning rate to 0.003803795166665284[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3008314)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3008314)[0m Epoch: 1 cost time: 5.3857643604278564[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3008314)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.5624570 Vali Loss: 0.2680174 Best vali loss: 0.2680174
[36m(_train_fn pid=3008314)[0m 	iters: 900, epoch: 2 | loss: 0.2391714[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3008314)[0m 	speed: 0.0047s/iter; left time: 30.6916s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3008314)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3008314)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.6087496 Vali Loss: 0.2818123 Best vali loss: 0.2680174
[36m(_train_fn pid=3008314)[0m Updating learning rate to 0.003375203287693807
[36m(_train_fn pid=3008314)[0m saving checkpoint...
[36m(_train_fn pid=3008314)[0m Epoch: 2 cost time: 5.13911771774292

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2024-08-26 14:12:47. Total running time: 1min 0s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 5d94c_00002 with best_valid_loss=0.21448706631386516 and params={'batch_size': 16, 'learning_rate': 6.863907390163623e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 4, 'dropout': 0.0026745414898141197, 'lradj': 'cosine', 'd_ff': 1536}
[36m(_train_fn pid=3008314)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00009_9_alpha_d_ff=2,batch_size=8,d_core=512,d_model=256,dropout=0.0018,e_layers=4,learning_rate=0.0040,lradj=cosine_2024-08-26_14-12-30/checkpoint_000002)
[36m(_train_fn pid=3008314)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00009_9_alpha_d_ff=2,batch_size=8,d_core=512,d_model=256,dropout=0.0018,e_layers=4,learning_rate=0.0040,lradj=cosine_2024-08-26_14-12-30/checkpoint_000003)
[36m(_train_fn pid=3008314)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00009_9_alpha_d_ff=2,batch_size=8,d_core=512,d_model=256,dropout=0.0018,e_layers=4,learning_rate=0.0040,lradj=cosine_2024-08-26_14-12-30/checkpoint_000004)
2024-08-26 14:13:06,344	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010' in 0.0108s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-5d94c_00009   RUNNING                 8       0.0039543           256              2        512            4   0.00179832   cosine         2           12.2405        0.60875        0.281812            0.268017 â”‚
â”‚ trial-5d94c_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           11.094         0.378799       0.223473            0.216524 â”‚
â”‚ trial-5d94c_00001   TERMINATED             32       0.000529475         512              1        256            3   0.00153171   type1          5           17.1199        0.274431       0.244337            0.228312 â”‚
â”‚ trial-5d94c_00002   TERMINATED             16       6.86391e-05         512              3        256            4   0.00267454   cosine         4           37.1816        0.334482       0.219621            0.214487 â”‚
â”‚ trial-5d94c_00003   TERMINATED             16       0.000169235         128              4         32            4   0.00301574   type1          5           41.7943        0.274258       0.221777            0.21782  â”‚
â”‚ trial-5d94c_00004   TERMINATED            128       0.000375705         256              2        256            2   0.00226503   type1          5            3.14637     nan              0.221675            0.218956 â”‚
â”‚ trial-5d94c_00005   TERMINATED            128       0.00242922          512              4        128            4   0.0022462    type1          8           16.2668      nan              0.270914            0.267164 â”‚
â”‚ trial-5d94c_00006   TERMINATED              8       0.000680944         256              1        256            2   0.0105814    type1          6           23.5656        0.305573       0.239705            0.220838 â”‚
â”‚ trial-5d94c_00007   TERMINATED            128       0.000137051          64              1         32            4   0.00194638   type1          8            6.55725     nan              0.230476            0.230476 â”‚
â”‚ trial-5d94c_00008   TERMINATED             32       0.000242293         128              1        128            3   0.00173673   type1          4            7.1123        0.516992       0.222341            0.218897 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3008314)[0m 	iters: 800, epoch: 3 | loss: 0.3227044[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3008314)[0m 	speed: 0.0049s/iter; left time: 27.4197s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3008314)[0m Updating learning rate to 0.0027337702117934655
[36m(_train_fn pid=3008314)[0m saving checkpoint...
[36m(_train_fn pid=3008314)[0m Validation loss decreased (0.2680 --> 0.2658).  Saving model state dict ...
[36m(_train_fn pid=3008314)[0m Epoch: 3 cost time: 5.109522104263306
[36m(_train_fn pid=3008314)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.3707874 Vali Loss: 0.2658105 Best vali loss: 0.2658105
[36m(_train_fn pid=3008314)[0m 	iters: 800, epoch: 4 | loss: 0.2434575[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3008314)[0m 	speed: 0.0043s/iter; left time: 19.4944s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3008314)[0m Updating learning rate to 0.001977148310164773
[36m(_train_fn pid=3008314)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3008314)[0m saving checkpoint...
[36m(_train_fn pid=3008314)[0m Epoch: 4 cost time: 4.649977922439575
[36m(_train_fn pid=3008314)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.6157590 Vali Loss: 0.2716888 Best vali loss: 0.2658105
[36m(_train_fn pid=3008314)[0m 	iters: 800, epoch: 5 | loss: 0.5981530[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3008314)[0m 	speed: 0.0043s/iter; left time: 14.8241s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3008314)[0m Updating learning rate to 0.0012205264085360806
[36m(_train_fn pid=3008314)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3008314)[0m saving checkpoint...
[36m(_train_fn pid=3008314)[0m Epoch: 5 cost time: 4.60571813583374
[36m(_train_fn pid=3008314)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.5466904 Vali Loss: 0.2704563 Best vali loss: 0.2658105
[36m(_train_fn pid=3008314)[0m 	iters: 800, epoch: 6 | loss: 0.7007311[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3008314)[0m 	speed: 0.0046s/iter; left time: 10.8148s[32m [repeated 10x across cluster][0m

Trial trial-5d94c_00009 completed after 6 iterations at 2024-08-26 14:13:06. Total running time: 1min 18s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-5d94c_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             5.36641 â”‚
â”‚ time_total_s                                33.88456 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.26581 â”‚
â”‚ train_loss                                   0.57751 â”‚
â”‚ valid_loss                                   0.27033 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:13:06. Total running time: 1min 18s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 5d94c_00002 with best_valid_loss=0.21448706631386516 and params={'batch_size': 16, 'learning_rate': 6.863907390163623e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 4, 'dropout': 0.0026745414898141197, 'lradj': 'cosine', 'd_ff': 1536}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-5d94c_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           11.094         0.378799       0.223473            0.216524 â”‚
â”‚ trial-5d94c_00001   TERMINATED             32       0.000529475         512              1        256            3   0.00153171   type1          5           17.1199        0.274431       0.244337            0.228312 â”‚
â”‚ trial-5d94c_00002   TERMINATED             16       6.86391e-05         512              3        256            4   0.00267454   cosine         4           37.1816        0.334482       0.219621            0.214487 â”‚
â”‚ trial-5d94c_00003   TERMINATED             16       0.000169235         128              4         32            4   0.00301574   type1          5           41.7943        0.274258       0.221777            0.21782  â”‚
â”‚ trial-5d94c_00004   TERMINATED            128       0.000375705         256              2        256            2   0.00226503   type1          5            3.14637     nan              0.221675            0.218956 â”‚
â”‚ trial-5d94c_00005   TERMINATED            128       0.00242922          512              4        128            4   0.0022462    type1          8           16.2668      nan              0.270914            0.267164 â”‚
â”‚ trial-5d94c_00006   TERMINATED              8       0.000680944         256              1        256            2   0.0105814    type1          6           23.5656        0.305573       0.239705            0.220838 â”‚
â”‚ trial-5d94c_00007   TERMINATED            128       0.000137051          64              1         32            4   0.00194638   type1          8            6.55725     nan              0.230476            0.230476 â”‚
â”‚ trial-5d94c_00008   TERMINATED             32       0.000242293         128              1        128            3   0.00173673   type1          4            7.1123        0.516992       0.222341            0.218897 â”‚
â”‚ trial-5d94c_00009   TERMINATED              8       0.0039543           256              2        512            4   0.00179832   cosine         6           33.8846        0.577505       0.270329            0.26581  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 16, 'learning_rate': 6.863907390163623e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 4, 'dropout': 0.0026745414898141197, 'lradj': 'cosine', 'd_ff': 1536}
[36m(_train_fn pid=3008314)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1010/trial-5d94c_00009_9_alpha_d_ff=2,batch_size=8,d_core=512,d_model=256,dropout=0.0018,e_layers=4,learning_rate=0.0040,lradj=cosine_2024-08-26_14-12-30/checkpoint_000005)
[36m(_train_fn pid=3008314)[0m Updating learning rate to 0.0005790933326357388
[36m(_train_fn pid=3008314)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3008314)[0m saving checkpoint...
[36m(_train_fn pid=3008314)[0m Epoch: 6 cost time: 4.715481996536255
[36m(_train_fn pid=3008314)[0m Epoch: 6, Steps: 1057 | Train Loss: 0.5775053 Vali Loss: 0.2703291 Best vali loss: 0.2658105
[36m(_train_fn pid=3008314)[0m Early stopping
[36m(_train_fn pid=3008314)[0m 	iters: 1000, epoch: 6 | loss: 0.2601040[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3008314)[0m 	speed: 0.0046s/iter; left time: 9.8969s[32m [repeated 2x across cluster][0m


Time taken (4 parallel trials): 83 seconds




2024-08-26 14:15:12,475	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:15:12,806	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:15:12,811	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:15:12,818	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:15:15,681	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3011870)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1001/trial-d7df6_00003_3_alpha_d_ff=2,batch_size=128,d_core=256,d_model=32,dropout=0.0030,e_layers=2,learning_rate=0.0046,lradj=cosine_2024-08-26_14-15-12/checkpoint_000000)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed1001   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator       â”‚
â”‚ Scheduler                        FIFOScheduler               â”‚
â”‚ Number of trials                 10                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1001
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-15-11_950982_3009633/artifacts/2024-08-26_14-15-12/ETTh2_96_96_test_seed1001/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:15:12. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-d7df6_00000   PENDING              32       0.0003              128              1         64            2   0             cosine  â”‚
â”‚ trial-d7df6_00001   PENDING             128       5.37491e-05         256              3         32            4   0.000983346   type1   â”‚
â”‚ trial-d7df6_00002   PENDING              32       0.000910274          64              1         32            3   0.00130339    cosine  â”‚
â”‚ trial-d7df6_00003   PENDING             128       0.00464733           32              2        256            2   0.00302068    cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d7df6_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00302 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00465 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3011870)[0m configuration
[36m(_train_fn pid=3011870)[0m {'batch_size': 128, 'learning_rate': 0.004647334829884925, 'd_model': 32, 'd_core': 256, 'e_layers': 2, 'dropout': 0.003020682207810305, 'lradj': 'cosine', 'd_ff': 64}
[36m(_train_fn pid=3011870)[0m Use GPU: cuda:0

Trial trial-d7df6_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                             0.0013 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00091 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d7df6_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d7df6_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00098 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00005 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3011870)[0m train 8449
[36m(_train_fn pid=3011870)[0m val 2785
[36m(_train_fn pid=3011868)[0m {'batch_size': 128, 'learning_rate': 5.3749068237455085e-05, 'd_model': 256, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0009833464057108156, 'lradj': 'type1', 'd_ff': 768}
[36m(_train_fn pid=3011870)[0m start_epoch 0
[36m(_train_fn pid=3011870)[0m max_epoch 8
[36m(_train_fn pid=3011870)[0m Updating learning rate to 0.004470456179971215
[36m(_train_fn pid=3011870)[0m saving checkpoint...
[36m(_train_fn pid=3011870)[0m Validation loss decreased (inf --> 0.2189).  Saving model state dict ...
[36m(_train_fn pid=3011870)[0m Epoch: 1 cost time: 0.7712411880493164
[36m(_train_fn pid=3011870)[0m Epoch: 1, Steps: 67 | Train Loss: 0.4400852 Vali Loss: 0.2189062 Best vali loss: 0.2189062
2024-08-26 14:15:16,345	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:16,400	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3011870)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1001/trial-d7df6_00003_3_alpha_d_ff=2,batch_size=128,d_core=256,d_model=32,dropout=0.0030,e_layers=2,learning_rate=0.0046,lradj=cosine_2024-08-26_14-15-12/checkpoint_000001)
2024-08-26 14:15:17,177	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:17,542	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:17,844	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:17,908	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:18,477	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:18,558	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:19,584	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:20,167	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:20,519	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:20,572	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:20,948	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3012868)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1001/trial-d7df6_00004_4_alpha_d_ff=2,batch_size=128,d_core=512,d_model=64,dropout=0.0014,e_layers=2,learning_rate=0.0003,lradj=cosine_2024-08-26_14-15-17/checkpoint_000001)[32m [repeated 12x across cluster][0m
2024-08-26 14:15:21,386	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:21,454	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:21,567	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:21,836	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:22,253	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:22,454	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:22,586	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:22,710	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:23,150	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:23,561	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:23,620	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:23,998	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:24,252	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3011867)[0m 	iters: 100, epoch: 1 | loss: 0.1955938
[36m(_train_fn pid=3011867)[0m 	speed: 0.0156s/iter; left time: 31.4535s
[36m(_train_fn pid=3011870)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3011870)[0m Epoch: 2 cost time: 0.5976808071136475
[36m(_train_fn pid=3011870)[0m Epoch: 2, Steps: 67 | Train Loss: 0.4066503 Vali Loss: 0.2232439 Best vali loss: 0.2189062
[36m(_train_fn pid=3011870)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3011868)[0m Validation loss decreased (0.2258 --> 0.2192).  Saving model state dict ...

Trial trial-d7df6_00003 completed after 4 iterations at 2024-08-26 14:15:17. Total running time: 5s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.66402 â”‚
â”‚ time_total_s                                 3.48765 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21891 â”‚
â”‚ train_loss                                   0.36927 â”‚
â”‚ valid_loss                                   0.23062 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3011870)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3011870)[0m Early stopping
[36m(_train_fn pid=3011868)[0m Validation loss decreased (0.2192 --> 0.2175).  Saving model state dict ...

Trial trial-d7df6_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00138 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00026 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3012868)[0m configuration[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=3012868)[0m {'batch_size': 128, 'learning_rate': 0.0002629910127638627, 'd_model': 64, 'd_core': 512, 'e_layers': 2, 'dropout': 0.0013794401768914425, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3012868)[0m Use GPU: cuda:0[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3011868)[0m Validation loss decreased (0.2175 --> 0.2169).  Saving model state dict ...
[36m(_train_fn pid=3011869)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3011869)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3012868)[0m start_epoch 0[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3012868)[0m max_epoch 8[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3012868)[0m Updating learning rate to 0.00025298151334540497[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3012868)[0m saving checkpoint...[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3012868)[0m Validation loss decreased (inf --> 0.2377).  Saving model state dict ...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3012868)[0m Epoch: 1 cost time: 0.5650126934051514[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=3012868)[0m Epoch: 1, Steps: 67 | Train Loss: 0.5078109 Vali Loss: 0.2376532 Best vali loss: 0.2376532[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=3011867)[0m 	iters: 100, epoch: 3 | loss: 0.4906641[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3011867)[0m 	speed: 0.0153s/iter; left time: 22.8323s[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3012868)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3011868)[0m Validation loss decreased (0.2164 --> 0.2164).  Saving model state dict ...[32m [repeated 8x across cluster][0m

Trial trial-d7df6_00004 completed after 8 iterations at 2024-08-26 14:15:23. Total running time: 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.40855 â”‚
â”‚ time_total_s                                 4.16253 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21973 â”‚
â”‚ train_loss                                   0.39332 â”‚
â”‚ valid_loss                                   0.21973 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d7df6_00001 completed after 8 iterations at 2024-08-26 14:15:23. Total running time: 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.03007 â”‚
â”‚ time_total_s                                 9.16447 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21639 â”‚
â”‚ train_loss                                     0.396 â”‚
â”‚ valid_loss                                   0.21639 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3012868)[0m train 8449
[36m(_train_fn pid=3012868)[0m val 2785

Trial trial-d7df6_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00889 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00409 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

2024-08-26 14:15:25,568	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:25,634	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3014266)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1001/trial-d7df6_00005_5_alpha_d_ff=4,batch_size=64,d_core=256,d_model=64,dropout=0.0089,e_layers=2,learning_rate=0.0041,lradj=cosine_2024-08-26_14-15-23/checkpoint_000000)[32m [repeated 16x across cluster][0m
2024-08-26 14:15:26,716	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:27,160	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:27,408	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:27,501	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:28,240	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:28,474	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:28,630	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:29,000	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:29,751	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:29,813	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:30,483	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
Trial trial-d7df6_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                             0.0113 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00023 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3014266)[0m configuration
[36m(_train_fn pid=3014266)[0m {'batch_size': 64, 'learning_rate': 0.004089144136024073, 'd_model': 64, 'd_core': 256, 'e_layers': 2, 'dropout': 0.008890321934297422, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=3014266)[0m Use GPU: cuda:0
[36m(_train_fn pid=3014266)[0m train 8449
[36m(_train_fn pid=3014266)[0m val 2785
[36m(_train_fn pid=3014266)[0m start_epoch 0
[36m(_train_fn pid=3014266)[0m max_epoch 8
[36m(_train_fn pid=3011867)[0m Updating learning rate to 9.259748514523653e-05[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3011867)[0m saving checkpoint...[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3011867)[0m Epoch: 5 cost time: 1.1642224788665771[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3011867)[0m Epoch: 5, Steps: 265 | Train Loss: 0.3484706 Vali Loss: 0.2198034 Best vali loss: 0.2198034[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3014268)[0m 	iters: 300, epoch: 1 | loss: 0.4375552[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3014268)[0m 	speed: 0.0021s/iter; left time: 17.0250s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3014266)[0m Validation loss decreased (inf --> 0.2325).  Saving model state dict ...

Trial trial-d7df6_00002 completed after 5 iterations at 2024-08-26 14:15:27. Total running time: 14s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.83812 â”‚
â”‚ time_total_s                                12.99459 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21943 â”‚
â”‚ train_loss                                   0.33222 â”‚
â”‚ valid_loss                                    0.2269 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3011869)[0m Early stopping
[36m(_train_fn pid=3011869)[0m EarlyStopping counter: 3 out of 3[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3014266)[0m Validation loss decreased (0.2325 --> 0.2256).  Saving model state dict ...[32m [repeated 5x across cluster][0m

Trial trial-d7df6_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00211 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00028 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d7df6_00005 completed after 5 iterations at 2024-08-26 14:15:29. Total running time: 16s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             0.74922 â”‚
â”‚ time_total_s                                  4.6034 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22557 â”‚
â”‚ train_loss                                   0.36586 â”‚
â”‚ valid_loss                                   0.23168 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d7df6_00000 completed after 8 iterations at 2024-08-26 14:15:29. Total running time: 16s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.33585 â”‚
â”‚ time_total_s                                 15.3893 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                               0.2198 â”‚
â”‚ train_loss                                   0.32875 â”‚
â”‚ valid_loss                                    0.2221 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3015009)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015009)[0m {'batch_size': 32, 'learning_rate': 0.0002845702828409408, 'd_model': 64, 'd_core': 64, 'e_layers': 1, 'dropout': 0.0021103748139641673, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015009)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015009)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015009)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015009)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015009)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015009)[0m Updating learning rate to 0.0002737394713593169[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=3015009)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=3015009)[0m Epoch: 1 cost time: 0.8951575756072998[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=3015009)[0m Epoch: 1, Steps: 265 | Train Loss: 0.4688358 Vali Loss: 0.2248639 Best vali loss: 0.2248639[32m [repeated 11x across cluster][0m

Trial trial-d7df6_00008 started with configuration:
2024-08-26 14:15:31,398	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:31,625	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:32,294	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3015009)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1001/trial-d7df6_00007_7_alpha_d_ff=2,batch_size=32,d_core=64,d_model=64,dropout=0.0021,e_layers=1,learning_rate=0.0003,lradj=cosine_2024-08-26_14-15-27/checkpoint_000002)[32m [repeated 13x across cluster][0m
2024-08-26 14:15:33,526	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:34,465	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:34,795	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:36,238	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:37,140	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:39,924	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3014268)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1001/trial-d7df6_00006_6_alpha_d_ff=3,batch_size=8,d_core=32,d_model=64,dropout=0.0113,e_layers=1,learning_rate=0.0002,lradj=cosine_2024-08-26_14-15-23/checkpoint_000004)[32m [repeated 6x across cluster][0m
2024-08-26 14:15:40,195	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:40,217	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:42,582	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00559 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00007 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d7df6_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                             0.0032 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00225 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3015009)[0m 	iters: 200, epoch: 2 | loss: 0.4282202[32m [repeated 31x across cluster][0m
[36m(_train_fn pid=3015009)[0m 	speed: 0.0025s/iter; left time: 4.2099s[32m [repeated 31x across cluster][0m
[36m(_train_fn pid=3015009)[0m Validation loss decreased (inf --> 0.2249).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3011867)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015009)[0m EarlyStopping counter: 1 out of 3[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3014268)[0m Validation loss decreased (0.2205 --> 0.2174).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-d7df6_00007 completed after 5 iterations at 2024-08-26 14:15:34. Total running time: 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.26736 â”‚
â”‚ time_total_s                                 5.79546 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21799 â”‚
â”‚ train_loss                                   0.38807 â”‚
â”‚ valid_loss                                   0.21966 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3015526)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015526)[0m {'batch_size': 8, 'learning_rate': 0.0022520475222203326, 'd_model': 64, 'd_core': 64, 'e_layers': 4, 'dropout': 0.003195563182597908, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015526)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015526)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015526)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015526)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015526)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015009)[0m Updating learning rate to 8.783497512713259e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3015009)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3015009)[0m Epoch: 5 cost time: 0.9260287284851074[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3015009)[0m Epoch: 5, Steps: 265 | Train Loss: 0.3880741 Vali Loss: 0.2196607 Best vali loss: 0.2179929[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3014268)[0m 	iters: 1000, epoch: 4 | loss: 0.4211382[32m [repeated 37x across cluster][0m
[36m(_train_fn pid=3014268)[0m 	speed: 0.0020s/iter; left time: 8.6674s[32m [repeated 37x across cluster][0m
[36m(_train_fn pid=3015524)[0m Validation loss decreased (inf --> 0.2517).  Saving model state dict ...
[36m(_train_fn pid=3015009)[0m Early stopping
[36m(_train_fn pid=3015009)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3014268)[0m Validation loss decreased (0.2169 --> 0.2163).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015526)[0m Validation loss decreased (inf --> 0.2425).  Saving model state dict ...
[36m(_train_fn pid=3015526)[0m Updating learning rate to 0.0021663340671212276[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3015526)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3015526)[0m Epoch: 1 cost time: 7.558406591415405[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3015526)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.4605659 Vali Loss: 0.2425349 Best vali loss: 0.2425349[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3014268)[0m 	iters: 900, epoch: 6 | loss: 0.7817447[32m [repeated 32x across cluster][0m
[36m(_train_fn pid=3014268)[0m 	speed: 0.0020s/iter; left time: 4.6025s[32m [repeated 32x across cluster][0m
[36m(_train_fn pid=3014268)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m

Trial status: 7 TERMINATED | 3 RUNNING
Current time: 2024-08-26 14:15:42. Total running time: 30s
Logical resource usage: 3.0/32 CPUs, 0.75/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: d7df6_00006 with best_valid_loss=0.2162925846165772 and params={'batch_size': 8, 'learning_rate': 0.0002275373366155108, 'd_model': 64, 'd_core': 32, 'e_layers': 1, 'dropout': 0.011299430142127418, 'lradj': 'cosine', 'd_ff': 192}
2024-08-26 14:15:44,132	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:45,305	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3014268)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1001/trial-d7df6_00006_6_alpha_d_ff=3,batch_size=8,d_core=32,d_model=64,dropout=0.0113,e_layers=1,learning_rate=0.0002,lradj=cosine_2024-08-26_14-15-23/checkpoint_000006)[32m [repeated 5x across cluster][0m
2024-08-26 14:15:47,774	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:47,906	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:51,870	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3015524)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1001/trial-d7df6_00008_8_alpha_d_ff=1,batch_size=16,d_core=64,d_model=32,dropout=0.0056,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-15-29/checkpoint_000004)[32m [repeated 3x across cluster][0m
2024-08-26 14:15:55,429	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:15:55,720	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-d7df6_00006   RUNNING                 8       0.000227537          64              3         32            1   0.0112994     cosine         6           17.408         0.367018       0.21945             0.216293 â”‚
â”‚ trial-d7df6_00008   RUNNING                16       7.46251e-05          32              1         64            4   0.00559245    cosine         2            8.90351       0.461034       0.238372            0.238372 â”‚
â”‚ trial-d7df6_00009   RUNNING                 8       0.00225205           64              2         64            4   0.00319556    cosine         1            8.87755       0.460566       0.242535            0.242535 â”‚
â”‚ trial-d7df6_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         8           15.3893        0.328749       0.2221              0.219803 â”‚
â”‚ trial-d7df6_00001   TERMINATED            128       5.37491e-05         256              3         32            4   0.000983346   type1          8            9.16447       0.396005       0.216392            0.216392 â”‚
â”‚ trial-d7df6_00002   TERMINATED             32       0.000910274          64              1         32            3   0.00130339    cosine         5           12.9946        0.33222        0.226898            0.219428 â”‚
â”‚ trial-d7df6_00003   TERMINATED            128       0.00464733           32              2        256            2   0.00302068    cosine         4            3.48765       0.369269       0.230616            0.218906 â”‚
â”‚ trial-d7df6_00004   TERMINATED            128       0.000262991          64              2        512            2   0.00137944    cosine         8            4.16253       0.393325       0.219728            0.219728 â”‚
â”‚ trial-d7df6_00005   TERMINATED             64       0.00408914           64              4        256            2   0.00889032    cosine         5            4.6034        0.365865       0.231684            0.225572 â”‚
â”‚ trial-d7df6_00007   TERMINATED             32       0.00028457           64              2         64            1   0.00211037    cosine         5            5.79546       0.388074       0.219661            0.217993 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3015524)[0m Validation loss decreased (0.2517 --> 0.2384).  Saving model state dict ...
[36m(_train_fn pid=3015524)[0m Validation loss decreased (0.2384 --> 0.2297).  Saving model state dict ...

Trial trial-d7df6_00006 completed after 7 iterations at 2024-08-26 14:15:45. Total running time: 32s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             2.72078 â”‚
â”‚ time_total_s                                20.12878 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.21629 â”‚
â”‚ train_loss                                   0.36081 â”‚
â”‚ valid_loss                                   0.21961 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3014268)[0m Early stopping
[36m(_train_fn pid=3014268)[0m Updating learning rate to 8.660124217154697e-06[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3014268)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3014268)[0m Epoch: 7 cost time: 2.2803642749786377[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3014268)[0m Epoch: 7, Steps: 1057 | Train Loss: 0.3608131 Vali Loss: 0.2196094 Best vali loss: 0.2162926[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3015524)[0m 	iters: 500, epoch: 4 | loss: 0.4738921[32m [repeated 27x across cluster][0m
[36m(_train_fn pid=3015524)[0m 	speed: 0.0057s/iter; left time: 12.2512s[32m [repeated 27x across cluster][0m
[36m(_train_fn pid=3014268)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3015524)[0m Validation loss decreased (0.2297 --> 0.2258).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015524)[0m Updating learning rate to 3.731253491911727e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015524)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015524)[0m Epoch: 4 cost time: 3.301149606704712[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015524)[0m Epoch: 4, Steps: 529 | Train Loss: 0.4342366 Vali Loss: 0.2257975 Best vali loss: 0.2257975[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015524)[0m 	iters: 100, epoch: 6 | loss: 0.3046001[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3015524)[0m 	speed: 0.0138s/iter; left time: 20.6036s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3015524)[0m Validation loss decreased (0.2258 --> 0.2239).  Saving model state dict ...
[36m(_train_fn pid=3015526)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3015524)[0m Validation loss decreased (0.2239 --> 0.2231).  Saving model state dict ...
[36m(_train_fn pid=3015524)[0m Updating learning rate to 1.0928588454549604e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3015524)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3015524)[0m Epoch: 6 cost time: 3.3696439266204834[32m [repeated 3x across cluster][0m
2024-08-26 14:15:59,703	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3015524)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1001/trial-d7df6_00008_8_alpha_d_ff=1,batch_size=16,d_core=64,d_model=32,dropout=0.0056,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-15-29/checkpoint_000006)[32m [repeated 3x across cluster][0m
2024-08-26 14:16:03,118	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:03,511	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:08,767	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1001' in 0.0089s.
[36m(_train_fn pid=3015524)[0m Epoch: 6, Steps: 529 | Train Loss: 0.4219651 Vali Loss: 0.2230697 Best vali loss: 0.2230697[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3015524)[0m 	iters: 300, epoch: 7 | loss: 0.2740989[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3015524)[0m 	speed: 0.0065s/iter; left time: 4.9275s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3015524)[0m Validation loss decreased (0.2231 --> 0.2226).  Saving model state dict ...
[36m(_train_fn pid=3015524)[0m Updating learning rate to 2.8402476012321447e-06
[36m(_train_fn pid=3015524)[0m saving checkpoint...
[36m(_train_fn pid=3015524)[0m Epoch: 7 cost time: 3.477102518081665
[36m(_train_fn pid=3015524)[0m Epoch: 7, Steps: 529 | Train Loss: 0.4198747 Vali Loss: 0.2226364 Best vali loss: 0.2226364
[36m(_train_fn pid=3015524)[0m 	iters: 500, epoch: 8 | loss: 0.2189748[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3015524)[0m 	speed: 0.0056s/iter; left time: 0.1668s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3015526)[0m Updating learning rate to 0.0011260237611101663
[36m(_train_fn pid=3015526)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3015526)[0m saving checkpoint...
[36m(_train_fn pid=3015526)[0m Epoch: 4 cost time: 6.7407824993133545
[36m(_train_fn pid=3015526)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3945029 Vali Loss: 0.2268388 Best vali loss: 0.2266510

Trial trial-d7df6_00008 completed after 8 iterations at 2024-08-26 14:16:03. Total running time: 50s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             3.80593 â”‚
â”‚ time_total_s                                32.20452 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22257 â”‚
â”‚ train_loss                                   0.41899 â”‚
â”‚ valid_loss                                   0.22257 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3015524)[0m Validation loss decreased (0.2226 --> 0.2226).  Saving model state dict ...

Trial trial-d7df6_00009 completed after 5 iterations at 2024-08-26 14:16:08. Total running time: 55s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d7df6_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.63699 â”‚
â”‚ time_total_s                                37.40691 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22665 â”‚
â”‚ train_loss                                   0.35987 â”‚
â”‚ valid_loss                                   0.23264 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:16:08. Total running time: 55s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: d7df6_00006 with best_valid_loss=0.2162925846165772 and params={'batch_size': 8, 'learning_rate': 0.0002275373366155108, 'd_model': 64, 'd_core': 32, 'e_layers': 1, 'dropout': 0.011299430142127418, 'lradj': 'cosine', 'd_ff': 192}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-d7df6_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         8           15.3893        0.328749       0.2221              0.219803 â”‚
â”‚ trial-d7df6_00001   TERMINATED            128       5.37491e-05         256              3         32            4   0.000983346   type1          8            9.16447       0.396005       0.216392            0.216392 â”‚
â”‚ trial-d7df6_00002   TERMINATED             32       0.000910274          64              1         32            3   0.00130339    cosine         5           12.9946        0.33222        0.226898            0.219428 â”‚
â”‚ trial-d7df6_00003   TERMINATED            128       0.00464733           32              2        256            2   0.00302068    cosine         4            3.48765       0.369269       0.230616            0.218906 â”‚
â”‚ trial-d7df6_00004   TERMINATED            128       0.000262991          64              2        512            2   0.00137944    cosine         8            4.16253       0.393325       0.219728            0.219728 â”‚
â”‚ trial-d7df6_00005   TERMINATED             64       0.00408914           64              4        256            2   0.00889032    cosine         5            4.6034        0.365865       0.231684            0.225572 â”‚
â”‚ trial-d7df6_00006   TERMINATED              8       0.000227537          64              3         32            1   0.0112994     cosine         7           20.1288        0.360813       0.219609            0.216293 â”‚
â”‚ trial-d7df6_00007   TERMINATED             32       0.00028457           64              2         64            1   0.00211037    cosine         5            5.79546       0.388074       0.219661            0.217993 â”‚
â”‚ trial-d7df6_00008   TERMINATED             16       7.46251e-05          32              1         64            4   0.00559245    cosine         8           32.2045        0.418991       0.222572            0.222572 â”‚
â”‚ trial-d7df6_00009   TERMINATED              8       0.00225205           64              2         64            4   0.00319556    cosine         5           37.4069        0.359866       0.232643            0.226651 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 0.0002275373366155108, 'd_model': 64, 'd_core': 32, 'e_layers': 1, 'dropout': 0.011299430142127418, 'lradj': 'cosine', 'd_ff': 192}
[36m(_train_fn pid=3015526)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1001/trial-d7df6_00009_9_alpha_d_ff=2,batch_size=8,d_core=64,d_model=64,dropout=0.0032,e_layers=4,learning_rate=0.0023,lradj=cosine_2024-08-26_14-15-29/checkpoint_000004)[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3015526)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3015526)[0m Early stopping
[36m(_train_fn pid=3015526)[0m 	iters: 1000, epoch: 5 | loss: 0.3954693[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3015526)[0m 	speed: 0.0047s/iter; left time: 15.3200s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3015526)[0m Updating learning rate to 0.00069511312328388[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015526)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015526)[0m Epoch: 5 cost time: 5.01434326171875[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3015526)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.3598657 Vali Loss: 0.2326429 Best vali loss: 0.2266510[32m [repeated 2x across cluster][0m


Time taken (4 parallel trials): 61 seconds


2024-08-26 14:16:12,914	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:16:13,287	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:16:13,292	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:16:13,299	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:16:17,290	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3019752)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002/trial-fbf36_00001_1_alpha_d_ff=4,batch_size=64,d_core=256,d_model=64,dropout=0.0020,e_layers=3,learning_rate=0.0003,lradj=cosine_2024-08-26_14-16-13/checkpoint_000000)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed1002   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator       â”‚
â”‚ Scheduler                        FIFOScheduler               â”‚
â”‚ Number of trials                 10                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-16-12_479977_3017529/artifacts/2024-08-26_14-16-13/ETTh2_96_96_test_seed1002/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:16:13. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-fbf36_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-fbf36_00001   PENDING              64       0.000259029          64              4        256            3   0.00199337   cosine  â”‚
â”‚ trial-fbf36_00002   PENDING               8       0.000118415         128              4         32            4   0.00238069   type1   â”‚
â”‚ trial-fbf36_00003   PENDING              32       0.000248197         512              4         64            3   0.00172192   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-fbf36_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-fbf36_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00238 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00012 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-fbf36_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00199 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00026 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3019753)[0m configuration
[36m(_train_fn pid=3019753)[0m {'batch_size': 8, 'learning_rate': 0.00011841502408887222, 'd_model': 128, 'd_core': 32, 'e_layers': 4, 'dropout': 0.002380685378266998, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=3019753)[0m Use GPU: cuda:0
[36m(_train_fn pid=3019751)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}

Trial trial-fbf36_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00172 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00025 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3019753)[0m train 8449
[36m(_train_fn pid=3019753)[0m val 2785
[36m(_train_fn pid=3019753)[0m start_epoch 0
[36m(_train_fn pid=3019753)[0m max_epoch 8
[36m(_train_fn pid=3019751)[0m 	iters: 100, epoch: 1 | loss: 0.4586933
[36m(_train_fn pid=3019751)[0m 	speed: 0.0133s/iter; left time: 26.8858s
[36m(_train_fn pid=3019752)[0m Updating learning rate to 0.0002491704625279662
[36m(_train_fn pid=3019752)[0m saving checkpoint...
[36m(_train_fn pid=3019752)[0m Validation loss decreased (inf --> 0.2252).  Saving model state dict ...
[36m(_train_fn pid=3019752)[0m Epoch: 1 cost time: 1.7301735877990723
2024-08-26 14:16:18,188	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:18,954	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:19,128	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:20,595	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:20,778	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:22,243	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:22,606	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3019754)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002/trial-fbf36_00003_3_alpha_d_ff=4,batch_size=32,d_core=64,d_model=512,dropout=0.0017,e_layers=3,learning_rate=0.0002,lradj=type1_2024-08-26_14-16-13/checkpoint_000001)[32m [repeated 7x across cluster][0m
2024-08-26 14:16:23,446	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:23,767	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:25,633	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:25,759	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:26,863	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:27,741	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3021009)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002/trial-fbf36_00004_4_alpha_d_ff=2,batch_size=64,d_core=512,d_model=32,dropout=0.0012,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-16-23/checkpoint_000001)[32m [repeated 6x across cluster][0m
2024-08-26 14:16:28,145	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:28,405	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:28,608	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:29,464	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3019752)[0m Epoch: 1, Steps: 133 | Train Loss: 0.4624317 Vali Loss: 0.2251669 Best vali loss: 0.2251669
[36m(_train_fn pid=3019752)[0m Validation loss decreased (0.2252 --> 0.2200).  Saving model state dict ...
[36m(_train_fn pid=3019754)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=3019754)[0m {'batch_size': 32, 'learning_rate': 0.00024819693139059096, 'd_model': 512, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0017219235090349335, 'lradj': 'type1', 'd_ff': 2048}
[36m(_train_fn pid=3019754)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3019752)[0m {'batch_size': 64, 'learning_rate': 0.0002590291734147386, 'd_model': 64, 'd_core': 256, 'e_layers': 3, 'dropout': 0.0019933659198190187, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=3019754)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3019754)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3019754)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3019754)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3019752)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3019754)[0m 	iters: 200, epoch: 2 | loss: 0.1881160[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3019754)[0m 	speed: 0.0107s/iter; left time: 17.7301s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3019752)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3019752)[0m Updating learning rate to 0.0001295145867073693[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3019752)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3019754)[0m Validation loss decreased (inf --> 0.2241).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3019752)[0m Epoch: 4 cost time: 1.4514636993408203[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3019752)[0m Epoch: 4, Steps: 133 | Train Loss: 0.3939872 Vali Loss: 0.2220857 Best vali loss: 0.2199689[32m [repeated 6x across cluster][0m

Trial trial-fbf36_00001 completed after 5 iterations at 2024-08-26 14:16:23. Total running time: 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                               1.521 â”‚
â”‚ time_total_s                                 8.85648 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21997 â”‚
â”‚ train_loss                                    0.3812 â”‚
â”‚ valid_loss                                   0.22287 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3019752)[0m Early stopping
[36m(_train_fn pid=3019754)[0m Validation loss decreased (0.2241 --> 0.2166).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-fbf36_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00122 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00005 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3021009)[0m {'batch_size': 64, 'learning_rate': 5.065485841998547e-05, 'd_model': 32, 'd_core': 512, 'e_layers': 4, 'dropout': 0.001215288141857975, 'lradj': 'cosine', 'd_ff': 64}
[36m(_train_fn pid=3021009)[0m configuration
[36m(_train_fn pid=3021009)[0m Use GPU: cuda:0
[36m(_train_fn pid=3021009)[0m train 8449
[36m(_train_fn pid=3021009)[0m val 2785
[36m(_train_fn pid=3019754)[0m EarlyStopping counter: 1 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3021009)[0m start_epoch 0
[36m(_train_fn pid=3021009)[0m max_epoch 8
[36m(_train_fn pid=3019751)[0m 	iters: 100, epoch: 5 | loss: 0.3762130[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3019751)[0m 	speed: 0.0154s/iter; left time: 14.8296s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3021009)[0m Updating learning rate to 4.323662615440084e-05[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3021009)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3021009)[0m Validation loss decreased (inf --> 0.2867).  Saving model state dict ...
[36m(_train_fn pid=3021009)[0m Epoch: 2 cost time: 0.7289154529571533[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3021009)[0m Epoch: 2, Steps: 133 | Train Loss: 0.5271283 Vali Loss: 0.2688708 Best vali loss: 0.2688708[32m [repeated 7x across cluster][0m

Trial trial-fbf36_00000 completed after 5 iterations at 2024-08-26 14:16:28. Total running time: 14s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             2.38451 â”‚
â”‚ time_total_s                                13.28163 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21527 â”‚
â”‚ train_loss                                   0.35237 â”‚
â”‚ valid_loss                                   0.22065 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3019751)[0m Early stopping
[36m(_train_fn pid=3021009)[0m Validation loss decreased (0.2689 --> 0.2581).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-fbf36_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00098 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00164 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
2024-08-26 14:16:30,337	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:30,760	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:31,195	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:31,391	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:32,063	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:33,029	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3021009)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002/trial-fbf36_00004_4_alpha_d_ff=2,batch_size=64,d_core=512,d_model=32,dropout=0.0012,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-16-23/checkpoint_000007)[32m [repeated 10x across cluster][0m
2024-08-26 14:16:35,267	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:38,973	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3021937)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002/trial-fbf36_00006_6_alpha_d_ff=4,batch_size=16,d_core=64,d_model=128,dropout=0.0014,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-16-30/checkpoint_000000)[32m [repeated 2x across cluster][0m
2024-08-26 14:16:39,027	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:41,814	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:42,740	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:42,788	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3021502)[0m configuration
[36m(_train_fn pid=3021502)[0m Use GPU: cuda:0

Trial trial-fbf36_00003 completed after 5 iterations at 2024-08-26 14:16:30. Total running time: 17s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             2.35206 â”‚
â”‚ time_total_s                                 15.7993 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21663 â”‚
â”‚ train_loss                                   0.28834 â”‚
â”‚ valid_loss                                   0.22678 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3019754)[0m Early stopping
[36m(_train_fn pid=3021502)[0m {'batch_size': 16, 'learning_rate': 0.001635766214133003, 'd_model': 64, 'd_core': 64, 'e_layers': 4, 'dropout': 0.000980048760838007, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=3021502)[0m train 8449
[36m(_train_fn pid=3021502)[0m val 2785
[36m(_train_fn pid=3019754)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3021502)[0m start_epoch 0
[36m(_train_fn pid=3021502)[0m max_epoch 8
[36m(_train_fn pid=3019753)[0m Validation loss decreased (inf --> 0.2204).  Saving model state dict ...
[36m(_train_fn pid=3021009)[0m 	iters: 100, epoch: 7 | loss: 0.5097677[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3021009)[0m 	speed: 0.0087s/iter; left time: 1.4611s[32m [repeated 16x across cluster][0m

Trial trial-fbf36_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00142 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3021937)[0m {'batch_size': 16, 'learning_rate': 5.5781240301010434e-05, 'd_model': 128, 'd_core': 64, 'e_layers': 4, 'dropout': 0.0014185659686031407, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=3021937)[0m train 8449
[36m(_train_fn pid=3021937)[0m val 2785
[36m(_train_fn pid=3021937)[0m start_epoch 0
[36m(_train_fn pid=3021937)[0m max_epoch 8
[36m(_train_fn pid=3021009)[0m Updating learning rate to 0.0[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3021009)[0m saving checkpoint...[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3021009)[0m Epoch: 7 cost time: 0.7244594097137451[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3021009)[0m Epoch: 7, Steps: 133 | Train Loss: 0.4704424 Vali Loss: 0.2466159 Best vali loss: 0.2466159[32m [repeated 9x across cluster][0m

Trial trial-fbf36_00004 completed after 8 iterations at 2024-08-26 14:16:33. Total running time: 19s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.96336 â”‚
â”‚ time_total_s                                 7.71778 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.24641 â”‚
â”‚ train_loss                                   0.46658 â”‚
â”‚ valid_loss                                   0.24641 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-fbf36_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00411 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00357 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3021009)[0m Validation loss decreased (0.2466 --> 0.2464).  Saving model state dict ...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3022273)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3022273)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3021502)[0m Validation loss decreased (inf --> 0.2394).  Saving model state dict ...
[36m(_train_fn pid=3022273)[0m 	iters: 300, epoch: 1 | loss: 0.3599075[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=3022273)[0m 	speed: 0.0057s/iter; left time: 22.6048s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=3022273)[0m {'batch_size': 16, 'learning_rate': 0.0035699398526018626, 'd_model': 256, 'd_core': 64, 'e_layers': 1, 'dropout': 0.004112532622702396, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=3022273)[0m train 8449
[36m(_train_fn pid=3022273)[0m val 2785
[36m(_train_fn pid=3022273)[0m start_epoch 0
[36m(_train_fn pid=3022273)[0m max_epoch 8
[36m(_train_fn pid=3021502)[0m Updating learning rate to 0.0015735085696719797
[36m(_train_fn pid=3021502)[0m saving checkpoint...
[36m(_train_fn pid=3021502)[0m Epoch: 1 cost time: 4.3084876537323[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3021502)[0m Epoch: 1, Steps: 529 | Train Loss: 0.4446497 Vali Loss: 0.2394262 Best vali loss: 0.2394262[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3021937)[0m Updating learning rate to 5.365819325660385e-05
[36m(_train_fn pid=3021937)[0m saving checkpoint...
[36m(_train_fn pid=3021937)[0m Validation loss decreased (inf --> 0.2235).  Saving model state dict ...
[36m(_train_fn pid=3021502)[0m Validation loss decreased (0.2394 --> 0.2232).  Saving model state dict ...
[36m(_train_fn pid=3021937)[0m 	iters: 300, epoch: 2 | loss: 0.1402895[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=3021937)[0m 	speed: 0.0092s/iter; left time: 31.4008s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=3019753)[0m Epoch: 2 cost time: 9.769496440887451[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3019753)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.4002390 Vali Loss: 0.2186813 Best vali loss: 0.2186813[32m [repeated 5x across cluster][0m

Trial status: 4 TERMINATED | 4 RUNNING
2024-08-26 14:16:45,370	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3021937)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002/trial-fbf36_00006_6_alpha_d_ff=4,batch_size=16,d_core=64,d_model=128,dropout=0.0014,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-16-30/checkpoint_000001)[32m [repeated 5x across cluster][0m
2024-08-26 14:16:46,629	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:48,097	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:50,539	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3022273)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002/trial-fbf36_00007_7_alpha_d_ff=2,batch_size=16,d_core=64,d_model=256,dropout=0.0041,e_layers=1,learning_rate=0.0036,lradj=cosine_2024-08-26_14-16-33/checkpoint_000003)[32m [repeated 3x across cluster][0m
2024-08-26 14:16:51,689	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:54,368	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:54,417	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:55,228	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:16:57,849	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3021937)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002/trial-fbf36_00006_6_alpha_d_ff=4,batch_size=16,d_core=64,d_model=128,dropout=0.0014,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-16-30/checkpoint_000003)[32m [repeated 5x across cluster][0m
2024-08-26 14:16:58,101	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
Current time: 2024-08-26 14:16:43. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: fbf36_00000 with best_valid_loss=0.21527297793213726 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-fbf36_00002   RUNNING                 8       0.000118415         128              4         32            4   0.00238069    type1          2           27.9106        0.400239       0.218681            0.218681 â”‚
â”‚ trial-fbf36_00005   RUNNING                16       0.00163577           64              2         64            4   0.000980049   cosine         2           12.1141        0.42359        0.223224            0.223224 â”‚
â”‚ trial-fbf36_00006   RUNNING                16       5.57812e-05         128              4         64            4   0.00141857    cosine         1            6.69184       0.457526       0.223502            0.223502 â”‚
â”‚ trial-fbf36_00007   RUNNING                16       0.00356994          256              2         64            1   0.00411253    cosine         2            8.18394       0.441806       0.232684            0.232684 â”‚
â”‚ trial-fbf36_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5           13.2816        0.352368       0.220653            0.215273 â”‚
â”‚ trial-fbf36_00001   TERMINATED             64       0.000259029          64              4        256            3   0.00199337    cosine         5            8.85648       0.381198       0.222871            0.219969 â”‚
â”‚ trial-fbf36_00003   TERMINATED             32       0.000248197         512              4         64            3   0.00172192    type1          5           15.7993        0.288344       0.226781            0.216627 â”‚
â”‚ trial-fbf36_00004   TERMINATED             64       5.06549e-05          32              2        512            4   0.00121529    cosine         8            7.71778       0.466575       0.246413            0.246413 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3022273)[0m Updating learning rate to 0.003047134265402372[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3022273)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3022273)[0m Validation loss decreased (inf --> 0.2724).  Saving model state dict ...
[36m(_train_fn pid=3022273)[0m Validation loss decreased (0.2327 --> 0.2227).  Saving model state dict ...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3022273)[0m 	iters: 100, epoch: 4 | loss: 0.7388631[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=3022273)[0m 	speed: 0.0168s/iter; left time: 42.6961s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=3021502)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3021502)[0m Epoch: 3 cost time: 5.346682548522949[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3021502)[0m Epoch: 3, Steps: 529 | Train Loss: 0.3920618 Vali Loss: 0.2241110 Best vali loss: 0.2232241[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3021502)[0m Updating learning rate to 0.0011308734217521346[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3021502)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3021937)[0m Validation loss decreased (0.2199 --> 0.2174).  Saving model state dict ...
[36m(_train_fn pid=3021502)[0m 	iters: 400, epoch: 4 | loss: 0.1373855[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=3021502)[0m 	speed: 0.0104s/iter; left time: 23.4642s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=3022273)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3021937)[0m Epoch: 3 cost time: 5.419494152069092[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3021937)[0m Epoch: 3, Steps: 529 | Train Loss: 0.4005375 Vali Loss: 0.2173966 Best vali loss: 0.2173966[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3022273)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3021502)[0m Updating learning rate to 0.0008178831070665015[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3021502)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3019753)[0m 	iters: 200, epoch: 4 | loss: 0.3297748[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=3019753)[0m 	speed: 0.0102s/iter; left time: 51.9992s[32m [repeated 19x across cluster][0m

Trial trial-fbf36_00007 completed after 6 iterations at 2024-08-26 14:16:58. Total running time: 44s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             3.72988 â”‚
â”‚ time_total_s                                23.53351 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22266 â”‚
â”‚ train_loss                                   0.34773 â”‚
â”‚ valid_loss                                   0.23513 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3022273)[0m Early stopping
[36m(_train_fn pid=3022273)[0m Epoch: 6 cost time: 2.95819091796875[32m [repeated 5x across cluster][0m
2024-08-26 14:17:00,366	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:02,530	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:03,198	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3023605)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002/trial-fbf36_00008_8_alpha_d_ff=4,batch_size=16,d_core=256,d_model=32,dropout=0.0020,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_14-16-58/checkpoint_000000)[32m [repeated 4x across cluster][0m
2024-08-26 14:17:04,609	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:05,559	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:06,056	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:06,648	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:07,951	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:08,551	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3023762)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002/trial-fbf36_00009_9_alpha_d_ff=3,batch_size=32,d_core=512,d_model=128,dropout=0.0028,e_layers=3,learning_rate=0.0004,lradj=type1_2024-08-26_14-17-00/checkpoint_000002)[32m [repeated 6x across cluster][0m
2024-08-26 14:17:08,909	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:10,206	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:11,792	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:11,839	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3022273)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3477340 Vali Loss: 0.2351254 Best vali loss: 0.2226596[32m [repeated 5x across cluster][0m

Trial trial-fbf36_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00205 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                       0.0002 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3022273)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3023605)[0m configuration
[36m(_train_fn pid=3023605)[0m {'batch_size': 16, 'learning_rate': 0.00020058235572555747, 'd_model': 32, 'd_core': 256, 'e_layers': 4, 'dropout': 0.0020493285022807196, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=3023605)[0m Use GPU: cuda:0
[36m(_train_fn pid=3023605)[0m train 8449
[36m(_train_fn pid=3023605)[0m val 2785
[36m(_train_fn pid=3022273)[0m Updating learning rate to 0.000522805587199491[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3022273)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3023605)[0m start_epoch 0
[36m(_train_fn pid=3023605)[0m max_epoch 8

Trial trial-fbf36_00005 completed after 5 iterations at 2024-08-26 14:17:00. Total running time: 47s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.94672 â”‚
â”‚ time_total_s                                 30.6577 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22322 â”‚
â”‚ train_loss                                   0.32788 â”‚
â”‚ valid_loss                                   0.23544 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-fbf36_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                             0.0028 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00043 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3023762)[0m {'batch_size': 32, 'learning_rate': 0.0004300984913149099, 'd_model': 128, 'd_core': 512, 'e_layers': 3, 'dropout': 0.002799666715724795, 'lradj': 'type1', 'd_ff': 384}
[36m(_train_fn pid=3023605)[0m 	iters: 400, epoch: 1 | loss: 0.3264134[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=3023605)[0m 	speed: 0.0046s/iter; left time: 17.5640s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=3023605)[0m Validation loss decreased (inf --> 0.2289).  Saving model state dict ...
[36m(_train_fn pid=3021502)[0m Early stopping
[36m(_train_fn pid=3023605)[0m Epoch: 1 cost time: 2.736790418624878[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3023605)[0m Epoch: 1, Steps: 529 | Train Loss: 0.4750365 Vali Loss: 0.2289116 Best vali loss: 0.2289116[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3021937)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3023762)[0m configuration
[36m(_train_fn pid=3023762)[0m Use GPU: cuda:0
[36m(_train_fn pid=3023762)[0m train 8449
[36m(_train_fn pid=3023762)[0m val 2785
[36m(_train_fn pid=3023762)[0m Updating learning rate to 0.0004300984913149099[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3023762)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3023762)[0m start_epoch 0
[36m(_train_fn pid=3023762)[0m max_epoch 8
[36m(_train_fn pid=3023605)[0m Validation loss decreased (0.2289 --> 0.2236).  Saving model state dict ...
[36m(_train_fn pid=3023762)[0m 	iters: 200, epoch: 3 | loss: 0.3367065[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=3023762)[0m 	speed: 0.0059s/iter; left time: 8.2245s[32m [repeated 24x across cluster][0m

Trial trial-fbf36_00006 completed after 6 iterations at 2024-08-26 14:17:07. Total running time: 54s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             5.41898 â”‚
â”‚ time_total_s                                35.65452 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                               0.2174 â”‚
â”‚ train_loss                                   0.37435 â”‚
â”‚ valid_loss                                   0.21948 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3021937)[0m Early stopping
[36m(_train_fn pid=3023762)[0m Validation loss decreased (inf --> 0.2237).  Saving model state dict ...
[36m(_train_fn pid=3023605)[0m Epoch: 3 cost time: 2.465744733810425[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3023605)[0m Epoch: 3, Steps: 529 | Train Loss: 0.4082867 Vali Loss: 0.2234968 Best vali loss: 0.2234968[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3023762)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3023762)[0m Updating learning rate to 5.3762311414363736e-05[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3023762)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3023605)[0m Validation loss decreased (0.2236 --> 0.2235).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-fbf36_00009 completed after 5 iterations at 2024-08-26 14:17:11. Total running time: 58s
2024-08-26 14:17:13,097	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:14,549	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3023605)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002/trial-fbf36_00008_8_alpha_d_ff=4,batch_size=16,d_core=256,d_model=32,dropout=0.0020,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_14-16-58/checkpoint_000004)[32m [repeated 6x across cluster][0m
2024-08-26 14:17:17,294	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.63008 â”‚
â”‚ time_total_s                                 9.94143 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22086 â”‚
â”‚ train_loss                                   0.32444 â”‚
â”‚ valid_loss                                   0.23079 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-fbf36_00002 completed after 5 iterations at 2024-08-26 14:17:13. Total running time: 59s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             7.53541 â”‚
â”‚ time_total_s                                58.21175 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21868 â”‚
â”‚ train_loss                                   0.35124 â”‚
â”‚ valid_loss                                   0.22285 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3023605)[0m 	iters: 200, epoch: 5 | loss: 0.2507069[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=3023605)[0m 	speed: 0.0044s/iter; left time: 8.4712s[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=3019753)[0m Early stopping[32m [repeated 2x across cluster][0m

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2024-08-26 14:17:13. Total running time: 1min 0s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: fbf36_00000 with best_valid_loss=0.21527297793213726 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-fbf36_00008   RUNNING                16       0.000200582          32              4        256            4   0.00204933    cosine         4           12.1457        0.395496       0.224868            0.223497 â”‚
â”‚ trial-fbf36_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5           13.2816        0.352368       0.220653            0.215273 â”‚
â”‚ trial-fbf36_00001   TERMINATED             64       0.000259029          64              4        256            3   0.00199337    cosine         5            8.85648       0.381198       0.222871            0.219969 â”‚
â”‚ trial-fbf36_00002   TERMINATED              8       0.000118415         128              4         32            4   0.00238069    type1          5           58.2118        0.35124        0.222851            0.218681 â”‚
â”‚ trial-fbf36_00003   TERMINATED             32       0.000248197         512              4         64            3   0.00172192    type1          5           15.7993        0.288344       0.226781            0.216627 â”‚
â”‚ trial-fbf36_00004   TERMINATED             64       5.06549e-05          32              2        512            4   0.00121529    cosine         8            7.71778       0.466575       0.246413            0.246413 â”‚
â”‚ trial-fbf36_00005   TERMINATED             16       0.00163577           64              2         64            4   0.000980049   cosine         5           30.6577        0.327883       0.23544             0.223224 â”‚
â”‚ trial-fbf36_00006   TERMINATED             16       5.57812e-05         128              4         64            4   0.00141857    cosine         6           35.6545        0.374352       0.219479            0.217397 â”‚
â”‚ trial-fbf36_00007   TERMINATED             16       0.00356994          256              2         64            1   0.00411253    cosine         6           23.5335        0.347734       0.235125            0.22266  â”‚
â”‚ trial-fbf36_00009   TERMINATED             32       0.000430098         128              3        512            3   0.00279967    type1          5            9.94143       0.32444        0.230795            0.220862 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3023605)[0m Epoch: 5 cost time: 2.4026734828948975[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3023605)[0m Epoch: 5, Steps: 529 | Train Loss: 0.3883529 Vali Loss: 0.2257694 Best vali loss: 0.2234968[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3023605)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3023605)[0m Updating learning rate to 6.191140568231287e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3023605)[0m saving checkpoint...[32m [repeated 4x across cluster][0m

Trial trial-fbf36_00008 completed after 6 iterations at 2024-08-26 14:17:17. Total running time: 1min 3s
2024-08-26 14:17:17,304	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002' in 0.0082s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-fbf36_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             2.74222 â”‚
â”‚ time_total_s                                17.64217 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                               0.2235 â”‚
â”‚ train_loss                                   0.38123 â”‚
â”‚ valid_loss                                   0.22352 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:17:17. Total running time: 1min 4s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: fbf36_00000 with best_valid_loss=0.21527297793213726 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-fbf36_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5           13.2816        0.352368       0.220653            0.215273 â”‚
â”‚ trial-fbf36_00001   TERMINATED             64       0.000259029          64              4        256            3   0.00199337    cosine         5            8.85648       0.381198       0.222871            0.219969 â”‚
â”‚ trial-fbf36_00002   TERMINATED              8       0.000118415         128              4         32            4   0.00238069    type1          5           58.2118        0.35124        0.222851            0.218681 â”‚
â”‚ trial-fbf36_00003   TERMINATED             32       0.000248197         512              4         64            3   0.00172192    type1          5           15.7993        0.288344       0.226781            0.216627 â”‚
â”‚ trial-fbf36_00004   TERMINATED             64       5.06549e-05          32              2        512            4   0.00121529    cosine         8            7.71778       0.466575       0.246413            0.246413 â”‚
â”‚ trial-fbf36_00005   TERMINATED             16       0.00163577           64              2         64            4   0.000980049   cosine         5           30.6577        0.327883       0.23544             0.223224 â”‚
â”‚ trial-fbf36_00006   TERMINATED             16       5.57812e-05         128              4         64            4   0.00141857    cosine         6           35.6545        0.374352       0.219479            0.217397 â”‚
â”‚ trial-fbf36_00007   TERMINATED             16       0.00356994          256              2         64            1   0.00411253    cosine         6           23.5335        0.347734       0.235125            0.22266  â”‚
â”‚ trial-fbf36_00008   TERMINATED             16       0.000200582          32              4        256            4   0.00204933    cosine         6           17.6422        0.381227       0.223524            0.223497 â”‚
â”‚ trial-fbf36_00009   TERMINATED             32       0.000430098         128              3        512            3   0.00279967    type1          5            9.94143       0.32444        0.230795            0.220862 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=3023605)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1002/trial-fbf36_00008_8_alpha_d_ff=4,batch_size=16,d_core=256,d_model=32,dropout=0.0020,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_14-16-58/checkpoint_000005)
[36m(_train_fn pid=3023605)[0m 	iters: 500, epoch: 6 | loss: 0.1746676[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3023605)[0m 	speed: 0.0045s/iter; left time: 4.8639s[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3023605)[0m Early stopping
[36m(_train_fn pid=3023605)[0m Epoch: 6 cost time: 2.392418622970581
[36m(_train_fn pid=3023605)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3812272 Vali Loss: 0.2235245 Best vali loss: 0.2234968
[36m(_train_fn pid=3023605)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3023605)[0m Updating learning rate to 2.937460590282174e-05
[36m(_train_fn pid=3023605)[0m saving checkpoint...


Time taken (4 parallel trials): 68 seconds


2024-08-26 14:17:21,450	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:17:21,843	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:17:21,848	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:17:21,856	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:17:24,727	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3027131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003/trial-24cd8_00003_3_alpha_d_ff=3,batch_size=128,d_core=256,d_model=64,dropout=0.0023,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_14-17-21/checkpoint_000000)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed1003   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator       â”‚
â”‚ Scheduler                        FIFOScheduler               â”‚
â”‚ Number of trials                 10                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-17-21_018468_3024905/artifacts/2024-08-26_14-17-21/ETTh2_96_96_test_seed1003/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:17:21. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-24cd8_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-24cd8_00001   PENDING              16       0.000163073          32              2        128            4   0.00613258   type1   â”‚
â”‚ trial-24cd8_00002   PENDING              32       0.000239797         256              3        128            2   0.00542299   type1   â”‚
â”‚ trial-24cd8_00003   PENDING             128       9.55808e-05          64              3        256            1   0.00229725   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-24cd8_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00542 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00024 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-24cd8_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00613 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00016 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3027129)[0m configuration
[36m(_train_fn pid=3027129)[0m {'batch_size': 16, 'learning_rate': 0.00016307283625952642, 'd_model': 32, 'd_core': 128, 'e_layers': 4, 'dropout': 0.006132581794902885, 'lradj': 'type1', 'd_ff': 64}

Trial trial-24cd8_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00003 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              3 â”‚
â”‚ batch_size                            128 â”‚
â”‚ d_core                                256 â”‚
â”‚ d_model                                64 â”‚
â”‚ dropout                            0.0023 â”‚
â”‚ e_layers                                1 â”‚
â”‚ learning_rate                      0.0001 â”‚
â”‚ lradj                               type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-24cd8_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3027129)[0m Use GPU: cuda:0
[36m(_train_fn pid=3027128)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=3027129)[0m train 8449
[36m(_train_fn pid=3027129)[0m val 2785
[36m(_train_fn pid=3027129)[0m start_epoch 0
[36m(_train_fn pid=3027129)[0m max_epoch 8
[36m(_train_fn pid=3027131)[0m Updating learning rate to 9.558079162204709e-05
[36m(_train_fn pid=3027131)[0m saving checkpoint...
[36m(_train_fn pid=3027131)[0m Validation loss decreased (inf --> 0.2745).  Saving model state dict ...
[36m(_train_fn pid=3027131)[0m Epoch: 1 cost time: 0.6901369094848633
[36m(_train_fn pid=3027131)[0m Epoch: 1, Steps: 67 | Train Loss: 0.5523515 Vali Loss: 0.2745378 Best vali loss: 0.2745378
[36m(_train_fn pid=3027128)[0m 	iters: 100, epoch: 1 | loss: 0.6942417
2024-08-26 14:17:25,237	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3027131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003/trial-24cd8_00003_3_alpha_d_ff=3,batch_size=128,d_core=256,d_model=64,dropout=0.0023,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_14-17-21/checkpoint_000001)
2024-08-26 14:17:25,733	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3027131)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003/trial-24cd8_00003_3_alpha_d_ff=3,batch_size=128,d_core=256,d_model=64,dropout=0.0023,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_14-17-21/checkpoint_000002)
2024-08-26 14:17:26,096	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:26,105	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:26,197	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:26,689	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:27,171	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:27,652	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:27,972	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:28,002	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:28,103	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:29,633	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:29,661	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3027129)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003/trial-24cd8_00001_1_alpha_d_ff=2,batch_size=16,d_core=128,d_model=32,dropout=0.0061,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_14-17-21/checkpoint_000000)[32m [repeated 12x across cluster][0m
2024-08-26 14:17:30,456	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:31,202	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:31,241	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:32,867	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:32,918	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:33,888	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:34,232	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3027128)[0m 	speed: 0.0113s/iter; left time: 22.8779s
[36m(_train_fn pid=3027131)[0m Updating learning rate to 4.7790395811023546e-05
[36m(_train_fn pid=3027131)[0m saving checkpoint...
[36m(_train_fn pid=3027131)[0m Validation loss decreased (0.2745 --> 0.2533).  Saving model state dict ...
[36m(_train_fn pid=3027131)[0m Epoch: 2 cost time: 0.381511926651001
[36m(_train_fn pid=3027131)[0m Epoch: 2, Steps: 67 | Train Loss: 0.5049747 Vali Loss: 0.2533122 Best vali loss: 0.2533122
[36m(_train_fn pid=3027131)[0m Updating learning rate to 2.3895197905511773e-05
[36m(_train_fn pid=3027131)[0m saving checkpoint...
[36m(_train_fn pid=3027131)[0m Validation loss decreased (0.2533 --> 0.2468).  Saving model state dict ...
[36m(_train_fn pid=3027131)[0m Epoch: 3 cost time: 0.374924898147583
[36m(_train_fn pid=3027131)[0m Epoch: 3, Steps: 67 | Train Loss: 0.4707391 Vali Loss: 0.2467879 Best vali loss: 0.2467879
[36m(_train_fn pid=3027131)[0m Validation loss decreased (0.2468 --> 0.2441).  Saving model state dict ...
[36m(_train_fn pid=3027131)[0m Validation loss decreased (0.2441 --> 0.2428).  Saving model state dict ...
[36m(_train_fn pid=3027131)[0m Validation loss decreased (0.2428 --> 0.2425).  Saving model state dict ...
[36m(_train_fn pid=3027131)[0m Validation loss decreased (0.2425 --> 0.2422).  Saving model state dict ...

Trial trial-24cd8_00003 completed after 8 iterations at 2024-08-26 14:17:28. Total running time: 6s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.44761 â”‚
â”‚ time_total_s                                 4.63697 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.24215 â”‚
â”‚ train_loss                                   0.45965 â”‚
â”‚ valid_loss                                   0.24215 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3027128)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=3027131)[0m {'batch_size': 128, 'learning_rate': 9.558079162204709e-05, 'd_model': 64, 'd_core': 256, 'e_layers': 1, 'dropout': 0.0022972474584072584, 'lradj': 'type1', 'd_ff': 192}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3027128)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3027128)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3027128)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3027128)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3027128)[0m max_epoch 8[32m [repeated 3x across cluster][0m

Trial trial-24cd8_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00108 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00059 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3027130)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3027128)[0m Updating learning rate to 0.00020740251485476345[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=3027128)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=3027128)[0m Validation loss decreased (inf --> 0.2204).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3027128)[0m Epoch: 3 cost time: 1.414358377456665[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=3027128)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3851224 Vali Loss: 0.2177114 Best vali loss: 0.2177114[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=3027129)[0m 	iters: 500, epoch: 1 | loss: 0.5077266[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3027129)[0m 	speed: 0.0092s/iter; left time: 34.3510s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3027128)[0m Validation loss decreased (0.2185 --> 0.2177).  Saving model state dict ...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3027130)[0m EarlyStopping counter: 2 out of 3

Trial trial-24cd8_00002 completed after 5 iterations at 2024-08-26 14:17:32. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.66276 â”‚
â”‚ time_total_s                                 9.42525 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21587 â”‚
â”‚ train_loss                                   0.31865 â”‚
â”‚ valid_loss                                   0.22487 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3027130)[0m Early stopping
[36m(_train_fn pid=3028521)[0m configuration
[36m(_train_fn pid=3028521)[0m {'batch_size': 8, 'learning_rate': 0.0005890293097086312, 'd_model': 32, 'd_core': 128, 'e_layers': 2, 'dropout': 0.0010757910959639108, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=3028521)[0m Use GPU: cuda:0
[36m(_train_fn pid=3028521)[0m train 8449
[36m(_train_fn pid=3028521)[0m val 2785
[36m(_train_fn pid=3028521)[0m start_epoch 0
[36m(_train_fn pid=3028521)[0m max_epoch 8

Trial trial-24cd8_00000 completed after 6 iterations at 2024-08-26 14:17:34. Total running time: 12s
2024-08-26 14:17:34,743	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:37,564	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3028521)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003/trial-24cd8_00004_4_alpha_d_ff=4,batch_size=8,d_core=128,d_model=32,dropout=0.0011,e_layers=2,learning_rate=0.0006,lradj=type1_2024-08-26_14-17-28/checkpoint_000001)[32m [repeated 8x across cluster][0m
2024-08-26 14:17:39,388	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:39,844	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:41,266	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:42,277	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:44,038	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3029081)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003/trial-24cd8_00005_5_alpha_d_ff=1,batch_size=16,d_core=32,d_model=512,dropout=0.0029,e_layers=3,learning_rate=0.0013,lradj=type1_2024-08-26_14-17-32/checkpoint_000001)[32m [repeated 5x across cluster][0m
2024-08-26 14:17:45,037	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:45,832	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.31131 â”‚
â”‚ time_total_s                                10.72931 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21771 â”‚
â”‚ train_loss                                   0.33596 â”‚
â”‚ valid_loss                                   0.22219 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-24cd8_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00291 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00132 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3029081)[0m configuration
[36m(_train_fn pid=3029081)[0m {'batch_size': 16, 'learning_rate': 0.0013234567037569034, 'd_model': 512, 'd_core': 32, 'e_layers': 3, 'dropout': 0.002914537678071102, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=3029081)[0m Use GPU: cuda:0
[36m(_train_fn pid=3029081)[0m train 8449
[36m(_train_fn pid=3029081)[0m val 2785
[36m(_train_fn pid=3027128)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3029081)[0m start_epoch 0
[36m(_train_fn pid=3029081)[0m max_epoch 8
[36m(_train_fn pid=3027129)[0m Updating learning rate to 8.153641812976321e-05[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3027129)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3028521)[0m Validation loss decreased (inf --> 0.2209).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3027129)[0m Epoch: 2 cost time: 3.9359664916992188[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3027129)[0m Epoch: 2, Steps: 529 | Train Loss: 0.4303083 Vali Loss: 0.2223652 Best vali loss: 0.2223652[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3028521)[0m 	iters: 300, epoch: 2 | loss: 0.6003966[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=3028521)[0m 	speed: 0.0030s/iter; left time: 21.2896s[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=3027129)[0m Validation loss decreased (0.2314 --> 0.2224).  Saving model state dict ...

Trial trial-24cd8_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00123 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00052 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3027128)[0m Early stopping
[36m(_train_fn pid=3029295)[0m configuration
[36m(_train_fn pid=3029295)[0m {'batch_size': 16, 'learning_rate': 0.0005154961912370065, 'd_model': 256, 'd_core': 32, 'e_layers': 4, 'dropout': 0.001230705730311713, 'lradj': 'type1', 'd_ff': 256}
[36m(_train_fn pid=3029295)[0m Use GPU: cuda:0
[36m(_train_fn pid=3029295)[0m train 8449
[36m(_train_fn pid=3029295)[0m val 2785
[36m(_train_fn pid=3027129)[0m Validation loss decreased (0.2224 --> 0.2192).  Saving model state dict ...
[36m(_train_fn pid=3028521)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3029295)[0m start_epoch 0
[36m(_train_fn pid=3029295)[0m max_epoch 8
[36m(_train_fn pid=3027129)[0m Updating learning rate to 4.0768209064881604e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3027129)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3029081)[0m Validation loss decreased (inf --> 0.2656).  Saving model state dict ...
[36m(_train_fn pid=3027129)[0m Epoch: 3 cost time: 4.35388445854187[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3027129)[0m Epoch: 3, Steps: 529 | Train Loss: 0.4163510 Vali Loss: 0.2192135 Best vali loss: 0.2192135[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3028521)[0m 	iters: 800, epoch: 3 | loss: 0.7329797[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=3028521)[0m 	speed: 0.0031s/iter; left time: 17.2195s[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=3028521)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3029295)[0m Validation loss decreased (inf --> 0.2202).  Saving model state dict ...

Trial trial-24cd8_00004 completed after 4 iterations at 2024-08-26 14:17:45. Total running time: 23s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             3.76714 â”‚
â”‚ time_total_s                                15.41568 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                               0.2209 â”‚
â”‚ train_loss                                    0.3614 â”‚
â”‚ valid_loss                                    0.2237 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3028521)[0m Early stopping
[36m(_train_fn pid=3028521)[0m Updating learning rate to 7.36286637135789e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3028521)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3028521)[0m Epoch: 4 cost time: 3.2582035064697266[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3028521)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3613979 Vali Loss: 0.2237048 Best vali loss: 0.2208988[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3029295)[0m 	iters: 300, epoch: 2 | loss: 0.3392709[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=3029295)[0m 	speed: 0.0094s/iter; left time: 31.8673s[32m [repeated 28x across cluster][0m

Trial trial-24cd8_00007 started with configuration:
2024-08-26 14:17:48,212	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:48,237	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:49,049	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:49,183	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3030076)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003/trial-24cd8_00007_7_alpha_d_ff=2,batch_size=64,d_core=64,d_model=64,dropout=0.0100,e_layers=2,learning_rate=0.0005,lradj=type1_2024-08-26_14-17-45/checkpoint_000001)[32m [repeated 6x across cluster][0m
2024-08-26 14:17:50,267	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:51,347	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:52,622	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00996 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00055 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3030076)[0m configuration
[36m(_train_fn pid=3030076)[0m {'batch_size': 64, 'learning_rate': 0.000545267102413704, 'd_model': 64, 'd_core': 64, 'e_layers': 2, 'dropout': 0.009960064350103852, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=3030076)[0m Use GPU: cuda:0
[36m(_train_fn pid=3027129)[0m EarlyStopping counter: 1 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3030076)[0m train 8449
[36m(_train_fn pid=3030076)[0m val 2785
[36m(_train_fn pid=3030076)[0m start_epoch 0
[36m(_train_fn pid=3030076)[0m max_epoch 8
[36m(_train_fn pid=3030076)[0m Validation loss decreased (inf --> 0.2187).  Saving model state dict ...
[36m(_train_fn pid=3030076)[0m Updating learning rate to 0.000272633551206852[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3030076)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3030076)[0m Epoch: 2 cost time: 0.761343240737915[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3030076)[0m Epoch: 2, Steps: 133 | Train Loss: 0.4139244 Vali Loss: 0.2198754 Best vali loss: 0.2187010[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3027129)[0m 	iters: 400, epoch: 5 | loss: 0.3553166[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3027129)[0m 	speed: 0.0119s/iter; left time: 20.4992s[32m [repeated 15x across cluster][0m

Trial trial-24cd8_00007 completed after 4 iterations at 2024-08-26 14:17:51. Total running time: 29s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.07763 â”‚
â”‚ time_total_s                                 4.83466 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                               0.2187 â”‚
â”‚ train_loss                                   0.39078 â”‚
â”‚ valid_loss                                   0.21972 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3030076)[0m Early stopping
[36m(_train_fn pid=3030076)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m

Trial status: 5 TERMINATED | 3 RUNNING | 1 PENDING
Current time: 2024-08-26 14:17:52. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 24cd8_00002 with best_valid_loss=0.2158650067165039 and params={'batch_size': 32, 'learning_rate': 0.00023979736321978667, 'd_model': 256, 'd_core': 128, 'e_layers': 2, 'dropout': 0.005422986825040879, 'lradj': 'type1', 'd_ff': 768}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-24cd8_00001   RUNNING                16       0.000163073          32              2        128            4   0.00613258   type1          4           22.3872        0.422964       0.219315            0.219213 â”‚
â”‚ trial-24cd8_00005   RUNNING                16       0.00132346          512              1         32            3   0.00291454   type1          3           14.6092        0.509604       0.26827             0.265636 â”‚
â”‚ trial-24cd8_00006   RUNNING                16       0.000515496         256              1         32            4   0.00123071   type1          2           12.4473        0.397655       0.224739            0.220157 â”‚
â”‚ trial-24cd8_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         6           10.7293        0.335956       0.222188            0.217711 â”‚
â”‚ trial-24cd8_00002   TERMINATED             32       0.000239797         256              3        128            2   0.00542299   type1          5            9.42525       0.31865        0.22487             0.215865 â”‚
â”‚ trial-24cd8_00003   TERMINATED            128       9.55808e-05          64              3        256            1   0.00229725   type1          8            4.63697       0.459648       0.242151            0.242151 â”‚
â”‚ trial-24cd8_00004   TERMINATED              8       0.000589029          32              4        128            2   0.00107579   type1          4           15.4157        0.361398       0.223705            0.220899 â”‚
â”‚ trial-24cd8_00007   TERMINATED             64       0.000545267          64              2         64            2   0.00996006   type1          4            4.83466       0.390781       0.219715            0.218701 â”‚
â”‚ trial-24cd8_00008   PENDING                16       0.000766683          32              4         32            1   0.0044934    type1                                                                                 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

2024-08-26 14:17:54,174	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:54,448	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3029295)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003/trial-24cd8_00006_6_alpha_d_ff=1,batch_size=16,d_core=32,d_model=256,dropout=0.0012,e_layers=4,learning_rate=0.0005,lradj=type1_2024-08-26_14-17-34/checkpoint_000002)[32m [repeated 5x across cluster][0m
2024-08-26 14:17:55,014	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:56,431	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:58,044	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:58,439	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:58,870	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:17:59,570	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3030645)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003/trial-24cd8_00008_8_alpha_d_ff=4,batch_size=16,d_core=32,d_model=32,dropout=0.0045,e_layers=1,learning_rate=0.0008,lradj=type1_2024-08-26_14-17-51/checkpoint_000003)[32m [repeated 6x across cluster][0m
2024-08-26 14:18:00,280	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:01,030	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:02,731	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:03,527	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:07,161	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
Trial trial-24cd8_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00449 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00077 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3030645)[0m configuration
[36m(_train_fn pid=3030645)[0m {'batch_size': 16, 'learning_rate': 0.0007666833135983706, 'd_model': 32, 'd_core': 32, 'e_layers': 1, 'dropout': 0.004493400959090864, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=3030645)[0m Use GPU: cuda:0
[36m(_train_fn pid=3030645)[0m train 8449
[36m(_train_fn pid=3030645)[0m val 2785
[36m(_train_fn pid=3030645)[0m start_epoch 0
[36m(_train_fn pid=3030645)[0m max_epoch 8
[36m(_train_fn pid=3029081)[0m Validation loss decreased (0.2656 --> 0.2500).  Saving model state dict ...
[36m(_train_fn pid=3030645)[0m Validation loss decreased (inf --> 0.2178).  Saving model state dict ...
[36m(_train_fn pid=3030645)[0m Updating learning rate to 0.0007666833135983706[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3030645)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3030645)[0m Epoch: 1 cost time: 1.4105358123779297[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3030645)[0m Epoch: 1, Steps: 529 | Train Loss: 0.4442327 Vali Loss: 0.2178074 Best vali loss: 0.2178074[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3027129)[0m 	iters: 300, epoch: 6 | loss: 0.2975323[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=3027129)[0m 	speed: 0.0098s/iter; left time: 12.6040s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=3029295)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3027129)[0m Validation loss decreased (0.2192 --> 0.2192).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-24cd8_00006 completed after 4 iterations at 2024-08-26 14:18:00. Total running time: 38s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             5.82979 â”‚
â”‚ time_total_s                                   24.51 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22016 â”‚
â”‚ train_loss                                   0.28175 â”‚
â”‚ valid_loss                                   0.23519 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3029295)[0m Early stopping
[36m(_train_fn pid=3029295)[0m Updating learning rate to 6.443702390462582e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3029295)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3029295)[0m Epoch: 4 cost time: 4.983021259307861[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3029295)[0m Epoch: 4, Steps: 529 | Train Loss: 0.2817502 Vali Loss: 0.2351888 Best vali loss: 0.2201573[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3030645)[0m 	iters: 400, epoch: 5 | loss: 0.4368298[32m [repeated 33x across cluster][0m
[36m(_train_fn pid=3030645)[0m 	speed: 0.0022s/iter; left time: 3.7231s[32m [repeated 33x across cluster][0m

Trial trial-24cd8_00008 completed after 5 iterations at 2024-08-26 14:18:01. Total running time: 39s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.45817 â”‚
â”‚ time_total_s                                 8.13234 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21698 â”‚
â”‚ train_loss                                   0.38249 â”‚
â”‚ valid_loss                                   0.21755 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-24cd8_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00133 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00021 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3030645)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3031482)[0m configuration
[36m(_train_fn pid=3031482)[0m {'batch_size': 16, 'learning_rate': 0.0002056347169541962, 'd_model': 64, 'd_core': 64, 'e_layers': 4, 'dropout': 0.0013330061312821111, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=3031482)[0m Use GPU: cuda:0
[36m(_train_fn pid=3031482)[0m train 8449
[36m(_train_fn pid=3031482)[0m val 2785
[36m(_train_fn pid=3031482)[0m start_epoch 0
[36m(_train_fn pid=3031482)[0m max_epoch 8
[36m(_train_fn pid=3027129)[0m Validation loss decreased (0.2192 --> 0.2192).  Saving model state dict ...
[36m(_train_fn pid=3030645)[0m Early stopping
[36m(_train_fn pid=3027129)[0m Updating learning rate to 2.5480130665551003e-06[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3027129)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3027129)[0m Epoch: 7 cost time: 4.2545881271362305[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3027129)[0m Epoch: 7, Steps: 529 | Train Loss: 0.4065652 Vali Loss: 0.2191745 Best vali loss: 0.2191745[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3027129)[0m 	iters: 200, epoch: 8 | loss: 0.3982520[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3027129)[0m 	speed: 0.0100s/iter; left time: 3.3062s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3029081)[0m EarlyStopping counter: 2 out of 3

Trial trial-24cd8_00005 completed after 7 iterations at 2024-08-26 14:18:07. Total running time: 45s
[36m(_train_fn pid=3029081)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003/trial-24cd8_00005_5_alpha_d_ff=1,batch_size=16,d_core=32,d_model=512,dropout=0.0029,e_layers=3,learning_rate=0.0013,lradj=type1_2024-08-26_14-17-32/checkpoint_000006)[32m [repeated 5x across cluster][0m
2024-08-26 14:18:07,767	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:08,564	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:10,576	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3031482)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003/trial-24cd8_00009_9_alpha_d_ff=1,batch_size=16,d_core=64,d_model=64,dropout=0.0013,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_14-18-00/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 14:18:13,496	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:16,428	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:19,419	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3031482)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003/trial-24cd8_00009_9_alpha_d_ff=1,batch_size=16,d_core=64,d_model=64,dropout=0.0013,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_14-18-00/checkpoint_000004)[32m [repeated 2x across cluster][0m
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             4.42867 â”‚
â”‚ time_total_s                                32.70911 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.24997 â”‚
â”‚ train_loss                                   0.48124 â”‚
â”‚ valid_loss                                   0.25122 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3029081)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3029081)[0m Early stopping
[36m(_train_fn pid=3031482)[0m Validation loss decreased (inf --> 0.2209).  Saving model state dict ...

Trial trial-24cd8_00001 completed after 8 iterations at 2024-08-26 14:18:08. Total running time: 46s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             5.03416 â”‚
â”‚ time_total_s                                45.10841 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21913 â”‚
â”‚ train_loss                                   0.40705 â”‚
â”‚ valid_loss                                   0.21913 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3027129)[0m Validation loss decreased (0.2192 --> 0.2191).  Saving model state dict ...
[36m(_train_fn pid=3031482)[0m Updating learning rate to 0.0001028173584770981[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3031482)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3031482)[0m Epoch: 2 cost time: 2.4608497619628906[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3031482)[0m Epoch: 2, Steps: 529 | Train Loss: 0.4130183 Vali Loss: 0.2197378 Best vali loss: 0.2197378[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3031482)[0m 	iters: 500, epoch: 2 | loss: 0.3486605[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3031482)[0m 	speed: 0.0042s/iter; left time: 13.4068s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3031482)[0m Validation loss decreased (0.2197 --> 0.2181).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3031482)[0m Updating learning rate to 5.140867923854905e-05
[36m(_train_fn pid=3031482)[0m saving checkpoint...
[36m(_train_fn pid=3031482)[0m Epoch: 3 cost time: 2.580080986022949
[36m(_train_fn pid=3031482)[0m Epoch: 3, Steps: 529 | Train Loss: 0.3997894 Vali Loss: 0.2180574 Best vali loss: 0.2180574
[36m(_train_fn pid=3031482)[0m 	iters: 500, epoch: 4 | loss: 0.5060875[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3031482)[0m 	speed: 0.0048s/iter; left time: 10.3256s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3031482)[0m Updating learning rate to 2.5704339619274526e-05
[36m(_train_fn pid=3031482)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3031482)[0m saving checkpoint...
[36m(_train_fn pid=3031482)[0m Epoch: 4 cost time: 2.5827550888061523
[36m(_train_fn pid=3031482)[0m Epoch: 4, Steps: 529 | Train Loss: 0.3901603 Vali Loss: 0.2198299 Best vali loss: 0.2180574
[36m(_train_fn pid=3031482)[0m Updating learning rate to 1.2852169809637263e-05
[36m(_train_fn pid=3031482)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3031482)[0m saving checkpoint...
[36m(_train_fn pid=3031482)[0m Epoch: 5 cost time: 2.6475231647491455
[36m(_train_fn pid=3031482)[0m Epoch: 5, Steps: 529 | Train Loss: 0.3845840 Vali Loss: 0.2202340 Best vali loss: 0.2180574
[36m(_train_fn pid=3031482)[0m 	iters: 400, epoch: 6 | loss: 1.0222883[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3031482)[0m 	speed: 0.0047s/iter; left time: 5.5390s[32m [repeated 9x across cluster][0m

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2024-08-26 14:18:22. Total running time: 1min 0s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 24cd8_00002 with best_valid_loss=0.2158650067165039 and params={'batch_size': 32, 'learning_rate': 0.00023979736321978667, 'd_model': 256, 'd_core': 128, 'e_layers': 2, 'dropout': 0.005422986825040879, 'lradj': 'type1', 'd_ff': 768}
2024-08-26 14:18:22,296	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:22,306	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003' in 0.0097s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-24cd8_00009   RUNNING                16       0.000205635          64              1         64            4   0.00133301   type1          5           17.612         0.384584       0.220234            0.218057 â”‚
â”‚ trial-24cd8_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         6           10.7293        0.335956       0.222188            0.217711 â”‚
â”‚ trial-24cd8_00001   TERMINATED             16       0.000163073          32              2        128            4   0.00613258   type1          8           45.1084        0.407046       0.219125            0.219125 â”‚
â”‚ trial-24cd8_00002   TERMINATED             32       0.000239797         256              3        128            2   0.00542299   type1          5            9.42525       0.31865        0.22487             0.215865 â”‚
â”‚ trial-24cd8_00003   TERMINATED            128       9.55808e-05          64              3        256            1   0.00229725   type1          8            4.63697       0.459648       0.242151            0.242151 â”‚
â”‚ trial-24cd8_00004   TERMINATED              8       0.000589029          32              4        128            2   0.00107579   type1          4           15.4157        0.361398       0.223705            0.220899 â”‚
â”‚ trial-24cd8_00005   TERMINATED             16       0.00132346          512              1         32            3   0.00291454   type1          7           32.7091        0.481235       0.251222            0.249974 â”‚
â”‚ trial-24cd8_00006   TERMINATED             16       0.000515496         256              1         32            4   0.00123071   type1          4           24.51          0.28175        0.235189            0.220157 â”‚
â”‚ trial-24cd8_00007   TERMINATED             64       0.000545267          64              2         64            2   0.00996006   type1          4            4.83466       0.390781       0.219715            0.218701 â”‚
â”‚ trial-24cd8_00008   TERMINATED             16       0.000766683          32              4         32            1   0.0044934    type1          5            8.13234       0.382494       0.217549            0.216981 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-24cd8_00009 completed after 6 iterations at 2024-08-26 14:18:22. Total running time: 1min 0s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-24cd8_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             2.87366 â”‚
â”‚ time_total_s                                20.48568 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21806 â”‚
â”‚ train_loss                                   0.38265 â”‚
â”‚ valid_loss                                   0.21982 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:18:22. Total running time: 1min 0s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 24cd8_00002 with best_valid_loss=0.2158650067165039 and params={'batch_size': 32, 'learning_rate': 0.00023979736321978667, 'd_model': 256, 'd_core': 128, 'e_layers': 2, 'dropout': 0.005422986825040879, 'lradj': 'type1', 'd_ff': 768}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-24cd8_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         6           10.7293        0.335956       0.222188            0.217711 â”‚
â”‚ trial-24cd8_00001   TERMINATED             16       0.000163073          32              2        128            4   0.00613258   type1          8           45.1084        0.407046       0.219125            0.219125 â”‚
â”‚ trial-24cd8_00002   TERMINATED             32       0.000239797         256              3        128            2   0.00542299   type1          5            9.42525       0.31865        0.22487             0.215865 â”‚
â”‚ trial-24cd8_00003   TERMINATED            128       9.55808e-05          64              3        256            1   0.00229725   type1          8            4.63697       0.459648       0.242151            0.242151 â”‚
â”‚ trial-24cd8_00004   TERMINATED              8       0.000589029          32              4        128            2   0.00107579   type1          4           15.4157        0.361398       0.223705            0.220899 â”‚
â”‚ trial-24cd8_00005   TERMINATED             16       0.00132346          512              1         32            3   0.00291454   type1          7           32.7091        0.481235       0.251222            0.249974 â”‚
â”‚ trial-24cd8_00006   TERMINATED             16       0.000515496         256              1         32            4   0.00123071   type1          4           24.51          0.28175        0.235189            0.220157 â”‚
â”‚ trial-24cd8_00007   TERMINATED             64       0.000545267          64              2         64            2   0.00996006   type1          4            4.83466       0.390781       0.219715            0.218701 â”‚
â”‚ trial-24cd8_00008   TERMINATED             16       0.000766683          32              4         32            1   0.0044934    type1          5            8.13234       0.382494       0.217549            0.216981 â”‚
â”‚ trial-24cd8_00009   TERMINATED             16       0.000205635          64              1         64            4   0.00133301   type1          6           20.4857        0.382645       0.219817            0.218057 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 32, 'learning_rate': 0.00023979736321978667, 'd_model': 256, 'd_core': 128, 'e_layers': 2, 'dropout': 0.005422986825040879, 'lradj': 'type1', 'd_ff': 768}
[36m(_train_fn pid=3031482)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1003/trial-24cd8_00009_9_alpha_d_ff=1,batch_size=16,d_core=64,d_model=64,dropout=0.0013,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_14-18-00/checkpoint_000005)
[36m(_train_fn pid=3031482)[0m Updating learning rate to 6.4260849048186316e-06
[36m(_train_fn pid=3031482)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3031482)[0m saving checkpoint...
[36m(_train_fn pid=3031482)[0m Epoch: 6 cost time: 2.5285801887512207
[36m(_train_fn pid=3031482)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3826453 Vali Loss: 0.2198168 Best vali loss: 0.2180574
[36m(_train_fn pid=3031482)[0m Early stopping
[36m(_train_fn pid=3031482)[0m 	iters: 500, epoch: 6 | loss: 0.3187124
[36m(_train_fn pid=3031482)[0m 	speed: 0.0047s/iter; left time: 5.1492s


Time taken (4 parallel trials): 65 seconds


2024-08-26 14:18:26,186	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:18:26,563	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:18:26,569	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:18:26,576	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:18:30,030	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3034559)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1004/trial-4b632_00002_2_alpha_d_ff=4,batch_size=128,d_core=128,d_model=512,dropout=0.0075,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_14-18-26/checkpoint_000000)
2024-08-26 14:18:31,167	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed1004   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator       â”‚
â”‚ Scheduler                        FIFOScheduler               â”‚
â”‚ Number of trials                 10                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1004
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-18-25_752763_3032335/artifacts/2024-08-26_14-18-26/ETTh2_96_96_test_seed1004/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:18:26. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-4b632_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-4b632_00001   PENDING              64       9.10485e-05          32              1         32            4   0.00429595   cosine  â”‚
â”‚ trial-4b632_00002   PENDING             128       8.91158e-05         512              4        128            2   0.00747051   cosine  â”‚
â”‚ trial-4b632_00003   PENDING              32       0.000134396         256              2         32            2   0.00192732   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-4b632_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00193 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00013 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-4b632_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                             0.0043 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00009 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-4b632_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00747 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00009 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3034559)[0m configuration
[36m(_train_fn pid=3034559)[0m {'batch_size': 128, 'learning_rate': 8.911579472795504e-05, 'd_model': 512, 'd_core': 128, 'e_layers': 2, 'dropout': 0.007470505338674834, 'lradj': 'cosine', 'd_ff': 2048}
[36m(_train_fn pid=3034559)[0m Use GPU: cuda:0

Trial trial-4b632_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3034559)[0m train 8449
[36m(_train_fn pid=3034559)[0m val 2785
[36m(_train_fn pid=3034559)[0m start_epoch 0
[36m(_train_fn pid=3034559)[0m max_epoch 8
[36m(_train_fn pid=3034559)[0m Updating learning rate to 8.572402675029497e-05
[36m(_train_fn pid=3034559)[0m saving checkpoint...
[36m(_train_fn pid=3034559)[0m Validation loss decreased (inf --> 0.2185).  Saving model state dict ...
[36m(_train_fn pid=3034560)[0m 	iters: 100, epoch: 1 | loss: 0.6560800
[36m(_train_fn pid=3034560)[0m 	speed: 0.0163s/iter; left time: 32.9123s
[36m(_train_fn pid=3034559)[0m Epoch: 1 cost time: 1.2225043773651123
[36m(_train_fn pid=3034559)[0m Epoch: 1, Steps: 67 | Train Loss: 0.5330190 Vali Loss: 0.2184851 Best vali loss: 0.2184851
[36m(_train_fn pid=3034559)[0m Updating learning rate to 7.606508874546021e-05
[36m(_train_fn pid=3034559)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1004/trial-4b632_00002_2_alpha_d_ff=4,batch_size=128,d_core=128,d_model=512,dropout=0.0075,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_14-18-26/checkpoint_000001)
2024-08-26 14:18:31,240	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:31,974	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:32,085	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:32,230	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:33,333	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:33,503	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:34,413	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:34,979	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:35,237	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3034557)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1004/trial-4b632_00000_0_2024-08-26_14-18-26/checkpoint_000001)[32m [repeated 9x across cluster][0m
2024-08-26 14:18:35,434	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:35,697	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:36,798	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:36,876	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:37,005	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:38,344	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:38,373	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:38,542	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:39,479	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:39,538	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:39,678	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3034559)[0m saving checkpoint...
[36m(_train_fn pid=3034559)[0m Validation loss decreased (0.2185 --> 0.2159).  Saving model state dict ...
[36m(_train_fn pid=3034559)[0m Epoch: 2 cost time: 0.9450392723083496
[36m(_train_fn pid=3034559)[0m Epoch: 2, Steps: 67 | Train Loss: 0.4085198 Vali Loss: 0.2159088 Best vali loss: 0.2159088
[36m(_train_fn pid=3034559)[0m Validation loss decreased (0.2159 --> 0.2135).  Saving model state dict ...
[36m(_train_fn pid=3034557)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=3034557)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3034557)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3034559)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3034557)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3034557)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3034557)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3034557)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3034559)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3034560)[0m Updating learning rate to 0.00011471441339228252[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3034560)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3034557)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3034557)[0m 	iters: 200, epoch: 2 | loss: 0.4663539[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3034557)[0m 	speed: 0.0099s/iter; left time: 16.3610s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3034560)[0m Epoch: 2 cost time: 2.495028257369995[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3034560)[0m Epoch: 2, Steps: 265 | Train Loss: 0.3996345 Vali Loss: 0.2135380 Best vali loss: 0.2135380[32m [repeated 8x across cluster][0m

Trial trial-4b632_00002 completed after 6 iterations at 2024-08-26 14:18:35. Total running time: 8s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.01778 â”‚
â”‚ time_total_s                                 7.26498 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21353 â”‚
â”‚ train_loss                                   0.36683 â”‚
â”‚ valid_loss                                   0.21374 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3034559)[0m Early stopping
[36m(_train_fn pid=3034558)[0m Validation loss decreased (0.2554 --> 0.2458).  Saving model state dict ...[32m [repeated 3x across cluster][0m

Trial trial-4b632_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00645 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00181 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3035968)[0m {'batch_size': 16, 'learning_rate': 0.0018118487608186741, 'd_model': 512, 'd_core': 256, 'e_layers': 1, 'dropout': 0.006445039802030522, 'lradj': 'type1', 'd_ff': 1536}
[36m(_train_fn pid=3035968)[0m configuration
[36m(_train_fn pid=3035968)[0m Use GPU: cuda:0
[36m(_train_fn pid=3034557)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m

Trial trial-4b632_00000 completed after 4 iterations at 2024-08-26 14:18:38. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.66334 â”‚
â”‚ time_total_s                                10.30013 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22053 â”‚
â”‚ train_loss                                   0.36278 â”‚
â”‚ valid_loss                                    0.2239 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3035968)[0m train 8449
[36m(_train_fn pid=3035968)[0m val 2785
[36m(_train_fn pid=3035968)[0m start_epoch 0
[36m(_train_fn pid=3035968)[0m max_epoch 8

Trial trial-4b632_00003 completed after 5 iterations at 2024-08-26 14:18:39. Total running time: 13s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.33129 â”‚
â”‚ time_total_s                                11.53564 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21354 â”‚
â”‚ train_loss                                    0.3472 â”‚
â”‚ valid_loss                                   0.22187 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3034560)[0m Updating learning rate to 4.148252980657428e-05[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3034560)[0m saving checkpoint...[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3035968)[0m Validation loss decreased (inf --> 0.2351).  Saving model state dict ...
[36m(_train_fn pid=3035968)[0m 	iters: 200, epoch: 2 | loss: 0.3577550[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=3035968)[0m 	speed: 0.0027s/iter; left time: 9.4005s[32m [repeated 21x across cluster][0m

Trial trial-4b632_00005 started with configuration:
2024-08-26 14:18:40,437	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3034558)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1004/trial-4b632_00001_1_alpha_d_ff=1,batch_size=64,d_core=32,d_model=32,dropout=0.0043,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-18-26/checkpoint_000006)[32m [repeated 12x across cluster][0m
2024-08-26 14:18:41,219	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:41,585	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:41,832	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:42,732	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:43,176	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:43,208	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:43,476	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:44,289	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:44,642	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:44,932	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:45,081	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:45,952	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3036807)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1004/trial-4b632_00006_6_alpha_d_ff=4,batch_size=128,d_core=128,d_model=256,dropout=0.0070,e_layers=2,learning_rate=0.0011,lradj=cosine_2024-08-26_14-18-39/checkpoint_000004)[32m [repeated 12x across cluster][0m
2024-08-26 14:18:46,200	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00088 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00051 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3034560)[0m Epoch: 5 cost time: 1.1096062660217285[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3034560)[0m Epoch: 5, Steps: 265 | Train Loss: 0.3472043 Vali Loss: 0.2218707 Best vali loss: 0.2135380[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3036553)[0m configuration
[36m(_train_fn pid=3036553)[0m {'batch_size': 64, 'learning_rate': 0.0005061420492371454, 'd_model': 256, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0008815980324397751, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=3036553)[0m Use GPU: cuda:0
[36m(_train_fn pid=3036553)[0m train 8449
[36m(_train_fn pid=3036553)[0m val 2785
[36m(_train_fn pid=3034560)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3036553)[0m start_epoch 0
[36m(_train_fn pid=3036553)[0m max_epoch 8

Trial trial-4b632_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00699 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00113 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3034558)[0m Validation loss decreased (0.2352 --> 0.2344).  Saving model state dict ...[32m [repeated 4x across cluster][0m

Trial trial-4b632_00001 completed after 8 iterations at 2024-08-26 14:18:41. Total running time: 15s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.14636 â”‚
â”‚ time_total_s                                13.43277 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.23416 â”‚
â”‚ train_loss                                   0.44304 â”‚
â”‚ valid_loss                                   0.23416 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3036553)[0m Validation loss decreased (inf --> 0.2199).  Saving model state dict ...

Trial trial-4b632_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00199 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00372 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3037113)[0m {'batch_size': 8, 'learning_rate': 0.00372219381766648, 'd_model': 32, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0019897233523587764, 'lradj': 'type1', 'd_ff': 32}
[36m(_train_fn pid=3036553)[0m EarlyStopping counter: 1 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3036807)[0m Updating learning rate to 0.0005668582766628549[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3036807)[0m saving checkpoint...[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3035968)[0m 	iters: 500, epoch: 4 | loss: 0.1994064[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=3035968)[0m 	speed: 0.0028s/iter; left time: 5.9061s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=3036807)[0m Epoch: 4 cost time: 0.6331684589385986[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3036807)[0m Epoch: 4, Steps: 67 | Train Loss: 0.3309244 Vali Loss: 0.2311101 Best vali loss: 0.2208115[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3037113)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3036807)[0m {'batch_size': 128, 'learning_rate': 0.0011337165533257098, 'd_model': 256, 'd_core': 128, 'e_layers': 2, 'dropout': 0.006986883484633005, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=3037113)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3037113)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3037113)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3037113)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3037113)[0m max_epoch 8[32m [repeated 2x across cluster][0m

Trial trial-4b632_00006 completed after 5 iterations at 2024-08-26 14:18:45. Total running time: 19s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                               0.868 â”‚
â”‚ time_total_s                                 4.71318 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22081 â”‚
â”‚ train_loss                                   0.29432 â”‚
â”‚ valid_loss                                    0.2332 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3036807)[0m Early stopping

Trial trial-4b632_00005 completed after 4 iterations at 2024-08-26 14:18:46. Total running time: 19s
2024-08-26 14:18:46,679	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:48,421	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:50,101	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:52,062	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3037963)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1004/trial-4b632_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=128,dropout=0.0010,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_14-18-45/checkpoint_000001)[32m [repeated 5x across cluster][0m
2024-08-26 14:18:53,584	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:53,924	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:55,930	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.55581 â”‚
â”‚ time_total_s                                 6.07964 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21991 â”‚
â”‚ train_loss                                   0.32521 â”‚
â”‚ valid_loss                                   0.22141 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3036807)[0m Validation loss decreased (0.2361 --> 0.2208).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3036807)[0m Validation loss decreased (inf --> 0.2361).  Saving model state dict ...

Trial trial-4b632_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00104 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-4b632_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00291 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00023 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3038003)[0m {'batch_size': 8, 'learning_rate': 0.00022957319104620179, 'd_model': 128, 'd_core': 128, 'e_layers': 4, 'dropout': 0.0029050189276768035, 'lradj': 'type1', 'd_ff': 384}[32m [repeated 2x across cluster][0m

Trial trial-4b632_00004 completed after 6 iterations at 2024-08-26 14:18:48. Total running time: 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.74035 â”‚
â”‚ time_total_s                                11.40221 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.23416 â”‚
â”‚ train_loss                                    0.3153 â”‚
â”‚ valid_loss                                     0.266 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3035968)[0m EarlyStopping counter: 3 out of 3[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3037963)[0m Validation loss decreased (inf --> 0.2279).  Saving model state dict ...
[36m(_train_fn pid=3037963)[0m Updating learning rate to 5.804316262105383e-05[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3037963)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3037113)[0m 	iters: 800, epoch: 1 | loss: 0.3979203[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=3037113)[0m 	speed: 0.0090s/iter; left time: 68.7199s[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=3037963)[0m Epoch: 1 cost time: 1.9081690311431885[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3037963)[0m Epoch: 1, Steps: 265 | Train Loss: 0.4808456 Vali Loss: 0.2279133 Best vali loss: 0.2279133[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3038003)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3038003)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3038003)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3038003)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3038003)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3038003)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3035968)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3037963)[0m Validation loss decreased (0.2279 --> 0.2198).  Saving model state dict ...
[36m(_train_fn pid=3037963)[0m Validation loss decreased (0.2198 --> 0.2187).  Saving model state dict ...
[36m(_train_fn pid=3037113)[0m Validation loss decreased (inf --> 0.2672).  Saving model state dict ...
[36m(_train_fn pid=3037963)[0m Updating learning rate to 1.4510790655263457e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3037963)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3037963)[0m 	iters: 200, epoch: 4 | loss: 0.5048717[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3037963)[0m 	speed: 0.0064s/iter; left time: 7.1717s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3037963)[0m Epoch: 3 cost time: 1.5838968753814697[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3037963)[0m Epoch: 3, Steps: 265 | Train Loss: 0.4134960 Vali Loss: 0.2187366 Best vali loss: 0.2187366[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3037963)[0m Validation loss decreased (0.2187 --> 0.2181).  Saving model state dict ...

Trial status: 7 TERMINATED | 3 RUNNING
Current time: 2024-08-26 14:18:56. Total running time: 30s
Logical resource usage: 3.0/32 CPUs, 0.75/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 4b632_00002 with best_valid_loss=0.213527969291009 and params={'batch_size': 128, 'learning_rate': 8.911579472795504e-05, 'd_model': 512, 'd_core': 128, 'e_layers': 2, 'dropout': 0.007470505338674834, 'lradj': 'cosine', 'd_ff': 2048}
2024-08-26 14:18:57,867	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3037963)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1004/trial-4b632_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=128,dropout=0.0010,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_14-18-45/checkpoint_000004)[32m [repeated 4x across cluster][0m
2024-08-26 14:18:58,774	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:18:59,778	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:01,732	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:03,670	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3037963)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1004/trial-4b632_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=128,dropout=0.0010,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_14-18-45/checkpoint_000007)[32m [repeated 4x across cluster][0m
2024-08-26 14:19:03,864	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:07,643	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-4b632_00007   RUNNING                 8       0.00372219           32              1        512            4   0.00198972    type1          1           10.4596        0.490306       0.267154            0.267154 â”‚
â”‚ trial-4b632_00008   RUNNING                32       5.80432e-05         128              2        512            3   0.00104088    type1          4            8.41534       0.409279       0.218142            0.218142 â”‚
â”‚ trial-4b632_00009   RUNNING                 8       0.000229573         128              3        128            4   0.00290502    type1                                                                                 â”‚
â”‚ trial-4b632_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         4           10.3001        0.362782       0.223901            0.220529 â”‚
â”‚ trial-4b632_00001   TERMINATED             64       9.10485e-05          32              1         32            4   0.00429595    cosine         8           13.4328        0.443043       0.234163            0.234163 â”‚
â”‚ trial-4b632_00002   TERMINATED            128       8.91158e-05         512              4        128            2   0.00747051    cosine         6            7.26498       0.366834       0.213745            0.213528 â”‚
â”‚ trial-4b632_00003   TERMINATED             32       0.000134396         256              2         32            2   0.00192732    cosine         5           11.5356        0.347204       0.221871            0.213538 â”‚
â”‚ trial-4b632_00004   TERMINATED             16       0.00181185          512              3        256            1   0.00644504    type1          6           11.4022        0.315302       0.266003            0.234158 â”‚
â”‚ trial-4b632_00005   TERMINATED             64       0.000506142         256              4         64            3   0.000881598   cosine         4            6.07964       0.32521        0.221405            0.219909 â”‚
â”‚ trial-4b632_00006   TERMINATED            128       0.00113372          256              4        128            2   0.00698688    cosine         5            4.71318       0.294322       0.233195            0.220812 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3037963)[0m Validation loss decreased (0.2181 --> 0.2180).  Saving model state dict ...
[36m(_train_fn pid=3038003)[0m Validation loss decreased (inf --> 0.2196).  Saving model state dict ...
[36m(_train_fn pid=3037963)[0m Validation loss decreased (0.2180 --> 0.2180).  Saving model state dict ...
[36m(_train_fn pid=3037963)[0m Updating learning rate to 1.8138488319079322e-06[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3037963)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3037113)[0m 	iters: 800, epoch: 2 | loss: 0.2063216[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3037113)[0m 	speed: 0.0084s/iter; left time: 55.4817s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3037963)[0m Epoch: 6 cost time: 1.6195883750915527[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3037963)[0m Epoch: 6, Steps: 265 | Train Loss: 0.4057380 Vali Loss: 0.2179850 Best vali loss: 0.2179850[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3037963)[0m Validation loss decreased (0.2180 --> 0.2180).  Saving model state dict ...

Trial trial-4b632_00008 completed after 8 iterations at 2024-08-26 14:19:03. Total running time: 37s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.93454 â”‚
â”‚ time_total_s                                16.14389 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21793 â”‚
â”‚ train_loss                                   0.40331 â”‚
â”‚ valid_loss                                   0.21793 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3037963)[0m Validation loss decreased (0.2180 --> 0.2179).  Saving model state dict ...
[36m(_train_fn pid=3037113)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3037113)[0m Updating learning rate to 0.00186109690883324[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3037113)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3038003)[0m 	iters: 900, epoch: 2 | loss: 0.2671461[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3038003)[0m 	speed: 0.0064s/iter; left time: 41.4269s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3037113)[0m Epoch: 2 cost time: 8.937703609466553[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3037113)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.5176409 Vali Loss: 0.2707282 Best vali loss: 0.2671535[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3038003)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3038003)[0m Updating learning rate to 0.00011478659552310089
[36m(_train_fn pid=3038003)[0m saving checkpoint...
[36m(_train_fn pid=3038003)[0m 	iters: 500, epoch: 3 | loss: 0.5155093[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3038003)[0m 	speed: 0.0061s/iter; left time: 35.8164s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3038003)[0m Epoch: 2 cost time: 7.906052827835083
2024-08-26 14:19:11,432	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3037113)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1004/trial-4b632_00007_7_alpha_d_ff=1,batch_size=8,d_core=512,d_model=32,dropout=0.0020,e_layers=4,learning_rate=0.0037,lradj=type1_2024-08-26_14-18-41/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 14:19:15,379	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:18,999	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3037113)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1004/trial-4b632_00007_7_alpha_d_ff=1,batch_size=8,d_core=512,d_model=32,dropout=0.0020,e_layers=4,learning_rate=0.0037,lradj=type1_2024-08-26_14-18-41/checkpoint_000003)[32m [repeated 2x across cluster][0m
2024-08-26 14:19:21,729	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:21,739	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1004' in 0.0088s.
[36m(_train_fn pid=3038003)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.3934384 Vali Loss: 0.2196808 Best vali loss: 0.2195589
[36m(_train_fn pid=3037113)[0m Updating learning rate to 0.00093054845441662
[36m(_train_fn pid=3037113)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3037113)[0m saving checkpoint...
[36m(_train_fn pid=3037113)[0m Epoch: 3 cost time: 6.621726751327515
[36m(_train_fn pid=3037113)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.5168839 Vali Loss: 0.2714627 Best vali loss: 0.2671535
[36m(_train_fn pid=3038003)[0m 	iters: 100, epoch: 4 | loss: 0.6176374[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3038003)[0m 	speed: 0.0202s/iter; left time: 104.6809s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3038003)[0m Updating learning rate to 5.7393297761550447e-05
[36m(_train_fn pid=3038003)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3038003)[0m saving checkpoint...
[36m(_train_fn pid=3038003)[0m Epoch: 3 cost time: 6.760528326034546
[36m(_train_fn pid=3038003)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.3558286 Vali Loss: 0.2284034 Best vali loss: 0.2195589
[36m(_train_fn pid=3037113)[0m Updating learning rate to 0.00046527422720831
[36m(_train_fn pid=3037113)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3037113)[0m saving checkpoint...
[36m(_train_fn pid=3037113)[0m Epoch: 4 cost time: 6.6137495040893555
[36m(_train_fn pid=3037113)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.5167236 Vali Loss: 0.2715057 Best vali loss: 0.2671535
[36m(_train_fn pid=3037113)[0m Early stopping

Trial trial-4b632_00007 completed after 4 iterations at 2024-08-26 14:19:19. Total running time: 52s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             7.56458 â”‚
â”‚ time_total_s                                35.86678 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.26715 â”‚
â”‚ train_loss                                   0.51672 â”‚
â”‚ valid_loss                                   0.27151 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-4b632_00009 completed after 4 iterations at 2024-08-26 14:19:21. Total running time: 55s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-4b632_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                              6.3476 â”‚
â”‚ time_total_s                                33.97841 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21956 â”‚
â”‚ train_loss                                   0.33402 â”‚
â”‚ valid_loss                                   0.22735 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:19:21. Total running time: 55s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 4b632_00002 with best_valid_loss=0.213527969291009 and params={'batch_size': 128, 'learning_rate': 8.911579472795504e-05, 'd_model': 512, 'd_core': 128, 'e_layers': 2, 'dropout': 0.007470505338674834, 'lradj': 'cosine', 'd_ff': 2048}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-4b632_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         4           10.3001        0.362782       0.223901            0.220529 â”‚
â”‚ trial-4b632_00001   TERMINATED             64       9.10485e-05          32              1         32            4   0.00429595    cosine         8           13.4328        0.443043       0.234163            0.234163 â”‚
â”‚ trial-4b632_00002   TERMINATED            128       8.91158e-05         512              4        128            2   0.00747051    cosine         6            7.26498       0.366834       0.213745            0.213528 â”‚
â”‚ trial-4b632_00003   TERMINATED             32       0.000134396         256              2         32            2   0.00192732    cosine         5           11.5356        0.347204       0.221871            0.213538 â”‚
â”‚ trial-4b632_00004   TERMINATED             16       0.00181185          512              3        256            1   0.00644504    type1          6           11.4022        0.315302       0.266003            0.234158 â”‚
â”‚ trial-4b632_00005   TERMINATED             64       0.000506142         256              4         64            3   0.000881598   cosine         4            6.07964       0.32521        0.221405            0.219909 â”‚
â”‚ trial-4b632_00006   TERMINATED            128       0.00113372          256              4        128            2   0.00698688    cosine         5            4.71318       0.294322       0.233195            0.220812 â”‚
â”‚ trial-4b632_00007   TERMINATED              8       0.00372219           32              1        512            4   0.00198972    type1          4           35.8668        0.516724       0.271506            0.267154 â”‚
â”‚ trial-4b632_00008   TERMINATED             32       5.80432e-05         128              2        512            3   0.00104088    type1          8           16.1439        0.403313       0.217934            0.217934 â”‚
â”‚ trial-4b632_00009   TERMINATED              8       0.000229573         128              3        128            4   0.00290502    type1          4           33.9784        0.334025       0.227346            0.219559 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

[36m(_train_fn pid=3038003)[0m 	iters: 1000, epoch: 4 | loss: 0.3493472[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3038003)[0m 	speed: 0.0044s/iter; left time: 18.6798s[32m [repeated 12x across cluster][0m
Best hyperparameters found were:  {'batch_size': 128, 'learning_rate': 8.911579472795504e-05, 'd_model': 512, 'd_core': 128, 'e_layers': 2, 'dropout': 0.007470505338674834, 'lradj': 'cosine', 'd_ff': 2048}
[36m(_train_fn pid=3038003)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1004/trial-4b632_00009_9_alpha_d_ff=3,batch_size=8,d_core=128,d_model=128,dropout=0.0029,e_layers=4,learning_rate=0.0002,lradj=type1_2024-08-26_14-18-46/checkpoint_000003)
[36m(_train_fn pid=3038003)[0m Updating learning rate to 2.8696648880775223e-05
[36m(_train_fn pid=3038003)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3038003)[0m saving checkpoint...
[36m(_train_fn pid=3038003)[0m Epoch: 4 cost time: 5.732873439788818
[36m(_train_fn pid=3038003)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3340248 Vali Loss: 0.2273456 Best vali loss: 0.2195589
[36m(_train_fn pid=3038003)[0m Early stopping


Time taken (4 parallel trials): 60 seconds


2024-08-26 14:19:25,793	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:19:26,184	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:19:26,189	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:19:26,196	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:19:30,849	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed1005   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator       â”‚
â”‚ Scheduler                        FIFOScheduler               â”‚
â”‚ Number of trials                 10                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-19-25_363611_3039545/artifacts/2024-08-26_14-19-26/ETTh2_96_96_test_seed1005/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:19:26. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-6eeb1_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-6eeb1_00001   PENDING              32       0.00363157           32              3         64            2   0.00767154   type1   â”‚
â”‚ trial-6eeb1_00002   PENDING               8       0.000105545         512              4        128            3   0.00200893   type1   â”‚
â”‚ trial-6eeb1_00003   PENDING              16       0.000109684         512              3        256            4   0.00268458   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-6eeb1_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-6eeb1_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00268 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3041768)[0m configuration
[36m(_train_fn pid=3041768)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=3041768)[0m Use GPU: cuda:0

Trial trial-6eeb1_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00201 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-6eeb1_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00767 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00363 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3041768)[0m train 8449
[36m(_train_fn pid=3041769)[0m {'batch_size': 32, 'learning_rate': 0.0036315672349473502, 'd_model': 32, 'd_core': 64, 'e_layers': 2, 'dropout': 0.007671537520346806, 'lradj': 'type1', 'd_ff': 96}
[36m(_train_fn pid=3041768)[0m val 2785
[36m(_train_fn pid=3041768)[0m start_epoch 0
[36m(_train_fn pid=3041768)[0m max_epoch 8
[36m(_train_fn pid=3041768)[0m 	iters: 100, epoch: 1 | loss: 0.5038708
[36m(_train_fn pid=3041768)[0m 	speed: 0.0122s/iter; left time: 24.6017s
[36m(_train_fn pid=3041768)[0m Updating learning rate to 0.000288581929876693
[36m(_train_fn pid=3041768)[0m saving checkpoint...
[36m(_train_fn pid=3041768)[0m Validation loss decreased (inf --> 0.2182).  Saving model state dict ...
[36m(_train_fn pid=3041768)[0m Epoch: 1 cost time: 2.2763984203338623
[36m(_train_fn pid=3041768)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00000_0_2024-08-26_14-19-26/checkpoint_000000)
2024-08-26 14:19:31,089	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:33,479	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:33,731	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:36,052	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3041768)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00000_0_2024-08-26_14-19-26/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 14:19:36,293	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:37,310	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:38,406	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:38,691	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:40,978	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:41,196	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3041769)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00001_1_alpha_d_ff=3,batch_size=32,d_core=64,d_model=32,dropout=0.0077,e_layers=2,learning_rate=0.0036,lradj=type1_2024-08-26_14-19-26/checkpoint_000004)[32m [repeated 6x across cluster][0m
2024-08-26 14:19:42,381	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:43,389	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:44,395	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3041768)[0m Epoch: 1, Steps: 265 | Train Loss: 0.4391879 Vali Loss: 0.2181639 Best vali loss: 0.2181639
[36m(_train_fn pid=3041770)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=3041771)[0m {'batch_size': 16, 'learning_rate': 0.00010968444518237043, 'd_model': 512, 'd_core': 256, 'e_layers': 4, 'dropout': 0.0026845813122190255, 'lradj': 'cosine', 'd_ff': 1536}
[36m(_train_fn pid=3041770)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3041771)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3041770)[0m {'batch_size': 8, 'learning_rate': 0.0001055454248805599, 'd_model': 512, 'd_core': 128, 'e_layers': 3, 'dropout': 0.0020089325462693216, 'lradj': 'type1', 'd_ff': 2048}
[36m(_train_fn pid=3041771)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3041771)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3041771)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3041768)[0m Validation loss decreased (0.2182 --> 0.2145).  Saving model state dict ...
[36m(_train_fn pid=3041769)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3041771)[0m 	iters: 400, epoch: 1 | loss: 0.6053497[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3041771)[0m 	speed: 0.0137s/iter; left time: 52.6044s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3041768)[0m Updating learning rate to 0.00020740251485476345[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3041768)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3041769)[0m Validation loss decreased (inf --> 0.2241).  Saving model state dict ...
[36m(_train_fn pid=3041768)[0m Epoch: 3 cost time: 2.1806957721710205[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3041768)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3864687 Vali Loss: 0.2205027 Best vali loss: 0.2145283[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3041771)[0m Validation loss decreased (inf --> 0.2203).  Saving model state dict ...
[36m(_train_fn pid=3041769)[0m Validation loss decreased (0.2241 --> 0.2211).  Saving model state dict ...
[36m(_train_fn pid=3041769)[0m EarlyStopping counter: 1 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3041769)[0m 	iters: 100, epoch: 5 | loss: 0.6232226[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=3041769)[0m 	speed: 0.0180s/iter; left time: 17.3197s[32m [repeated 17x across cluster][0m

Trial trial-6eeb1_00000 completed after 5 iterations at 2024-08-26 14:19:40. Total running time: 14s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             2.57037 â”‚
â”‚ time_total_s                                13.21683 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21453 â”‚
â”‚ train_loss                                   0.34837 â”‚
â”‚ valid_loss                                   0.22581 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3041768)[0m Early stopping
[36m(_train_fn pid=3041769)[0m Updating learning rate to 0.0002269729521842094[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3041769)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3041769)[0m Epoch: 5 cost time: 2.162952423095703[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3041769)[0m Epoch: 5, Steps: 265 | Train Loss: 0.3328592 Vali Loss: 0.2308044 Best vali loss: 0.2210771[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3041770)[0m Validation loss decreased (inf --> 0.2207).  Saving model state dict ...

Trial trial-6eeb1_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00761 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00154 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3043123)[0m configuration
[36m(_train_fn pid=3043123)[0m {'batch_size': 64, 'learning_rate': 0.0015393773960695272, 'd_model': 512, 'd_core': 32, 'e_layers': 4, 'dropout': 0.007611693265995761, 'lradj': 'type1', 'd_ff': 1536}
[36m(_train_fn pid=3043123)[0m Use GPU: cuda:0
[36m(_train_fn pid=3043123)[0m train 8449
[36m(_train_fn pid=3043123)[0m val 2785
[36m(_train_fn pid=3043123)[0m start_epoch 0
[36m(_train_fn pid=3043123)[0m max_epoch 8

Trial trial-6eeb1_00001 completed after 6 iterations at 2024-08-26 14:19:43. Total running time: 17s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             2.19109 â”‚
â”‚ time_total_s                                15.56955 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22108 â”‚
â”‚ train_loss                                   0.32369 â”‚
â”‚ valid_loss                                   0.23244 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3041771)[0m EarlyStopping counter: 1 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3041770)[0m 	iters: 300, epoch: 2 | loss: 0.3383341[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3041770)[0m 	speed: 0.0068s/iter; left time: 48.2137s[32m [repeated 12x across cluster][0m

Trial trial-6eeb1_00005 started with configuration:
2024-08-26 14:19:45,857	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:48,373	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3043123)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00004_4_alpha_d_ff=3,batch_size=64,d_core=32,d_model=512,dropout=0.0076,e_layers=4,learning_rate=0.0015,lradj=type1_2024-08-26_14-19-40/checkpoint_000001)[32m [repeated 5x across cluster][0m
2024-08-26 14:19:50,947	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:51,614	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:53,531	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3043123)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00004_4_alpha_d_ff=3,batch_size=64,d_core=32,d_model=512,dropout=0.0076,e_layers=4,learning_rate=0.0015,lradj=type1_2024-08-26_14-19-40/checkpoint_000003)[32m [repeated 3x across cluster][0m
2024-08-26 14:19:53,891	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:56,055	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:19:58,570	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3043123)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00004_4_alpha_d_ff=3,batch_size=64,d_core=32,d_model=512,dropout=0.0076,e_layers=4,learning_rate=0.0015,lradj=type1_2024-08-26_14-19-40/checkpoint_000005)[32m [repeated 3x across cluster][0m
2024-08-26 14:19:58,698	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:00,159	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00093 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3043123)[0m Validation loss decreased (inf --> 0.2714).  Saving model state dict ...
[36m(_train_fn pid=3041769)[0m Early stopping
[36m(_train_fn pid=3043123)[0m Updating learning rate to 0.0015393773960695272[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3043123)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3043123)[0m Epoch: 1 cost time: 2.5450260639190674[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3043123)[0m Epoch: 1, Steps: 133 | Train Loss: 0.4979567 Vali Loss: 0.2713913 Best vali loss: 0.2713913[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3043356)[0m configuration
[36m(_train_fn pid=3043356)[0m {'batch_size': 8, 'learning_rate': 5.542581094178534e-05, 'd_model': 128, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0009294048939907084, 'lradj': 'type1', 'd_ff': 256}
[36m(_train_fn pid=3043356)[0m Use GPU: cuda:0
[36m(_train_fn pid=3043356)[0m train 8449
[36m(_train_fn pid=3043356)[0m val 2785
[36m(_train_fn pid=3043356)[0m start_epoch 0
[36m(_train_fn pid=3043356)[0m max_epoch 8
[36m(_train_fn pid=3043123)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3043123)[0m 	iters: 100, epoch: 3 | loss: 0.3761734[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3043123)[0m 	speed: 0.0252s/iter; left time: 17.5835s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3043123)[0m Validation loss decreased (0.2714 --> 0.2644).  Saving model state dict ...
[36m(_train_fn pid=3041771)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3041771)[0m Updating learning rate to 7.582943257091025e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3041771)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3041771)[0m Epoch: 3 cost time: 6.235420227050781[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3041771)[0m Epoch: 3, Steps: 529 | Train Loss: 0.3353949 Vali Loss: 0.2243971 Best vali loss: 0.2202656[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3043123)[0m Validation loss decreased (0.2644 --> 0.2643).  Saving model state dict ...
[36m(_train_fn pid=3043123)[0m 	iters: 100, epoch: 5 | loss: 0.5476007[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3043123)[0m 	speed: 0.0257s/iter; left time: 11.1492s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3041770)[0m Validation loss decreased (0.2207 --> 0.2190).  Saving model state dict ...

Trial status: 2 TERMINATED | 4 RUNNING
Current time: 2024-08-26 14:19:56. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 6eeb1_00000 with best_valid_loss=0.2145282702985315 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-6eeb1_00002   RUNNING                 8       0.000105545         512              4        128            3   0.00200893    type1          2            26.0854       0.382216       0.218988            0.218988 â”‚
â”‚ trial-6eeb1_00003   RUNNING                16       0.000109684         512              3        256            4   0.00268458    cosine         3            23.8391       0.335395       0.224397            0.220266 â”‚
â”‚ trial-6eeb1_00004   RUNNING                64       0.00153938          512              3         32            4   0.00761169    type1          5            13.472        0.503381       0.267139            0.264269 â”‚
â”‚ trial-6eeb1_00005   RUNNING                 8       5.54258e-05         128              2        512            4   0.000929405   type1                                                                                 â”‚
â”‚ trial-6eeb1_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5            13.2168       0.348367       0.225813            0.214528 â”‚
â”‚ trial-6eeb1_00001   TERMINATED             32       0.00363157           32              3         64            2   0.00767154    type1          6            15.5695       0.323686       0.232436            0.221077 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3043123)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3043123)[0m Updating learning rate to 9.621108725434545e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3043123)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3043123)[0m Epoch: 5 cost time: 2.2992424964904785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3043123)[0m Epoch: 5, Steps: 133 | Train Loss: 0.5033811 Vali Loss: 0.2671394 Best vali loss: 0.2642693[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3041771)[0m Validation loss decreased (0.2203 --> 0.2156).  Saving model state dict ...
[36m(_train_fn pid=3043123)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3043356)[0m Validation loss decreased (inf --> 0.2208).  Saving model state dict ...
2024-08-26 14:20:01,081	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3041771)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00003_3_alpha_d_ff=3,batch_size=16,d_core=256,d_model=512,dropout=0.0027,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-19-26/checkpoint_000004)[32m [repeated 4x across cluster][0m
2024-08-26 14:20:07,026	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:10,152	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3041771)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00003_3_alpha_d_ff=3,batch_size=16,d_core=256,d_model=512,dropout=0.0027,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-19-26/checkpoint_000005)[32m [repeated 3x across cluster][0m
2024-08-26 14:20:17,525	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:18,531	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3041770)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00002_2_alpha_d_ff=4,batch_size=8,d_core=128,d_model=512,dropout=0.0020,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_14-19-26/checkpoint_000003)[32m [repeated 3x across cluster][0m
2024-08-26 14:20:23,193	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:24,352	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:25,971	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3043123)[0m 	iters: 100, epoch: 7 | loss: 0.3917129[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3043123)[0m 	speed: 0.0251s/iter; left time: 4.1892s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3043123)[0m EarlyStopping counter: 3 out of 3

Trial trial-6eeb1_00004 completed after 7 iterations at 2024-08-26 14:20:01. Total running time: 34s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             2.50873 â”‚
â”‚ time_total_s                                18.49208 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26427 â”‚
â”‚ train_loss                                   0.53296 â”‚
â”‚ valid_loss                                   0.26433 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3043123)[0m Early stopping
[36m(_train_fn pid=3043123)[0m Updating learning rate to 2.4052771813586362e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3043123)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3043123)[0m Epoch: 7 cost time: 2.296902656555176[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3043123)[0m Epoch: 7, Steps: 133 | Train Loss: 0.5329573 Vali Loss: 0.2643340 Best vali loss: 0.2642693[32m [repeated 4x across cluster][0m

Trial trial-6eeb1_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00142 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00081 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3044365)[0m configuration
[36m(_train_fn pid=3044365)[0m {'batch_size': 16, 'learning_rate': 0.0008054350462599405, 'd_model': 512, 'd_core': 128, 'e_layers': 2, 'dropout': 0.0014223997558608875, 'lradj': 'type1', 'd_ff': 1536}
[36m(_train_fn pid=3044365)[0m Use GPU: cuda:0
[36m(_train_fn pid=3044365)[0m train 8449
[36m(_train_fn pid=3044365)[0m val 2785
[36m(_train_fn pid=3044365)[0m start_epoch 0
[36m(_train_fn pid=3044365)[0m max_epoch 8
[36m(_train_fn pid=3043356)[0m 	iters: 400, epoch: 2 | loss: 0.3250121[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3043356)[0m 	speed: 0.0152s/iter; left time: 106.6886s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3041771)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3041770)[0m Updating learning rate to 2.6386356220139975e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3041770)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3041770)[0m Epoch: 3 cost time: 10.84281325340271[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3041770)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.3249955 Vali Loss: 0.2207885 Best vali loss: 0.2189875[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3044365)[0m Validation loss decreased (inf --> 0.2639).  Saving model state dict ...
[36m(_train_fn pid=3041770)[0m 	iters: 300, epoch: 4 | loss: 0.2410596[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3041770)[0m 	speed: 0.0125s/iter; left time: 62.3861s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3041770)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3044365)[0m Updating learning rate to 0.0008054350462599405
[36m(_train_fn pid=3044365)[0m saving checkpoint...
[36m(_train_fn pid=3044365)[0m Epoch: 1 cost time: 5.792773962020874
[36m(_train_fn pid=3044365)[0m Epoch: 1, Steps: 529 | Train Loss: 0.4829561 Vali Loss: 0.2639222 Best vali loss: 0.2639222
[36m(_train_fn pid=3044365)[0m 	iters: 500, epoch: 2 | loss: 0.3911743[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3044365)[0m 	speed: 0.0110s/iter; left time: 35.2796s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3041771)[0m Updating learning rate to 1.6062915101616082e-05
[36m(_train_fn pid=3041771)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3041771)[0m saving checkpoint...
[36m(_train_fn pid=3041771)[0m Epoch: 6 cost time: 8.698508739471436
[36m(_train_fn pid=3041771)[0m Epoch: 6, Steps: 529 | Train Loss: 0.2584498 Vali Loss: 0.2280991 Best vali loss: 0.2156321
[36m(_train_fn pid=3044365)[0m Validation loss decreased (0.2639 --> 0.2403).  Saving model state dict ...
[36m(_train_fn pid=3041771)[0m 	iters: 300, epoch: 7 | loss: 0.3137553[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=3041771)[0m 	speed: 0.0155s/iter; left time: 11.7561s[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=3043356)[0m Updating learning rate to 2.771290547089267e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3043356)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3043356)[0m Epoch: 2 cost time: 15.821184396743774[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3043356)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.4137243 Vali Loss: 0.2204141 Best vali loss: 0.2204141[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3043356)[0m Validation loss decreased (0.2208 --> 0.2204).  Saving model state dict ...
[36m(_train_fn pid=3041770)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3044365)[0m Validation loss decreased (0.2403 --> 0.2274).  Saving model state dict ...

Trial trial-6eeb1_00003 completed after 7 iterations at 2024-08-26 14:20:25. Total running time: 59s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             9.30559 â”‚
â”‚ time_total_s                                58.17761 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.21563 â”‚
â”‚ train_loss                                   0.24726 â”‚
â”‚ valid_loss                                   0.23234 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3041771)[0m Early stopping

Trial status: 4 TERMINATED | 3 RUNNING | 1 PENDING
Current time: 2024-08-26 14:20:26. Total running time: 1min 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 6eeb1_00000 with best_valid_loss=0.2145282702985315 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
2024-08-26 14:20:29,395	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3045174)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00007_7_alpha_d_ff=1,batch_size=128,d_core=128,d_model=512,dropout=0.0033,e_layers=4,learning_rate=0.0005,lradj=cosine_2024-08-26_14-20-25/checkpoint_000000)[32m [repeated 3x across cluster][0m
2024-08-26 14:20:29,847	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:30,547	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:31,697	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:32,814	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:33,927	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:34,422	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:34,456	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:34,608	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-6eeb1_00002   RUNNING                 8       0.000105545         512              4        128            3   0.00200893    type1          4            55.376        0.292807       0.220782            0.218988 â”‚
â”‚ trial-6eeb1_00005   RUNNING                 8       5.54258e-05         128              2        512            4   0.000929405   type1          2            33.6346       0.413724       0.220414            0.220414 â”‚
â”‚ trial-6eeb1_00006   RUNNING                16       0.000805435         512              3        128            2   0.0014224     type1          3            21.7679       0.423125       0.227417            0.227417 â”‚
â”‚ trial-6eeb1_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5            13.2168       0.348367       0.225813            0.214528 â”‚
â”‚ trial-6eeb1_00001   TERMINATED             32       0.00363157           32              3         64            2   0.00767154    type1          6            15.5695       0.323686       0.232436            0.221077 â”‚
â”‚ trial-6eeb1_00003   TERMINATED             16       0.000109684         512              3        256            4   0.00268458    cosine         7            58.1776       0.247256       0.232342            0.215632 â”‚
â”‚ trial-6eeb1_00004   TERMINATED             64       0.00153938          512              3         32            4   0.00761169    type1          7            18.4921       0.532957       0.264334            0.264269 â”‚
â”‚ trial-6eeb1_00007   PENDING               128       0.000453106         512              1        128            4   0.00325847    cosine                                                                                â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3041770)[0m 	iters: 300, epoch: 5 | loss: 0.2668417[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3041770)[0m 	speed: 0.0099s/iter; left time: 38.9531s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3041771)[0m Updating learning rate to 4.17461562176109e-06[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3041771)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3041771)[0m Epoch: 7 cost time: 8.004539966583252[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3041771)[0m Epoch: 7, Steps: 529 | Train Loss: 0.2472563 Vali Loss: 0.2323421 Best vali loss: 0.2156321[32m [repeated 3x across cluster][0m

Trial trial-6eeb1_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00326 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00045 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3045174)[0m configuration
[36m(_train_fn pid=3045174)[0m {'batch_size': 128, 'learning_rate': 0.0004531058755261338, 'd_model': 512, 'd_core': 128, 'e_layers': 4, 'dropout': 0.003258471756653313, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=3045174)[0m Use GPU: cuda:0
[36m(_train_fn pid=3045174)[0m train 8449
[36m(_train_fn pid=3045174)[0m val 2785
[36m(_train_fn pid=3045174)[0m start_epoch 0
[36m(_train_fn pid=3045174)[0m max_epoch 8
[36m(_train_fn pid=3041771)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3045174)[0m Validation loss decreased (inf --> 0.2326).  Saving model state dict ...
[36m(_train_fn pid=3044365)[0m Validation loss decreased (0.2274 --> 0.2256).  Saving model state dict ...
[36m(_train_fn pid=3044365)[0m 	iters: 200, epoch: 5 | loss: 0.2435734[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3044365)[0m 	speed: 0.0079s/iter; left time: 15.1324s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3045174)[0m Updating learning rate to 0.0003132509935986319[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3045174)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3045174)[0m Epoch: 3 cost time: 0.9557487964630127[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3045174)[0m Epoch: 3, Steps: 67 | Train Loss: 0.3233560 Vali Loss: 0.2256849 Best vali loss: 0.2256849[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3045174)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3045174)[0m EarlyStopping counter: 2 out of 3

Trial trial-6eeb1_00002 completed after 5 iterations at 2024-08-26 14:20:34. Total running time: 1min 8s
[36m(_train_fn pid=3041770)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00002_2_alpha_d_ff=4,batch_size=8,d_core=128,d_model=512,dropout=0.0020,e_layers=3,learning_rate=0.0001,lradj=type1_2024-08-26_14-19-26/checkpoint_000004)[32m [repeated 8x across cluster][0m
2024-08-26 14:20:35,037	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:37,472	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:38,129	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:38,237	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:38,821	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:39,536	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:40,245	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3045926)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00008_8_alpha_d_ff=2,batch_size=128,d_core=128,d_model=256,dropout=0.0074,e_layers=2,learning_rate=0.0001,lradj=type1_2024-08-26_14-20-34/checkpoint_000004)[32m [repeated 7x across cluster][0m
2024-08-26 14:20:40,942	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:41,644	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:43,449	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:43,808	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                            11.41301 â”‚
â”‚ time_total_s                                66.78899 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21899 â”‚
â”‚ train_loss                                   0.27775 â”‚
â”‚ valid_loss                                   0.22032 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3041770)[0m Early stopping
[36m(_train_fn pid=3044365)[0m Validation loss decreased (0.2256 --> 0.2209).  Saving model state dict ...[32m [repeated 4x across cluster][0m

Trial trial-6eeb1_00007 completed after 6 iterations at 2024-08-26 14:20:35. Total running time: 1min 8s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                              1.1058 â”‚
â”‚ time_total_s                                 7.48249 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22568 â”‚
â”‚ train_loss                                   0.25694 â”‚
â”‚ valid_loss                                   0.23257 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-6eeb1_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00743 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00007 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3045926)[0m configuration
[36m(_train_fn pid=3045926)[0m {'batch_size': 128, 'learning_rate': 7.230229153042438e-05, 'd_model': 256, 'd_core': 128, 'e_layers': 2, 'dropout': 0.007431181259537102, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=3045926)[0m Use GPU: cuda:0
[36m(_train_fn pid=3045926)[0m train 8449
[36m(_train_fn pid=3045926)[0m val 2785

Trial trial-6eeb1_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00768 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00008 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3045926)[0m start_epoch 0
[36m(_train_fn pid=3045926)[0m max_epoch 8
[36m(_train_fn pid=3046022)[0m {'batch_size': 16, 'learning_rate': 8.170056099872747e-05, 'd_model': 32, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0076827771965463365, 'lradj': 'cosine', 'd_ff': 32}
[36m(_train_fn pid=3044365)[0m 	iters: 500, epoch: 6 | loss: 0.1925740[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3044365)[0m 	speed: 0.0055s/iter; left time: 5.9977s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3045926)[0m Validation loss decreased (inf --> 0.2326).  Saving model state dict ...
[36m(_train_fn pid=3045926)[0m Updating learning rate to 7.230229153042438e-05[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3045926)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3045926)[0m Epoch: 1 cost time: 0.784308910369873[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3045926)[0m Epoch: 1, Steps: 67 | Train Loss: 0.4898387 Vali Loss: 0.2325946 Best vali loss: 0.2325946[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3045174)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3045174)[0m Early stopping
[36m(_train_fn pid=3045926)[0m Validation loss decreased (0.2190 --> 0.2187).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3046022)[0m configuration
[36m(_train_fn pid=3046022)[0m Use GPU: cuda:0
[36m(_train_fn pid=3046022)[0m train 8449
[36m(_train_fn pid=3046022)[0m val 2785

Trial trial-6eeb1_00008 completed after 7 iterations at 2024-08-26 14:20:41. Total running time: 1min 15s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             0.69968 â”‚
â”‚ time_total_s                                 5.50143 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.21865 â”‚
â”‚ train_loss                                   0.40775 â”‚
â”‚ valid_loss                                   0.21869 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3045926)[0m Early stopping
[36m(_train_fn pid=3046022)[0m start_epoch 0
[36m(_train_fn pid=3046022)[0m max_epoch 8
[36m(_train_fn pid=3046022)[0m 	iters: 400, epoch: 1 | loss: 0.3417255[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3046022)[0m 	speed: 0.0111s/iter; left time: 42.3600s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3045926)[0m Updating learning rate to 1.129723305162881e-06[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3045926)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3045926)[0m Epoch: 7 cost time: 0.56304931640625[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3045926)[0m Epoch: 7, Steps: 67 | Train Loss: 0.4077537 Vali Loss: 0.2186887 Best vali loss: 0.2186536[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3045926)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
2024-08-26 14:20:47,265	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3044365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00006_6_alpha_d_ff=3,batch_size=16,d_core=128,d_model=512,dropout=0.0014,e_layers=2,learning_rate=0.0008,lradj=type1_2024-08-26_14-20-01/checkpoint_000007)[32m [repeated 5x across cluster][0m
2024-08-26 14:20:47,322	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:48,255	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:51,816	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:20:54,438	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3043356)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00005_5_alpha_d_ff=2,batch_size=8,d_core=512,d_model=128,dropout=0.0009,e_layers=4,learning_rate=0.0001,lradj=type1_2024-08-26_14-19-43/checkpoint_000004)[32m [repeated 4x across cluster][0m
2024-08-26 14:20:55,158	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3046022)[0m Validation loss decreased (inf --> 0.2501).  Saving model state dict ...
[36m(_train_fn pid=3044365)[0m Validation loss decreased (0.2209 --> 0.2206).  Saving model state dict ...

Trial trial-6eeb1_00006 completed after 8 iterations at 2024-08-26 14:20:47. Total running time: 1min 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             3.80967 â”‚
â”‚ time_total_s                                44.65506 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22059 â”‚
â”‚ train_loss                                   0.34486 â”‚
â”‚ valid_loss                                   0.22132 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3046022)[0m 	iters: 400, epoch: 2 | loss: 0.2074503[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3046022)[0m 	speed: 0.0066s/iter; left time: 21.9633s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3043356)[0m Validation loss decreased (0.2196 --> 0.2186).  Saving model state dict ...
[36m(_train_fn pid=3043356)[0m Updating learning rate to 6.9282263677231676e-06[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3043356)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3043356)[0m Epoch: 4 cost time: 11.422616481781006[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3043356)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3848321 Vali Loss: 0.2186305 Best vali loss: 0.2186305[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3044365)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3046022)[0m 	iters: 100, epoch: 4 | loss: 0.3857318[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3046022)[0m 	speed: 0.0128s/iter; left time: 32.5192s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3046022)[0m Validation loss decreased (0.2342 --> 0.2280).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3046022)[0m Updating learning rate to 5.648300605393695e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3046022)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3046022)[0m Epoch: 3 cost time: 3.0573482513427734[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3046022)[0m Epoch: 3, Steps: 529 | Train Loss: 0.4391133 Vali Loss: 0.2280006 Best vali loss: 0.2280006[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3043356)[0m EarlyStopping counter: 1 out of 3

Trial status: 8 TERMINATED | 2 RUNNING
Current time: 2024-08-26 14:20:56. Total running time: 1min 30s
Logical resource usage: 2.0/32 CPUs, 0.5/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 6eeb1_00000 with best_valid_loss=0.2145282702985315 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-6eeb1_00005   RUNNING                 8       5.54258e-05         128              2        512            4   0.000929405   type1          5           69.5311        0.380082       0.219244            0.218631 â”‚
â”‚ trial-6eeb1_00009   RUNNING                16       8.17006e-05          32              1         64            3   0.00768278    cosine         4           18.6352        0.42975        0.223897            0.223897 â”‚
â”‚ trial-6eeb1_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5           13.2168        0.348367       0.225813            0.214528 â”‚
â”‚ trial-6eeb1_00001   TERMINATED             32       0.00363157           32              3         64            2   0.00767154    type1          6           15.5695        0.323686       0.232436            0.221077 â”‚
â”‚ trial-6eeb1_00002   TERMINATED              8       0.000105545         512              4        128            3   0.00200893    type1          5           66.789         0.277751       0.220317            0.218988 â”‚
â”‚ trial-6eeb1_00003   TERMINATED             16       0.000109684         512              3        256            4   0.00268458    cosine         7           58.1776        0.247256       0.232342            0.215632 â”‚
â”‚ trial-6eeb1_00004   TERMINATED             64       0.00153938          512              3         32            4   0.00761169    type1          7           18.4921        0.532957       0.264334            0.264269 â”‚
â”‚ trial-6eeb1_00006   TERMINATED             16       0.000805435         512              3        128            2   0.0014224     type1          8           44.6551        0.344856       0.221319            0.220595 â”‚
â”‚ trial-6eeb1_00007   TERMINATED            128       0.000453106         512              1        128            4   0.00325847    cosine         6            7.48249       0.25694        0.232567            0.225685 â”‚
â”‚ trial-6eeb1_00008   TERMINATED            128       7.23023e-05         256              2        128            2   0.00743118    type1          7            5.50143       0.407754       0.218689            0.218654 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3043356)[0m 	iters: 600, epoch: 6 | loss: 0.5098986[32m [repeated 16x across cluster][0m
2024-08-26 14:20:58,700	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:01,577	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3043356)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00005_5_alpha_d_ff=2,batch_size=8,d_core=512,d_model=128,dropout=0.0009,e_layers=4,learning_rate=0.0001,lradj=type1_2024-08-26_14-19-43/checkpoint_000005)[32m [repeated 3x across cluster][0m
2024-08-26 14:21:01,880	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:04,195	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:06,603	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:06,614	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005' in 0.0018s.
[36m(_train_fn pid=3043356)[0m 	speed: 0.0059s/iter; left time: 15.1530s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3046022)[0m Validation loss decreased (0.2280 --> 0.2239).  Saving model state dict ...
[36m(_train_fn pid=3046022)[0m Updating learning rate to 4.085028049936374e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3046022)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3046022)[0m Epoch: 4 cost time: 2.836045742034912[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3046022)[0m Epoch: 4, Steps: 529 | Train Loss: 0.4297503 Vali Loss: 0.2238967 Best vali loss: 0.2238967[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3046022)[0m Validation loss decreased (0.2239 --> 0.2221).  Saving model state dict ...

Trial trial-6eeb1_00005 completed after 6 iterations at 2024-08-26 14:21:01. Total running time: 1min 35s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             7.13623 â”‚
â”‚ time_total_s                                76.66733 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21863 â”‚
â”‚ train_loss                                    0.3769 â”‚
â”‚ valid_loss                                   0.21951 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3043356)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3046022)[0m Validation loss decreased (0.2221 --> 0.2215).  Saving model state dict ...
[36m(_train_fn pid=3046022)[0m 	iters: 300, epoch: 7 | loss: 0.2498383[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3046022)[0m 	speed: 0.0036s/iter; left time: 2.7336s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3046022)[0m Updating learning rate to 1.1964770144891056e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3046022)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3046022)[0m Epoch: 6 cost time: 2.8653082847595215[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3046022)[0m Epoch: 6, Steps: 529 | Train Loss: 0.4215024 Vali Loss: 0.2214804 Best vali loss: 0.2214804[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3046022)[0m Validation loss decreased (0.2215 --> 0.2211).  Saving model state dict ...
[36m(_train_fn pid=3046022)[0m Validation loss decreased (0.2211 --> 0.2210).  Saving model state dict ...

Trial trial-6eeb1_00009 completed after 8 iterations at 2024-08-26 14:21:06. Total running time: 1min 40s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-6eeb1_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             2.40582 â”‚
â”‚ time_total_s                                30.07073 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22099 â”‚
â”‚ train_loss                                   0.41911 â”‚
â”‚ valid_loss                                   0.22099 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:21:06. Total running time: 1min 40s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 6eeb1_00000 with best_valid_loss=0.2145282702985315 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-6eeb1_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5           13.2168        0.348367       0.225813            0.214528 â”‚
â”‚ trial-6eeb1_00001   TERMINATED             32       0.00363157           32              3         64            2   0.00767154    type1          6           15.5695        0.323686       0.232436            0.221077 â”‚
â”‚ trial-6eeb1_00002   TERMINATED              8       0.000105545         512              4        128            3   0.00200893    type1          5           66.789         0.277751       0.220317            0.218988 â”‚
â”‚ trial-6eeb1_00003   TERMINATED             16       0.000109684         512              3        256            4   0.00268458    cosine         7           58.1776        0.247256       0.232342            0.215632 â”‚
â”‚ trial-6eeb1_00004   TERMINATED             64       0.00153938          512              3         32            4   0.00761169    type1          7           18.4921        0.532957       0.264334            0.264269 â”‚
â”‚ trial-6eeb1_00005   TERMINATED              8       5.54258e-05         128              2        512            4   0.000929405   type1          6           76.6673        0.376901       0.219514            0.218631 â”‚
â”‚ trial-6eeb1_00006   TERMINATED             16       0.000805435         512              3        128            2   0.0014224     type1          8           44.6551        0.344856       0.221319            0.220595 â”‚
â”‚ trial-6eeb1_00007   TERMINATED            128       0.000453106         512              1        128            4   0.00325847    cosine         6            7.48249       0.25694        0.232567            0.225685 â”‚
â”‚ trial-6eeb1_00008   TERMINATED            128       7.23023e-05         256              2        128            2   0.00743118    type1          7            5.50143       0.407754       0.218689            0.218654 â”‚
â”‚ trial-6eeb1_00009   TERMINATED             16       8.17006e-05          32              1         64            3   0.00768278    cosine         8           30.0707        0.419111       0.220993            0.220993 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=3046022)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1005/trial-6eeb1_00009_9_alpha_d_ff=1,batch_size=16,d_core=64,d_model=32,dropout=0.0077,e_layers=3,learning_rate=0.0001,lradj=cosine_2024-08-26_14-20-35/checkpoint_000007)[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3046022)[0m 	iters: 500, epoch: 8 | loss: 0.1682414[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3046022)[0m 	speed: 0.0040s/iter; left time: 0.1186s[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3046022)[0m Updating learning rate to 0.0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3046022)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3046022)[0m Epoch: 8 cost time: 2.1056926250457764[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3046022)[0m Epoch: 8, Steps: 529 | Train Loss: 0.4191107 Vali Loss: 0.2209931 Best vali loss: 0.2209931[32m [repeated 2x across cluster][0m


Time taken (4 parallel trials): 104 seconds


2024-08-26 14:21:10,681	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:21:11,060	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:21:11,066	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:21:11,073	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed1006   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator       â”‚
â”‚ Scheduler                        FIFOScheduler               â”‚
â”‚ Number of trials                 10                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1006
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-21-10_147179_3047725/artifacts/2024-08-26_14-21-11/ETTh2_96_96_test_seed1006/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:21:11. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-ad5fc_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-ad5fc_00001   PENDING              32       0.00079558           64              3         64            2   0.00697738   cosine  â”‚
â”‚ trial-ad5fc_00002   PENDING              16       0.000323979          64              1         64            4   0.00300701   type1   â”‚
â”‚ trial-ad5fc_00003   PENDING              32       0.00174864          512              2         32            4   0.00688669   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ad5fc_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00689 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00175 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ad5fc_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00301 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00032 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ad5fc_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-ad5fc_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00698 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                       0.0008 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3049948)[0m configuration
[36m(_train_fn pid=3049948)[0m {'batch_size': 32, 'learning_rate': 0.0007955800641673679, 'd_model': 64, 'd_core': 64, 'e_layers': 2, 'dropout': 0.006977379165277406, 'lradj': 'cosine', 'd_ff': 192}
[36m(_train_fn pid=3049947)[0m Use GPU: cuda:0
[36m(_train_fn pid=3049949)[0m {'batch_size': 16, 'learning_rate': 0.000323978942024507, 'd_model': 64, 'd_core': 64, 'e_layers': 4, 'dropout': 0.0030070085825686568, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=3049948)[0m train 8449
[36m(_train_fn pid=3049948)[0m val 2785
[36m(_train_fn pid=3049948)[0m start_epoch 0
[36m(_train_fn pid=3049948)[0m max_epoch 8
[36m(_train_fn pid=3049948)[0m 	iters: 100, epoch: 1 | loss: 0.2554667
[36m(_train_fn pid=3049948)[0m 	speed: 0.0133s/iter; left time: 26.7842s
[36m(_train_fn pid=3049948)[0m Updating learning rate to 0.0007653001009628076
[36m(_train_fn pid=3049948)[0m saving checkpoint...
[36m(_train_fn pid=3049948)[0m Validation loss decreased (inf --> 0.2191).  Saving model state dict ...
[36m(_train_fn pid=3049948)[0m Epoch: 1 cost time: 2.315915584564209
2024-08-26 14:21:15,851	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3049948)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1006/trial-ad5fc_00001_1_alpha_d_ff=3,batch_size=32,d_core=64,d_model=64,dropout=0.0070,e_layers=2,learning_rate=0.0008,lradj=cosine_2024-08-26_14-21-11/checkpoint_000000)
2024-08-26 14:21:15,882	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:16,918	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:18,190	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:18,218	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:20,422	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:20,564	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:20,588	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3049949)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1006/trial-ad5fc_00002_2_alpha_d_ff=1,batch_size=16,d_core=64,d_model=64,dropout=0.0030,e_layers=4,learning_rate=0.0003,lradj=type1_2024-08-26_14-21-11/checkpoint_000000)[32m [repeated 8x across cluster][0m
2024-08-26 14:21:22,038	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:22,989	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:23,003	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:23,685	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:25,029	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:26,225	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:26,295	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:27,269	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3051251)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1006/trial-ad5fc_00004_4_alpha_d_ff=4,batch_size=32,d_core=512,d_model=32,dropout=0.0111,e_layers=2,learning_rate=0.0003,lradj=type1_2024-08-26_14-21-22/checkpoint_000001)[32m [repeated 7x across cluster][0m
2024-08-26 14:21:27,919	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:28,342	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:28,580	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3049948)[0m Epoch: 1, Steps: 265 | Train Loss: 0.4356478 Vali Loss: 0.2190559 Best vali loss: 0.2190559
[36m(_train_fn pid=3049949)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=3049950)[0m {'batch_size': 32, 'learning_rate': 0.0017486424448695482, 'd_model': 512, 'd_core': 32, 'e_layers': 4, 'dropout': 0.006886691584055082, 'lradj': 'cosine', 'd_ff': 1024}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3049948)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3049949)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3049949)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3049948)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3049949)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3049949)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3049947)[0m Validation loss decreased (0.2191 --> 0.2178).  Saving model state dict ...
[36m(_train_fn pid=3049950)[0m 	iters: 200, epoch: 2 | loss: 0.8870139[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=3049950)[0m 	speed: 0.0115s/iter; left time: 18.9900s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=3049948)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3049947)[0m Updating learning rate to 0.00020740251485476345[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3049947)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3049950)[0m Validation loss decreased (inf --> 0.2745).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3049947)[0m Epoch: 3 cost time: 2.051651954650879[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3049947)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3820052 Vali Loss: 0.2180964 Best vali loss: 0.2178336[32m [repeated 7x across cluster][0m

Trial trial-ad5fc_00001 completed after 4 iterations at 2024-08-26 14:21:22. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.42237 â”‚
â”‚ time_total_s                                10.27488 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21906 â”‚
â”‚ train_loss                                   0.35414 â”‚
â”‚ valid_loss                                   0.22589 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3049948)[0m Early stopping
[36m(_train_fn pid=3049947)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3049950)[0m Validation loss decreased (0.2745 --> 0.2677).  Saving model state dict ...

Trial trial-ad5fc_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                             0.0111 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00031 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3049949)[0m 	iters: 200, epoch: 2 | loss: 0.2002702[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3049949)[0m 	speed: 0.0113s/iter; left time: 39.4945s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3051251)[0m configuration
[36m(_train_fn pid=3051251)[0m {'batch_size': 32, 'learning_rate': 0.0003138145210678971, 'd_model': 32, 'd_core': 512, 'e_layers': 2, 'dropout': 0.011095170845475785, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=3051251)[0m Use GPU: cuda:0
[36m(_train_fn pid=3051251)[0m train 8449
[36m(_train_fn pid=3051251)[0m val 2785

Trial trial-ad5fc_00000 completed after 5 iterations at 2024-08-26 14:21:25. Total running time: 13s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             2.02286 â”‚
â”‚ time_total_s                                12.32006 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21783 â”‚
â”‚ train_loss                                   0.35363 â”‚
â”‚ valid_loss                                   0.22629 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3051251)[0m start_epoch 0
[36m(_train_fn pid=3051251)[0m max_epoch 8
[36m(_train_fn pid=3049950)[0m Validation loss decreased (0.2677 --> 0.2667).  Saving model state dict ...

Trial trial-ad5fc_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00005 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                            128 â”‚
â”‚ d_core                                512 â”‚
â”‚ d_model                                32 â”‚
â”‚ dropout                            0.0031 â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0019 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3051251)[0m Updating learning rate to 0.0003138145210678971[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3051251)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3051251)[0m Validation loss decreased (inf --> 0.2318).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3051251)[0m Epoch: 1 cost time: 1.047347068786621[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3051251)[0m Epoch: 1, Steps: 265 | Train Loss: 0.4836075 Vali Loss: 0.2317542 Best vali loss: 0.2317542[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3051511)[0m {'batch_size': 128, 'learning_rate': 0.0019036369797439112, 'd_model': 32, 'd_core': 512, 'e_layers': 2, 'dropout': 0.003102547616230147, 'lradj': 'cosine', 'd_ff': 32}
[36m(_train_fn pid=3049947)[0m Early stopping
2024-08-26 14:21:28,772	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:28,918	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:29,156	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:29,424	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:29,795	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:30,458	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:30,526	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:31,147	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:31,645	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:31,678	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:32,743	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3051251)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1006/trial-ad5fc_00004_4_alpha_d_ff=4,batch_size=32,d_core=512,d_model=32,dropout=0.0111,e_layers=2,learning_rate=0.0003,lradj=type1_2024-08-26_14-21-22/checkpoint_000006)[32m [repeated 14x across cluster][0m
2024-08-26 14:21:34,143	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:34,697	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:34,808	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:35,661	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3049947)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3049950)[0m 	iters: 100, epoch: 6 | loss: 0.4309385[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3049950)[0m 	speed: 0.0188s/iter; left time: 13.0563s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3051511)[0m configuration
[36m(_train_fn pid=3051511)[0m Use GPU: cuda:0
[36m(_train_fn pid=3051511)[0m train 8449
[36m(_train_fn pid=3051511)[0m val 2785
[36m(_train_fn pid=3051511)[0m start_epoch 0
[36m(_train_fn pid=3051511)[0m max_epoch 8

Trial trial-ad5fc_00005 completed after 6 iterations at 2024-08-26 14:21:31. Total running time: 20s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             0.68715 â”‚
â”‚ time_total_s                                 4.60087 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22017 â”‚
â”‚ train_loss                                   0.34521 â”‚
â”‚ valid_loss                                   0.22908 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3051511)[0m Early stopping
[36m(_train_fn pid=3051251)[0m Validation loss decreased (0.2202 --> 0.2196).  Saving model state dict ...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3049950)[0m Updating learning rate to 0.0002560827571158336[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3049950)[0m saving checkpoint...[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3051511)[0m Validation loss decreased (inf --> 0.2264).  Saving model state dict ...
[36m(_train_fn pid=3049950)[0m Epoch: 6 cost time: 2.444950819015503[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3049950)[0m Epoch: 6, Steps: 265 | Train Loss: 0.5162311 Vali Loss: 0.2691588 Best vali loss: 0.2667065[32m [repeated 13x across cluster][0m

Trial trial-ad5fc_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.01022 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00007 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3052623)[0m configuration
[36m(_train_fn pid=3052623)[0m {'batch_size': 64, 'learning_rate': 7.381907048296366e-05, 'd_model': 64, 'd_core': 32, 'e_layers': 4, 'dropout': 0.010219666051203235, 'lradj': 'cosine', 'd_ff': 192}
[36m(_train_fn pid=3052623)[0m Use GPU: cuda:0

Trial trial-ad5fc_00004 completed after 7 iterations at 2024-08-26 14:21:32. Total running time: 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             1.09487 â”‚
â”‚ time_total_s                                 8.17755 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.21964 â”‚
â”‚ train_loss                                   0.39459 â”‚
â”‚ valid_loss                                   0.21976 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3052623)[0m train 8449
[36m(_train_fn pid=3052623)[0m val 2785
[36m(_train_fn pid=3052623)[0m start_epoch 0
[36m(_train_fn pid=3052623)[0m max_epoch 8

Trial trial-ad5fc_00003 completed after 7 iterations at 2024-08-26 14:21:34. Total running time: 23s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             2.46138 â”‚
â”‚ time_total_s                                21.45457 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26671 â”‚
â”‚ train_loss                                     0.518 â”‚
â”‚ valid_loss                                   0.27006 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3049950)[0m EarlyStopping counter: 3 out of 3[32m [repeated 9x across cluster][0m

Trial trial-ad5fc_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00131 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00157 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3052877)[0m {'batch_size': 64, 'learning_rate': 0.0015679623600840416, 'd_model': 32, 'd_core': 256, 'e_layers': 1, 'dropout': 0.0013075840179784712, 'lradj': 'type1', 'd_ff': 96}
[36m(_train_fn pid=3052623)[0m Validation loss decreased (inf --> 0.2531).  Saving model state dict ...
[36m(_train_fn pid=3052877)[0m 	iters: 100, epoch: 1 | loss: 0.6358866[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3052877)[0m 	speed: 0.0099s/iter; left time: 9.5421s[32m [repeated 16x across cluster][0m

Trial trial-ad5fc_00008 started with configuration:
2024-08-26 14:21:35,861	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:36,295	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:36,896	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:36,937	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:37,092	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:37,329	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:37,598	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:37,774	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3053014)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1006/trial-ad5fc_00008_8_alpha_d_ff=4,batch_size=128,d_core=64,d_model=128,dropout=0.0011,e_layers=2,learning_rate=0.0002,lradj=cosine_2024-08-26_14-21-34/checkpoint_000002)[32m [repeated 12x across cluster][0m
2024-08-26 14:21:38,227	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:38,265	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:38,631	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:39,034	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:39,228	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:39,312	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:39,467	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:40,474	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00114 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00021 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3049950)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3052623)[0m Validation loss decreased (0.2531 --> 0.2361).  Saving model state dict ...
[36m(_train_fn pid=3053014)[0m Updating learning rate to 0.0001997207148469286[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3053014)[0m saving checkpoint...[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3053014)[0m Epoch: 1 cost time: 0.5534522533416748[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3053014)[0m Epoch: 1, Steps: 67 | Train Loss: 0.4669845 Vali Loss: 0.2238148 Best vali loss: 0.2238148[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3052623)[0m Validation loss decreased (0.2361 --> 0.2288).  Saving model state dict ...

Trial trial-ad5fc_00007 completed after 4 iterations at 2024-08-26 14:21:37. Total running time: 26s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.65884 â”‚
â”‚ time_total_s                                 3.31815 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21921 â”‚
â”‚ train_loss                                   0.38696 â”‚
â”‚ valid_loss                                   0.21984 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3053014)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3053014)[0m {'batch_size': 128, 'learning_rate': 0.0002076228905936, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0011410809928204693, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=3053014)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3053014)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3053014)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3053014)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3053014)[0m max_epoch 8[32m [repeated 2x across cluster][0m

Trial trial-ad5fc_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00235 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3053014)[0m EarlyStopping counter: 2 out of 3[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3053884)[0m {'batch_size': 16, 'learning_rate': 5.528250432364244e-05, 'd_model': 64, 'd_core': 256, 'e_layers': 1, 'dropout': 0.002351845610726406, 'lradj': 'type1', 'd_ff': 256}

Trial trial-ad5fc_00008 completed after 7 iterations at 2024-08-26 14:21:39. Total running time: 28s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             0.42933 â”‚
â”‚ time_total_s                                 3.69153 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                               0.2163 â”‚
â”‚ train_loss                                   0.38298 â”‚
â”‚ valid_loss                                   0.21732 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3053014)[0m Validation loss decreased (inf --> 0.2238).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3053884)[0m 	iters: 100, epoch: 1 | loss: 1.3474292[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3053884)[0m 	speed: 0.0103s/iter; left time: 42.6263s[32m [repeated 15x across cluster][0m

Trial status: 7 TERMINATED | 3 RUNNING
Current time: 2024-08-26 14:21:41. Total running time: 30s
Logical resource usage: 3.0/32 CPUs, 0.75/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: ad5fc_00008 with best_valid_loss=0.21629977889720156 and params={'batch_size': 128, 'learning_rate': 0.0002076228905936, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0011410809928204693, 'lradj': 'cosine', 'd_ff': 512}
2024-08-26 14:21:41,657	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:42,735	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:42,815	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3052623)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1006/trial-ad5fc_00006_6_alpha_d_ff=3,batch_size=64,d_core=32,d_model=64,dropout=0.0102,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-21-31/checkpoint_000007)[32m [repeated 11x across cluster][0m
2024-08-26 14:21:43,663	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:44,357	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:45,884	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-ad5fc_00002   RUNNING                16       0.000323979          64              1         64            4   0.00300701   type1          4           26.5299        0.369834       0.223554            0.218321 â”‚
â”‚ trial-ad5fc_00006   RUNNING                64       7.38191e-05          64              3         32            4   0.0102197    cosine         6            7.80846       0.41856        0.22236             0.22236  â”‚
â”‚ trial-ad5fc_00009   RUNNING                16       5.52825e-05          64              4        256            1   0.00235185   type1                                                                                 â”‚
â”‚ trial-ad5fc_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5           12.3201        0.353632       0.226293            0.217834 â”‚
â”‚ trial-ad5fc_00001   TERMINATED             32       0.00079558           64              3         64            2   0.00697738   cosine         4           10.2749        0.35414        0.225889            0.219056 â”‚
â”‚ trial-ad5fc_00003   TERMINATED             32       0.00174864          512              2         32            4   0.00688669   cosine         7           21.4546        0.517998       0.270058            0.266706 â”‚
â”‚ trial-ad5fc_00004   TERMINATED             32       0.000313815          32              4        512            2   0.0110952    type1          7            8.17755       0.39459        0.219759            0.219637 â”‚
â”‚ trial-ad5fc_00005   TERMINATED            128       0.00190364           32              1        512            2   0.00310255   cosine         6            4.60087       0.345206       0.229081            0.220167 â”‚
â”‚ trial-ad5fc_00007   TERMINATED             64       0.00156796           32              3        256            1   0.00130758   type1          4            3.31815       0.386963       0.219843            0.219208 â”‚
â”‚ trial-ad5fc_00008   TERMINATED            128       0.000207623         128              4         64            2   0.00114108   cosine         7            3.69153       0.382981       0.217323            0.2163   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3053014)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3052623)[0m Updating learning rate to 2.8095710773727336e-06[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3052623)[0m saving checkpoint...[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3052623)[0m Epoch: 7 cost time: 1.0146968364715576[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3052623)[0m Epoch: 7, Steps: 133 | Train Loss: 0.4201857 Vali Loss: 0.2219549 Best vali loss: 0.2219549[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3052623)[0m Validation loss decreased (0.2224 --> 0.2220).  Saving model state dict ...[32m [repeated 7x across cluster][0m

Trial trial-ad5fc_00006 completed after 8 iterations at 2024-08-26 14:21:42. Total running time: 31s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.15604 â”‚
â”‚ time_total_s                                10.14466 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22186 â”‚
â”‚ train_loss                                   0.41821 â”‚
â”‚ valid_loss                                   0.22186 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3053884)[0m configuration
[36m(_train_fn pid=3053884)[0m Use GPU: cuda:0
[36m(_train_fn pid=3053884)[0m train 8449
[36m(_train_fn pid=3053884)[0m val 2785
[36m(_train_fn pid=3053884)[0m start_epoch 0
[36m(_train_fn pid=3053884)[0m max_epoch 8

Trial trial-ad5fc_00002 completed after 5 iterations at 2024-08-26 14:21:43. Total running time: 32s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             4.43184 â”‚
â”‚ time_total_s                                30.96177 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21832 â”‚
â”‚ train_loss                                   0.36125 â”‚
â”‚ valid_loss                                   0.22377 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3049949)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3053884)[0m Validation loss decreased (inf --> 0.2451).  Saving model state dict ...
[36m(_train_fn pid=3053884)[0m 	iters: 500, epoch: 3 | loss: 0.2671675[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=3053884)[0m 	speed: 0.0021s/iter; left time: 5.7372s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=3049949)[0m Early stopping
2024-08-26 14:21:47,406	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:48,971	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3053884)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1006/trial-ad5fc_00009_9_alpha_d_ff=4,batch_size=16,d_core=256,d_model=64,dropout=0.0024,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_14-21-37/checkpoint_000004)[32m [repeated 5x across cluster][0m
2024-08-26 14:21:50,422	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:51,863	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:53,221	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:21:53,231	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1006' in 0.0089s.
[36m(_train_fn pid=3053884)[0m Updating learning rate to 1.382062608091061e-05[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3053884)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3053884)[0m Epoch: 3 cost time: 1.281484842300415[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3053884)[0m Epoch: 3, Steps: 529 | Train Loss: 0.4365269 Vali Loss: 0.2259205 Best vali loss: 0.2259205[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3053884)[0m Validation loss decreased (0.2259 --> 0.2245).  Saving model state dict ...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3053884)[0m 	iters: 100, epoch: 7 | loss: 0.4681131[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3053884)[0m 	speed: 0.0056s/iter; left time: 5.3727s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3053884)[0m Updating learning rate to 8.637891300569132e-07[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3053884)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3053884)[0m Epoch: 7 cost time: 1.2006902694702148[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3053884)[0m Epoch: 7, Steps: 529 | Train Loss: 0.4262926 Vali Loss: 0.2234175 Best vali loss: 0.2234175[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3053884)[0m Validation loss decreased (0.2236 --> 0.2234).  Saving model state dict ...[32m [repeated 3x across cluster][0m

Trial trial-ad5fc_00009 completed after 8 iterations at 2024-08-26 14:21:53. Total running time: 42s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-ad5fc_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.35599 â”‚
â”‚ time_total_s                                14.04449 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22334 â”‚
â”‚ train_loss                                   0.42601 â”‚
â”‚ valid_loss                                   0.22334 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:21:53. Total running time: 42s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: ad5fc_00008 with best_valid_loss=0.21629977889720156 and params={'batch_size': 128, 'learning_rate': 0.0002076228905936, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0011410809928204693, 'lradj': 'cosine', 'd_ff': 512}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-ad5fc_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5           12.3201        0.353632       0.226293            0.217834 â”‚
â”‚ trial-ad5fc_00001   TERMINATED             32       0.00079558           64              3         64            2   0.00697738   cosine         4           10.2749        0.35414        0.225889            0.219056 â”‚
â”‚ trial-ad5fc_00002   TERMINATED             16       0.000323979          64              1         64            4   0.00300701   type1          5           30.9618        0.361252       0.223772            0.218321 â”‚
â”‚ trial-ad5fc_00003   TERMINATED             32       0.00174864          512              2         32            4   0.00688669   cosine         7           21.4546        0.517998       0.270058            0.266706 â”‚
â”‚ trial-ad5fc_00004   TERMINATED             32       0.000313815          32              4        512            2   0.0110952    type1          7            8.17755       0.39459        0.219759            0.219637 â”‚
â”‚ trial-ad5fc_00005   TERMINATED            128       0.00190364           32              1        512            2   0.00310255   cosine         6            4.60087       0.345206       0.229081            0.220167 â”‚
â”‚ trial-ad5fc_00006   TERMINATED             64       7.38191e-05          64              3         32            4   0.0102197    cosine         8           10.1447        0.418207       0.221861            0.221861 â”‚
â”‚ trial-ad5fc_00007   TERMINATED             64       0.00156796           32              3        256            1   0.00130758   type1          4            3.31815       0.386963       0.219843            0.219208 â”‚
â”‚ trial-ad5fc_00008   TERMINATED            128       0.000207623         128              4         64            2   0.00114108   cosine         7            3.69153       0.382981       0.217323            0.2163   â”‚
â”‚ trial-ad5fc_00009   TERMINATED             16       5.52825e-05          64              4        256            1   0.00235185   type1          8           14.0445        0.426009       0.223345            0.223345 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 128, 'learning_rate': 0.0002076228905936, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0011410809928204693, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=3053884)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1006/trial-ad5fc_00009_9_alpha_d_ff=4,batch_size=16,d_core=256,d_model=64,dropout=0.0024,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_14-21-37/checkpoint_000007)[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3053884)[0m 	iters: 500, epoch: 8 | loss: 1.1135201[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3053884)[0m 	speed: 0.0020s/iter; left time: 0.0604s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3053884)[0m Updating learning rate to 4.318945650284566e-07
[36m(_train_fn pid=3053884)[0m saving checkpoint...
[36m(_train_fn pid=3053884)[0m Epoch: 8 cost time: 1.1163382530212402
[36m(_train_fn pid=3053884)[0m Epoch: 8, Steps: 529 | Train Loss: 0.4260095 Vali Loss: 0.2233446 Best vali loss: 0.2233446
[36m(_train_fn pid=3053884)[0m Validation loss decreased (0.2234 --> 0.2233).  Saving model state dict ...


Time taken (4 parallel trials): 47 seconds


2024-08-26 14:21:57,205	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:21:57,616	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:21:57,621	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:21:57,628	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:22:02,240	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed1007   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator       â”‚
â”‚ Scheduler                        FIFOScheduler               â”‚
â”‚ Number of trials                 10                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-21-56_773764_3055370/artifacts/2024-08-26_14-21-57/ETTh2_96_96_test_seed1007/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:21:57. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-c92a5_00000   PENDING              32       0.0003              128              1         64            2   0             cosine  â”‚
â”‚ trial-c92a5_00001   PENDING               8       0.00214255           32              3        256            1   0.00394089    cosine  â”‚
â”‚ trial-c92a5_00002   PENDING               8       0.00039933          512              3         32            2   0.000898698   cosine  â”‚
â”‚ trial-c92a5_00003   PENDING               8       0.000466361         512              4        512            4   0.0111416     type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c92a5_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c92a5_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00394 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00214 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c92a5_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00002 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              3 â”‚
â”‚ batch_size                              8 â”‚
â”‚ d_core                                 32 â”‚
â”‚ d_model                               512 â”‚
â”‚ dropout                            0.0009 â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0004 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3057594)[0m configuration
[36m(_train_fn pid=3057594)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=3057594)[0m Use GPU: cuda:0

Trial trial-c92a5_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.01114 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00047 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3057594)[0m train 8449
[36m(_train_fn pid=3057597)[0m {'batch_size': 8, 'learning_rate': 0.0004663607813198773, 'd_model': 512, 'd_core': 512, 'e_layers': 4, 'dropout': 0.011141605300521001, 'lradj': 'type1', 'd_ff': 2048}
[36m(_train_fn pid=3057594)[0m val 2785
[36m(_train_fn pid=3057594)[0m start_epoch 0
[36m(_train_fn pid=3057594)[0m max_epoch 8
[36m(_train_fn pid=3057594)[0m 	iters: 100, epoch: 1 | loss: 0.4777271
[36m(_train_fn pid=3057594)[0m 	speed: 0.0128s/iter; left time: 25.7993s
[36m(_train_fn pid=3057594)[0m Updating learning rate to 0.000288581929876693
[36m(_train_fn pid=3057594)[0m saving checkpoint...
[36m(_train_fn pid=3057594)[0m Validation loss decreased (inf --> 0.2180).  Saving model state dict ...
[36m(_train_fn pid=3057594)[0m Epoch: 1 cost time: 2.24137020111084
[36m(_train_fn pid=3057594)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00000_0_2024-08-26_14-21-57/checkpoint_000000)
2024-08-26 14:22:04,664	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3057594)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00000_0_2024-08-26_14-21-57/checkpoint_000001)
[36m(_train_fn pid=3057594)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00000_0_2024-08-26_14-21-57/checkpoint_000002)
2024-08-26 14:22:07,032	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:09,363	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3057594)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00000_0_2024-08-26_14-21-57/checkpoint_000003)
2024-08-26 14:22:09,915	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3057595)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00001_1_alpha_d_ff=3,batch_size=8,d_core=256,d_model=32,dropout=0.0039,e_layers=1,learning_rate=0.0021,lradj=cosine_2024-08-26_14-21-57/checkpoint_000000)
2024-08-26 14:22:11,362	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:11,488	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:14,190	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:18,317	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3057595)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00001_1_alpha_d_ff=3,batch_size=8,d_core=256,d_model=32,dropout=0.0039,e_layers=1,learning_rate=0.0021,lradj=cosine_2024-08-26_14-21-57/checkpoint_000001)[32m [repeated 4x across cluster][0m
2024-08-26 14:22:19,346	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:20,254	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:24,967	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3058664)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=512,dropout=0.0020,e_layers=3,learning_rate=0.0008,lradj=type1_2024-08-26_14-22-11/checkpoint_000001)[32m [repeated 3x across cluster][0m
2024-08-26 14:22:26,078	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:26,383	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3057594)[0m Epoch: 1, Steps: 265 | Train Loss: 0.4433076 Vali Loss: 0.2180349 Best vali loss: 0.2180349
[36m(_train_fn pid=3057597)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=3057596)[0m {'batch_size': 8, 'learning_rate': 0.0003993298027855629, 'd_model': 512, 'd_core': 32, 'e_layers': 2, 'dropout': 0.0008986978215488756, 'lradj': 'cosine', 'd_ff': 1536}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3057596)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3057596)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3057596)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3057594)[0m Updating learning rate to 0.00025606601717798207
[36m(_train_fn pid=3057594)[0m saving checkpoint...
[36m(_train_fn pid=3057594)[0m Validation loss decreased (0.2180 --> 0.2165).  Saving model state dict ...
[36m(_train_fn pid=3057594)[0m Epoch: 2 cost time: 2.0534794330596924
[36m(_train_fn pid=3057594)[0m Epoch: 2, Steps: 265 | Train Loss: 0.4021628 Vali Loss: 0.2164555 Best vali loss: 0.2164555
[36m(_train_fn pid=3057596)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3057596)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3057595)[0m 	iters: 800, epoch: 1 | loss: 0.3504689[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=3057595)[0m 	speed: 0.0073s/iter; left time: 56.0948s[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=3057594)[0m Updating learning rate to 0.00020740251485476345
[36m(_train_fn pid=3057594)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3057594)[0m saving checkpoint...
[36m(_train_fn pid=3057594)[0m Epoch: 3 cost time: 1.9987478256225586
[36m(_train_fn pid=3057594)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3833804 Vali Loss: 0.2186895 Best vali loss: 0.2164555
[36m(_train_fn pid=3057594)[0m Updating learning rate to 0.00015
[36m(_train_fn pid=3057594)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3057594)[0m saving checkpoint...
[36m(_train_fn pid=3057594)[0m Epoch: 4 cost time: 1.9658310413360596
[36m(_train_fn pid=3057594)[0m Epoch: 4, Steps: 265 | Train Loss: 0.3659325 Vali Loss: 0.2220577 Best vali loss: 0.2164555
[36m(_train_fn pid=3057595)[0m Validation loss decreased (inf --> 0.2203).  Saving model state dict ...
[36m(_train_fn pid=3057595)[0m 	iters: 200, epoch: 2 | loss: 0.8016773[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=3057595)[0m 	speed: 0.0067s/iter; left time: 48.3854s[32m [repeated 18x across cluster][0m

Trial trial-c92a5_00000 completed after 5 iterations at 2024-08-26 14:22:11. Total running time: 13s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             2.12262 â”‚
â”‚ time_total_s                                12.31853 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21646 â”‚
â”‚ train_loss                                   0.34954 â”‚
â”‚ valid_loss                                   0.22156 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3057594)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3057594)[0m Early stopping

Trial trial-c92a5_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00199 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00081 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3058664)[0m configuration
[36m(_train_fn pid=3058664)[0m {'batch_size': 8, 'learning_rate': 0.0008135554653130499, 'd_model': 512, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0019879291959628577, 'lradj': 'type1', 'd_ff': 1536}
[36m(_train_fn pid=3058664)[0m Use GPU: cuda:0
[36m(_train_fn pid=3058664)[0m train 8449
[36m(_train_fn pid=3058664)[0m val 2785
[36m(_train_fn pid=3058664)[0m start_epoch 0
[36m(_train_fn pid=3058664)[0m max_epoch 8
[36m(_train_fn pid=3057597)[0m Updating learning rate to 0.0004663607813198773[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3057597)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3057597)[0m Epoch: 1 cost time: 12.851121664047241[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3057597)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.5142986 Vali Loss: 0.2760755 Best vali loss: 0.2760755[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3057597)[0m Validation loss decreased (inf --> 0.2761).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3058664)[0m 	iters: 600, epoch: 1 | loss: 0.2357550[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=3058664)[0m 	speed: 0.0047s/iter; left time: 36.9289s[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=3057595)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3058664)[0m Updating learning rate to 0.0008135554653130499[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3058664)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3058664)[0m Epoch: 1 cost time: 5.2377402782440186[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3058664)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.5201227 Vali Loss: 0.2703193 Best vali loss: 0.2703193[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3057596)[0m Validation loss decreased (0.2238 --> 0.2201).  Saving model state dict ...
[36m(_train_fn pid=3058664)[0m Validation loss decreased (inf --> 0.2703).  Saving model state dict ...
[36m(_train_fn pid=3058664)[0m 	iters: 500, epoch: 2 | loss: 0.7134371[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=3058664)[0m 	speed: 0.0046s/iter; left time: 32.0453s[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=3058664)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3058664)[0m Updating learning rate to 0.00040677773265652495[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3058664)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3058664)[0m Epoch: 2 cost time: 4.990406513214111[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3058664)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.5213531 Vali Loss: 0.2800118 Best vali loss: 0.2703193[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3057597)[0m Validation loss decreased (0.2761 --> 0.2751).  Saving model state dict ...
[36m(_train_fn pid=3058664)[0m 	iters: 400, epoch: 3 | loss: 0.2020683[32m [repeated 25x across cluster][0m
2024-08-26 14:22:29,125	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:30,772	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3058664)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=512,dropout=0.0020,e_layers=3,learning_rate=0.0008,lradj=type1_2024-08-26_14-22-11/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 14:22:34,526	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:36,493	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3058664)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=512,dropout=0.0020,e_layers=3,learning_rate=0.0008,lradj=type1_2024-08-26_14-22-11/checkpoint_000003)[32m [repeated 2x across cluster][0m
2024-08-26 14:22:37,245	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3058664)[0m 	speed: 0.0049s/iter; left time: 28.8792s[32m [repeated 25x across cluster][0m

Trial status: 1 TERMINATED | 4 RUNNING
Current time: 2024-08-26 14:22:27. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: c92a5_00000 with best_valid_loss=0.2164555068289998 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-c92a5_00001   RUNNING                 8       0.00214255           32              3        256            1   0.00394089    cosine         3            27.1804       0.38273        0.240099            0.220315 â”‚
â”‚ trial-c92a5_00002   RUNNING                 8       0.00039933          512              3         32            2   0.000898698   cosine         2            21.0456       0.426412       0.220079            0.220079 â”‚
â”‚ trial-c92a5_00003   RUNNING                 8       0.000466361         512              4        512            4   0.0111416     type1          2            26.847        0.526231       0.275136            0.275136 â”‚
â”‚ trial-c92a5_00004   RUNNING                 8       0.000813555         512              3         64            3   0.00198793    type1          2            11.9663       0.521353       0.280012            0.270319 â”‚
â”‚ trial-c92a5_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5            12.3185       0.349537       0.221555            0.216456 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3057596)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3057596)[0m Updating learning rate to 0.00027607335118060825[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3057596)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3057596)[0m Epoch: 3 cost time: 7.276347398757935[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3057596)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.3906115 Vali Loss: 0.2372932 Best vali loss: 0.2200793[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3058664)[0m Validation loss decreased (0.2703 --> 0.2662).  Saving model state dict ...
[36m(_train_fn pid=3057597)[0m 	iters: 600, epoch: 3 | loss: 0.3986953[32m [repeated 27x across cluster][0m
[36m(_train_fn pid=3057597)[0m 	speed: 0.0102s/iter; left time: 58.5659s[32m [repeated 27x across cluster][0m

Trial trial-c92a5_00001 completed after 4 iterations at 2024-08-26 14:22:34. Total running time: 36s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             8.14089 â”‚
â”‚ time_total_s                                35.32125 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22031 â”‚
â”‚ train_loss                                   0.35023 â”‚
â”‚ valid_loss                                    0.2273 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3057595)[0m Early stopping
[36m(_train_fn pid=3057595)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3057595)[0m Updating learning rate to 0.0010712743314939543[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3057595)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3057595)[0m Epoch: 4 cost time: 6.411823272705078[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3057595)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3502271 Vali Loss: 0.2273036 Best vali loss: 0.2203149[32m [repeated 2x across cluster][0m

Trial trial-c92a5_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                             0.0061 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00092 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3059543)[0m configuration
[36m(_train_fn pid=3059543)[0m {'batch_size': 128, 'learning_rate': 0.0009157990320752741, 'd_model': 256, 'd_core': 256, 'e_layers': 1, 'dropout': 0.006096021325947016, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=3059543)[0m Use GPU: cuda:0
[36m(_train_fn pid=3059543)[0m train 8449
[36m(_train_fn pid=3059543)[0m val 2785
[36m(_train_fn pid=3058664)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3059543)[0m start_epoch 0
[36m(_train_fn pid=3059543)[0m max_epoch 8
[36m(_train_fn pid=3058664)[0m 	iters: 100, epoch: 5 | loss: 0.3768648[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=3058664)[0m 	speed: 0.0145s/iter; left time: 59.9329s[32m [repeated 21x across cluster][0m
2024-08-26 14:22:37,681	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:37,684	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:37,708	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:38,258	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:38,777	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:42,165	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3060133)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00006_6_alpha_d_ff=4,batch_size=128,d_core=32,d_model=512,dropout=0.0027,e_layers=2,learning_rate=0.0015,lradj=cosine_2024-08-26_14-22-38/checkpoint_000000)[32m [repeated 7x across cluster][0m
2024-08-26 14:22:42,236	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:43,278	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:44,386	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:45,474	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:46,550	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:47,624	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3060133)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00006_6_alpha_d_ff=4,batch_size=128,d_core=32,d_model=512,dropout=0.0027,e_layers=2,learning_rate=0.0015,lradj=cosine_2024-08-26_14-22-38/checkpoint_000005)[32m [repeated 6x across cluster][0m
2024-08-26 14:22:47,971	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:49,198	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3059543)[0m Validation loss decreased (inf --> 0.2171).  Saving model state dict ...
[36m(_train_fn pid=3057597)[0m Validation loss decreased (0.2751 --> 0.2741).  Saving model state dict ...

Trial trial-c92a5_00005 completed after 4 iterations at 2024-08-26 14:22:38. Total running time: 41s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.51724 â”‚
â”‚ time_total_s                                 2.69015 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21709 â”‚
â”‚ train_loss                                   0.34345 â”‚
â”‚ valid_loss                                   0.22604 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3059543)[0m Early stopping

Trial trial-c92a5_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00267 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00152 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3059543)[0m Updating learning rate to 0.00045789951603763705[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3059543)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3059543)[0m Epoch: 4 cost time: 0.3846113681793213[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3059543)[0m Epoch: 4, Steps: 67 | Train Loss: 0.3434497 Vali Loss: 0.2260444 Best vali loss: 0.2170904[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3060133)[0m configuration
[36m(_train_fn pid=3060133)[0m {'batch_size': 128, 'learning_rate': 0.0015189333411663505, 'd_model': 512, 'd_core': 32, 'e_layers': 2, 'dropout': 0.0026720769698796113, 'lradj': 'cosine', 'd_ff': 2048}
[36m(_train_fn pid=3060133)[0m Use GPU: cuda:0
[36m(_train_fn pid=3060133)[0m train 8449
[36m(_train_fn pid=3060133)[0m val 2785
[36m(_train_fn pid=3059543)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3060133)[0m start_epoch 0
[36m(_train_fn pid=3060133)[0m max_epoch 8
[36m(_train_fn pid=3057597)[0m 	iters: 400, epoch: 4 | loss: 0.4440002[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=3057597)[0m 	speed: 0.0138s/iter; left time: 67.2837s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=3060133)[0m Validation loss decreased (inf --> 0.2351).  Saving model state dict ...
[36m(_train_fn pid=3060133)[0m Validation loss decreased (0.2351 --> 0.2314).  Saving model state dict ...
[36m(_train_fn pid=3060133)[0m Validation loss decreased (0.2314 --> 0.2260).  Saving model state dict ...
[36m(_train_fn pid=3060133)[0m Updating learning rate to 0.0007594666705831752[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3060133)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3060133)[0m Epoch: 4 cost time: 0.9126608371734619[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3060133)[0m Epoch: 4, Steps: 67 | Train Loss: 0.3875831 Vali Loss: 0.2266575 Best vali loss: 0.2260371[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3060133)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3057596)[0m 	iters: 1000, epoch: 5 | loss: 0.1990977[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=3057596)[0m 	speed: 0.0121s/iter; left time: 39.1087s[32m [repeated 18x across cluster][0m

Trial trial-c92a5_00006 completed after 6 iterations at 2024-08-26 14:22:47. Total running time: 49s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.07141 â”‚
â”‚ time_total_s                                 7.31934 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22604 â”‚
â”‚ train_loss                                   0.34697 â”‚
â”‚ valid_loss                                   0.22733 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3060133)[0m Early stopping

Trial trial-c92a5_00004 completed after 6 iterations at 2024-08-26 14:22:47. Total running time: 50s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             5.73264 â”‚
â”‚ time_total_s                                34.94398 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.26622 â”‚
â”‚ train_loss                                   0.51698 â”‚
â”‚ valid_loss                                   0.27038 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c92a5_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00671 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00026 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-c92a5_00002 completed after 5 iterations at 2024-08-26 14:22:49. Total running time: 51s
2024-08-26 14:22:50,396	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:51,176	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:51,266	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:51,892	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:52,027	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:52,450	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:52,988	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3060776)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00007_7_alpha_d_ff=1,batch_size=128,d_core=64,d_model=32,dropout=0.0067,e_layers=3,learning_rate=0.0003,lradj=type1_2024-08-26_14-22-47/checkpoint_000003)[32m [repeated 9x across cluster][0m
2024-08-26 14:22:53,003	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:53,516	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:53,889	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:54,040	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:54,595	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:54,819	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:55,149	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:55,708	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:55,776	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:22:56,787	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                            11.51138 â”‚
â”‚ time_total_s                                49.97953 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22008 â”‚
â”‚ train_loss                                   0.31021 â”‚
â”‚ valid_loss                                   0.23971 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3060776)[0m configuration
[36m(_train_fn pid=3060776)[0m {'batch_size': 128, 'learning_rate': 0.0002581189974141956, 'd_model': 32, 'd_core': 64, 'e_layers': 3, 'dropout': 0.006714207481521372, 'lradj': 'type1', 'd_ff': 32}
[36m(_train_fn pid=3060776)[0m Use GPU: cuda:0
[36m(_train_fn pid=3060776)[0m train 8449
[36m(_train_fn pid=3060776)[0m val 2785

Trial trial-c92a5_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00414 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00088 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3060776)[0m start_epoch 0
[36m(_train_fn pid=3060776)[0m max_epoch 8
[36m(_train_fn pid=3060776)[0m Validation loss decreased (inf --> 0.2628).  Saving model state dict ...

Trial trial-c92a5_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                             0.0016 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00008 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3060947)[0m {'batch_size': 64, 'learning_rate': 7.509282231736743e-05, 'd_model': 64, 'd_core': 256, 'e_layers': 1, 'dropout': 0.0015989323006242138, 'lradj': 'cosine', 'd_ff': 64}
[36m(_train_fn pid=3060776)[0m Updating learning rate to 0.0002581189974141956[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3060776)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3060776)[0m Epoch: 1 cost time: 0.7058005332946777[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3060776)[0m Epoch: 1, Steps: 67 | Train Loss: 0.5325606 Vali Loss: 0.2628245 Best vali loss: 0.2628245[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3057597)[0m Validation loss decreased (0.2741 --> 0.2737).  Saving model state dict ...
[36m(_train_fn pid=3057596)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3057597)[0m 	iters: 100, epoch: 5 | loss: 0.3966367[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3057597)[0m 	speed: 0.0317s/iter; left time: 130.9652s[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3057596)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060947)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060870)[0m {'batch_size': 16, 'learning_rate': 0.000882855252481078, 'd_model': 512, 'd_core': 512, 'e_layers': 4, 'dropout': 0.004144657164237894, 'lradj': 'type1', 'd_ff': 1536}
[36m(_train_fn pid=3060947)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060947)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060947)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060947)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060947)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060947)[0m Validation loss decreased (inf --> 0.2683).  Saving model state dict ...

Trial trial-c92a5_00009 completed after 8 iterations at 2024-08-26 14:22:55. Total running time: 58s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.55626 â”‚
â”‚ time_total_s                                 4.99021 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22875 â”‚
â”‚ train_loss                                   0.43841 â”‚
â”‚ valid_loss                                   0.22875 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3060776)[0m Updating learning rate to 4.0331093345968065e-06[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3060776)[0m saving checkpoint...[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3060776)[0m Epoch: 7 cost time: 0.7933230400085449[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3060776)[0m Epoch: 7, Steps: 67 | Train Loss: 0.4364953 Vali Loss: 0.2331478 Best vali loss: 0.2331478[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3060776)[0m Validation loss decreased (0.2334 --> 0.2331).  Saving model state dict ...[32m [repeated 13x across cluster][0m

Trial trial-c92a5_00007 completed after 8 iterations at 2024-08-26 14:22:56. Total running time: 59s
2024-08-26 14:22:57,789	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3060870)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00008_8_alpha_d_ff=3,batch_size=16,d_core=512,d_model=512,dropout=0.0041,e_layers=4,learning_rate=0.0009,lradj=type1_2024-08-26_14-22-47/checkpoint_000001)[32m [repeated 12x across cluster][0m
2024-08-26 14:23:04,371	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3060870)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00008_8_alpha_d_ff=3,batch_size=16,d_core=512,d_model=512,dropout=0.0041,e_layers=4,learning_rate=0.0009,lradj=type1_2024-08-26_14-22-47/checkpoint_000002)[32m [repeated 2x across cluster][0m
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.00887 â”‚
â”‚ time_total_s                                 7.61011 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.23302 â”‚
â”‚ train_loss                                   0.43771 â”‚
â”‚ valid_loss                                   0.23302 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3060870)[0m Validation loss decreased (inf --> 0.2713).  Saving model state dict ...
[36m(_train_fn pid=3057597)[0m 	iters: 500, epoch: 5 | loss: 0.4314115[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3057597)[0m 	speed: 0.0092s/iter; left time: 34.1438s[32m [repeated 14x across cluster][0m

Trial status: 8 TERMINATED | 2 RUNNING
Current time: 2024-08-26 14:22:57. Total running time: 1min 0s
Logical resource usage: 2.0/32 CPUs, 0.5/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: c92a5_00000 with best_valid_loss=0.2164555068289998 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-c92a5_00003   RUNNING                 8       0.000466361         512              4        512            4   0.0111416     type1          4           52.0131        0.517521       0.273698            0.273698 â”‚
â”‚ trial-c92a5_00008   RUNNING                16       0.000882855         512              3        512            4   0.00414466    type1          1            8.19592       0.523182       0.271322            0.271322 â”‚
â”‚ trial-c92a5_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5           12.3185        0.349537       0.221555            0.216456 â”‚
â”‚ trial-c92a5_00001   TERMINATED              8       0.00214255           32              3        256            1   0.00394089    cosine         4           35.3212        0.350227       0.227304            0.220315 â”‚
â”‚ trial-c92a5_00002   TERMINATED              8       0.00039933          512              3         32            2   0.000898698   cosine         5           49.9795        0.310214       0.239714            0.220079 â”‚
â”‚ trial-c92a5_00004   TERMINATED              8       0.000813555         512              3         64            3   0.00198793    type1          6           34.944         0.516975       0.27038             0.26622  â”‚
â”‚ trial-c92a5_00005   TERMINATED            128       0.000915799         256              2        256            1   0.00609602    cosine         4            2.69015       0.34345        0.226044            0.21709  â”‚
â”‚ trial-c92a5_00006   TERMINATED            128       0.00151893          512              4         32            2   0.00267208    cosine         6            7.31934       0.346972       0.227329            0.226037 â”‚
â”‚ trial-c92a5_00007   TERMINATED            128       0.000258119          32              1         64            3   0.00671421    type1          8            7.61011       0.437713       0.233022            0.233022 â”‚
â”‚ trial-c92a5_00009   TERMINATED             64       7.50928e-05          64              1        256            1   0.00159893    cosine         8            4.99021       0.438413       0.228746            0.228746 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3060870)[0m Updating learning rate to 0.000882855252481078[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060870)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060870)[0m Epoch: 1 cost time: 6.855964422225952[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060870)[0m Epoch: 1, Steps: 529 | Train Loss: 0.5231817 Vali Loss: 0.2713223 Best vali loss: 0.2713223[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060776)[0m Validation loss decreased (0.2331 --> 0.2330).  Saving model state dict ...
[36m(_train_fn pid=3060870)[0m 	iters: 500, epoch: 2 | loss: 0.4383017[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3060870)[0m 	speed: 0.0105s/iter; left time: 33.6788s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3060870)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3057597)[0m Validation loss decreased (0.2737 --> 0.2726).  Saving model state dict ...
[36m(_train_fn pid=3057597)[0m Updating learning rate to 2.9147548832492332e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3057597)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3057597)[0m Epoch: 5 cost time: 11.868252992630005[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3057597)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.5181370 Vali Loss: 0.2725812 Best vali loss: 0.2725812[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3057597)[0m 	iters: 400, epoch: 6 | loss: 0.2183300[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3057597)[0m 	speed: 0.0106s/iter; left time: 29.4590s[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3060870)[0m Validation loss decreased (0.2713 --> 0.2711).  Saving model state dict ...
[36m(_train_fn pid=3060870)[0m Updating learning rate to 0.0002207138131202695
[36m(_train_fn pid=3060870)[0m saving checkpoint...
[36m(_train_fn pid=3060870)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00008_8_alpha_d_ff=3,batch_size=16,d_core=512,d_model=512,dropout=0.0041,e_layers=4,learning_rate=0.0009,lradj=type1_2024-08-26_14-22-47/checkpoint_000003)
2024-08-26 14:23:16,572	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3057597)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00003_3_alpha_d_ff=4,batch_size=8,d_core=512,d_model=512,dropout=0.0111,e_layers=4,learning_rate=0.0005,lradj=type1_2024-08-26_14-21-57/checkpoint_000005)
2024-08-26 14:23:19,777	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:23,058	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3060870)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00008_8_alpha_d_ff=3,batch_size=16,d_core=512,d_model=512,dropout=0.0041,e_layers=4,learning_rate=0.0009,lradj=type1_2024-08-26_14-22-47/checkpoint_000005)[32m [repeated 2x across cluster][0m
2024-08-26 14:23:26,291	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:26,294	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007' in 0.0019s.
[36m(_train_fn pid=3060870)[0m Epoch: 3 cost time: 5.443658351898193
[36m(_train_fn pid=3060870)[0m Epoch: 3, Steps: 529 | Train Loss: 0.5179436 Vali Loss: 0.2711454 Best vali loss: 0.2711454
[36m(_train_fn pid=3057597)[0m 	iters: 900, epoch: 6 | loss: 0.2609846[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3057597)[0m 	speed: 0.0107s/iter; left time: 24.4011s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3060870)[0m Updating learning rate to 0.00011035690656013475
[36m(_train_fn pid=3060870)[0m saving checkpoint...
[36m(_train_fn pid=3060870)[0m Validation loss decreased (0.2711 --> 0.2698).  Saving model state dict ...
[36m(_train_fn pid=3060870)[0m Epoch: 4 cost time: 5.580353260040283
[36m(_train_fn pid=3060870)[0m Epoch: 4, Steps: 529 | Train Loss: 0.5171955 Vali Loss: 0.2698165 Best vali loss: 0.2698165

Trial trial-c92a5_00003 completed after 6 iterations at 2024-08-26 14:23:16. Total running time: 1min 18s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                            12.18702 â”‚
â”‚ time_total_s                                77.29483 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.27056 â”‚
â”‚ train_loss                                    0.5171 â”‚
â”‚ valid_loss                                   0.27056 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3060870)[0m 	iters: 500, epoch: 5 | loss: 0.2653393[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3060870)[0m 	speed: 0.0054s/iter; left time: 8.7506s[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3060870)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3060870)[0m Epoch: 5 cost time: 2.892956495285034
[36m(_train_fn pid=3060870)[0m Epoch: 5, Steps: 529 | Train Loss: 0.5169538 Vali Loss: 0.2702158 Best vali loss: 0.2698165
[36m(_train_fn pid=3060870)[0m Updating learning rate to 5.517845328006738e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060870)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3057597)[0m Validation loss decreased (0.2726 --> 0.2706).  Saving model state dict ...
[36m(_train_fn pid=3060870)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3060870)[0m Epoch: 6 cost time: 2.895219087600708
[36m(_train_fn pid=3060870)[0m Epoch: 6, Steps: 529 | Train Loss: 0.5163950 Vali Loss: 0.2708212 Best vali loss: 0.2698165
[36m(_train_fn pid=3060870)[0m 	iters: 300, epoch: 7 | loss: 0.2825904[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3060870)[0m 	speed: 0.0052s/iter; left time: 3.9615s[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3060870)[0m EarlyStopping counter: 3 out of 3

Trial trial-c92a5_00008 completed after 7 iterations at 2024-08-26 14:23:26. Total running time: 1min 28s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-c92a5_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             3.23046 â”‚
â”‚ time_total_s                                36.66201 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26982 â”‚
â”‚ train_loss                                   0.51634 â”‚
â”‚ valid_loss                                   0.27078 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:23:26. Total running time: 1min 28s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: c92a5_00000 with best_valid_loss=0.2164555068289998 and params={'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-c92a5_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         5           12.3185        0.349537       0.221555            0.216456 â”‚
â”‚ trial-c92a5_00001   TERMINATED              8       0.00214255           32              3        256            1   0.00394089    cosine         4           35.3212        0.350227       0.227304            0.220315 â”‚
â”‚ trial-c92a5_00002   TERMINATED              8       0.00039933          512              3         32            2   0.000898698   cosine         5           49.9795        0.310214       0.239714            0.220079 â”‚
â”‚ trial-c92a5_00003   TERMINATED              8       0.000466361         512              4        512            4   0.0111416     type1          6           77.2948        0.517099       0.270558            0.270558 â”‚
â”‚ trial-c92a5_00004   TERMINATED              8       0.000813555         512              3         64            3   0.00198793    type1          6           34.944         0.516975       0.27038             0.26622  â”‚
â”‚ trial-c92a5_00005   TERMINATED            128       0.000915799         256              2        256            1   0.00609602    cosine         4            2.69015       0.34345        0.226044            0.21709  â”‚
â”‚ trial-c92a5_00006   TERMINATED            128       0.00151893          512              4         32            2   0.00267208    cosine         6            7.31934       0.346972       0.227329            0.226037 â”‚
â”‚ trial-c92a5_00007   TERMINATED            128       0.000258119          32              1         64            3   0.00671421    type1          8            7.61011       0.437713       0.233022            0.233022 â”‚
â”‚ trial-c92a5_00008   TERMINATED             16       0.000882855         512              3        512            4   0.00414466    type1          7           36.662         0.516336       0.270782            0.269816 â”‚
â”‚ trial-c92a5_00009   TERMINATED             64       7.50928e-05          64              1        256            1   0.00159893    cosine         8            4.99021       0.438413       0.228746            0.228746 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=3060870)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1007/trial-c92a5_00008_8_alpha_d_ff=3,batch_size=16,d_core=512,d_model=512,dropout=0.0041,e_layers=4,learning_rate=0.0009,lradj=type1_2024-08-26_14-22-47/checkpoint_000006)
[36m(_train_fn pid=3060870)[0m Epoch: 7 cost time: 2.8310177326202393
[36m(_train_fn pid=3060870)[0m Epoch: 7, Steps: 529 | Train Loss: 0.5163358 Vali Loss: 0.2707820 Best vali loss: 0.2698165
[36m(_train_fn pid=3060870)[0m Early stopping
[36m(_train_fn pid=3060870)[0m Updating learning rate to 1.3794613320016844e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060870)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060870)[0m 	iters: 500, epoch: 7 | loss: 0.3007349[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3060870)[0m 	speed: 0.0052s/iter; left time: 2.9111s[32m [repeated 2x across cluster][0m


Time taken (4 parallel trials): 93 seconds


2024-08-26 14:23:30,225	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:23:30,636	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:23:30,642	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:23:30,649	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:23:33,371	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3065260)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008/trial-009b7_00003_3_alpha_d_ff=4,batch_size=128,d_core=128,d_model=32,dropout=0.0027,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_14-23-30/checkpoint_000000)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed1008   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator       â”‚
â”‚ Scheduler                        FIFOScheduler               â”‚
â”‚ Number of trials                 10                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-23-29_789946_3063035/artifacts/2024-08-26_14-23-30/ETTh2_96_96_test_seed1008/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:23:30. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-009b7_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-009b7_00001   PENDING              16       0.00211384           32              3         64            1   0.0114659    type1   â”‚
â”‚ trial-009b7_00002   PENDING              16       0.000313846         512              3         32            4   0.00219474   cosine  â”‚
â”‚ trial-009b7_00003   PENDING             128       0.000125414          32              4        128            1   0.00271787   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-009b7_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00272 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00013 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3065260)[0m configuration
[36m(_train_fn pid=3065260)[0m {'batch_size': 128, 'learning_rate': 0.00012541395839329412, 'd_model': 32, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0027178683505576316, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=3065260)[0m Use GPU: cuda:0

Trial trial-009b7_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.01147 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00211 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-009b7_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00219 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00031 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-009b7_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3065259)[0m {'batch_size': 16, 'learning_rate': 0.00031384644400941404, 'd_model': 512, 'd_core': 32, 'e_layers': 4, 'dropout': 0.0021947390867552317, 'lradj': 'cosine', 'd_ff': 1536}
[36m(_train_fn pid=3065260)[0m train 8449
[36m(_train_fn pid=3065258)[0m val 2785
[36m(_train_fn pid=3065258)[0m start_epoch 0
[36m(_train_fn pid=3065258)[0m max_epoch 8
[36m(_train_fn pid=3065260)[0m Updating learning rate to 0.00012541395839329412
[36m(_train_fn pid=3065260)[0m saving checkpoint...
[36m(_train_fn pid=3065260)[0m Validation loss decreased (inf --> 0.2782).  Saving model state dict ...
[36m(_train_fn pid=3065260)[0m Epoch: 1 cost time: 0.6056618690490723
[36m(_train_fn pid=3065260)[0m Epoch: 1, Steps: 67 | Train Loss: 0.5551415 Vali Loss: 0.2781714 Best vali loss: 0.2781714
2024-08-26 14:23:33,891	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3065260)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008/trial-009b7_00003_3_alpha_d_ff=4,batch_size=128,d_core=128,d_model=32,dropout=0.0027,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_14-23-30/checkpoint_000001)
2024-08-26 14:23:34,397	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3065260)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008/trial-009b7_00003_3_alpha_d_ff=4,batch_size=128,d_core=128,d_model=32,dropout=0.0027,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_14-23-30/checkpoint_000002)
2024-08-26 14:23:34,905	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3065260)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008/trial-009b7_00003_3_alpha_d_ff=4,batch_size=128,d_core=128,d_model=32,dropout=0.0027,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_14-23-30/checkpoint_000003)
2024-08-26 14:23:35,213	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:35,404	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:35,905	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:36,413	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:36,900	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:37,351	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:37,410	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:39,167	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3065257)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008/trial-009b7_00000_0_2024-08-26_14-23-30/checkpoint_000002)[32m [repeated 8x across cluster][0m
2024-08-26 14:23:39,247	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:40,958	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:41,115	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:41,820	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:43,081	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3065258)[0m 	iters: 100, epoch: 1 | loss: 0.3747082
[36m(_train_fn pid=3065258)[0m 	speed: 0.0120s/iter; left time: 49.4093s
[36m(_train_fn pid=3065260)[0m Updating learning rate to 6.270697919664706e-05
[36m(_train_fn pid=3065260)[0m saving checkpoint...
[36m(_train_fn pid=3065260)[0m Validation loss decreased (0.2782 --> 0.2632).  Saving model state dict ...
[36m(_train_fn pid=3065260)[0m Epoch: 2 cost time: 0.3882913589477539
[36m(_train_fn pid=3065260)[0m Epoch: 2, Steps: 67 | Train Loss: 0.5115362 Vali Loss: 0.2632500 Best vali loss: 0.2632500
[36m(_train_fn pid=3065260)[0m Updating learning rate to 3.135348959832353e-05
[36m(_train_fn pid=3065260)[0m saving checkpoint...
[36m(_train_fn pid=3065260)[0m Validation loss decreased (0.2632 --> 0.2575).  Saving model state dict ...
[36m(_train_fn pid=3065260)[0m Epoch: 3 cost time: 0.3809630870819092
[36m(_train_fn pid=3065260)[0m Epoch: 3, Steps: 67 | Train Loss: 0.4922177 Vali Loss: 0.2575106 Best vali loss: 0.2575106
[36m(_train_fn pid=3065260)[0m Updating learning rate to 1.5676744799161764e-05
[36m(_train_fn pid=3065260)[0m saving checkpoint...
[36m(_train_fn pid=3065260)[0m Validation loss decreased (0.2575 --> 0.2549).  Saving model state dict ...
[36m(_train_fn pid=3065260)[0m Epoch: 4 cost time: 0.37952160835266113
[36m(_train_fn pid=3065260)[0m Epoch: 4, Steps: 67 | Train Loss: 0.4850044 Vali Loss: 0.2549496 Best vali loss: 0.2549496
[36m(_train_fn pid=3065260)[0m Validation loss decreased (0.2549 --> 0.2537).  Saving model state dict ...
[36m(_train_fn pid=3065260)[0m Validation loss decreased (0.2537 --> 0.2531).  Saving model state dict ...
[36m(_train_fn pid=3065260)[0m Validation loss decreased (0.2531 --> 0.2528).  Saving model state dict ...

Trial trial-009b7_00003 completed after 8 iterations at 2024-08-26 14:23:36. Total running time: 6s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.48415 â”‚
â”‚ time_total_s                                 4.68412 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.25266 â”‚
â”‚ train_loss                                   0.48301 â”‚
â”‚ valid_loss                                   0.25266 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3065260)[0m Validation loss decreased (0.2528 --> 0.2527).  Saving model state dict ...
[36m(_train_fn pid=3065257)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=3065258)[0m {'batch_size': 16, 'learning_rate': 0.0021138370213937157, 'd_model': 32, 'd_core': 64, 'e_layers': 1, 'dropout': 0.01146592567435474, 'lradj': 'type1', 'd_ff': 96}
[36m(_train_fn pid=3065257)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3065257)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=3065257)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3065257)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3065257)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3065257)[0m max_epoch 8[32m [repeated 3x across cluster][0m

Trial trial-009b7_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00708 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00034 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3066458)[0m {'batch_size': 16, 'learning_rate': 0.0003363473848958155, 'd_model': 512, 'd_core': 64, 'e_layers': 3, 'dropout': 0.007080274560457111, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=3065257)[0m Updating learning rate to 0.00025606601717798207[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3065257)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3065258)[0m Validation loss decreased (inf --> 0.2197).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3065258)[0m Epoch: 1 cost time: 3.913919448852539[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3065258)[0m Epoch: 1, Steps: 529 | Train Loss: 0.4357927 Vali Loss: 0.2196582 Best vali loss: 0.2196582[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3065257)[0m 	iters: 200, epoch: 3 | loss: 0.4187849[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=3065257)[0m 	speed: 0.0056s/iter; left time: 7.7781s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=3065257)[0m Validation loss decreased (0.2185 --> 0.2148).  Saving model state dict ...
[36m(_train_fn pid=3065257)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3065258)[0m Validation loss decreased (0.2197 --> 0.2192).  Saving model state dict ...
[36m(_train_fn pid=3065257)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3066458)[0m configuration
[36m(_train_fn pid=3066458)[0m Use GPU: cuda:0
[36m(_train_fn pid=3066458)[0m train 8449

Trial trial-009b7_00000 completed after 5 iterations at 2024-08-26 14:23:43. Total running time: 12s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.96249 â”‚
â”‚ time_total_s                                10.78479 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21483 â”‚
â”‚ train_loss                                   0.34732 â”‚
â”‚ valid_loss                                   0.22216 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3065257)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3065257)[0m Early stopping
[36m(_train_fn pid=3066458)[0m val 2785
[36m(_train_fn pid=3066458)[0m start_epoch 0
[36m(_train_fn pid=3066458)[0m max_epoch 8
[36m(_train_fn pid=3065257)[0m Updating learning rate to 9.259748514523653e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3065257)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
2024-08-26 14:23:44,236	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:44,500	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:44,508	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3065258)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008/trial-009b7_00001_1_alpha_d_ff=3,batch_size=16,d_core=64,d_model=32,dropout=0.0115,e_layers=1,learning_rate=0.0021,lradj=type1_2024-08-26_14-23-30/checkpoint_000002)[32m [repeated 7x across cluster][0m
2024-08-26 14:23:47,126	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:48,484	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:49,664	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:49,721	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3065259)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008/trial-009b7_00002_2_alpha_d_ff=3,batch_size=16,d_core=32,d_model=512,dropout=0.0022,e_layers=4,learning_rate=0.0003,lradj=cosine_2024-08-26_14-23-30/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 14:23:51,063	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:52,203	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:53,651	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3066458)[0m Validation loss decreased (inf --> 0.2195).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3065257)[0m Epoch: 5 cost time: 1.6321401596069336[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3065257)[0m Epoch: 5, Steps: 265 | Train Loss: 0.3473250 Vali Loss: 0.2221581 Best vali loss: 0.2148268[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3066458)[0m 	iters: 500, epoch: 2 | loss: 0.7229229[32m [repeated 27x across cluster][0m
[36m(_train_fn pid=3066458)[0m 	speed: 0.0045s/iter; left time: 14.3062s[32m [repeated 27x across cluster][0m
[36m(_train_fn pid=3065258)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m

Trial trial-009b7_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00206 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00132 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3067108)[0m configuration
[36m(_train_fn pid=3067108)[0m {'batch_size': 16, 'learning_rate': 0.0013191565527330515, 'd_model': 256, 'd_core': 64, 'e_layers': 3, 'dropout': 0.002057778330735626, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=3067108)[0m Use GPU: cuda:0
[36m(_train_fn pid=3067108)[0m train 8449
[36m(_train_fn pid=3067108)[0m val 2785
[36m(_train_fn pid=3067108)[0m start_epoch 0
[36m(_train_fn pid=3067108)[0m max_epoch 8
[36m(_train_fn pid=3065258)[0m Updating learning rate to 0.00026422962767421446[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3065258)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3065258)[0m Epoch: 4 cost time: 3.128462314605713[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3065258)[0m Epoch: 4, Steps: 529 | Train Loss: 0.3512538 Vali Loss: 0.2241540 Best vali loss: 0.2192296[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3065258)[0m 	iters: 100, epoch: 5 | loss: 0.7424600[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=3065258)[0m 	speed: 0.0161s/iter; left time: 32.5126s[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=3065259)[0m Validation loss decreased (0.2239 --> 0.2211).  Saving model state dict ...
[36m(_train_fn pid=3065258)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m

Trial trial-009b7_00004 completed after 4 iterations at 2024-08-26 14:23:49. Total running time: 19s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.59206 â”‚
â”‚ time_total_s                                11.27687 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21952 â”‚
â”‚ train_loss                                   0.26292 â”‚
â”‚ valid_loss                                    0.2385 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3066458)[0m Early stopping
[36m(_train_fn pid=3067108)[0m Validation loss decreased (inf --> 0.2554).  Saving model state dict ...

Trial trial-009b7_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00193 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3067709)[0m configuration
[36m(_train_fn pid=3067709)[0m {'batch_size': 64, 'learning_rate': 6.195069120386388e-05, 'd_model': 64, 'd_core': 64, 'e_layers': 4, 'dropout': 0.0019274647635264472, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=3067709)[0m Use GPU: cuda:0
[36m(_train_fn pid=3067709)[0m train 8449
[36m(_train_fn pid=3067709)[0m val 2785
[36m(_train_fn pid=3067709)[0m start_epoch 0
[36m(_train_fn pid=3067709)[0m max_epoch 8

Trial trial-009b7_00001 completed after 5 iterations at 2024-08-26 14:23:52. Total running time: 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             3.71684 â”‚
â”‚ time_total_s                                19.95687 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21923 â”‚
â”‚ train_loss                                   0.34072 â”‚
â”‚ valid_loss                                   0.22752 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-009b7_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00154 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00008 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3067939)[0m {'batch_size': 128, 'learning_rate': 8.432427450575295e-05, 'd_model': 512, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0015418682030659719, 'lradj': 'type1', 'd_ff': 1536}
[36m(_train_fn pid=3067709)[0m Updating learning rate to 5.959283341602036e-05[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3067709)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
2024-08-26 14:23:54,961	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3067939)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008/trial-009b7_00007_7_alpha_d_ff=3,batch_size=128,d_core=128,d_model=512,dropout=0.0015,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_14-23-52/checkpoint_000000)[32m [repeated 5x across cluster][0m
2024-08-26 14:23:55,243	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:55,589	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:55,686	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:56,220	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:56,751	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:56,843	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:57,107	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:57,473	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:58,103	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:58,371	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:58,717	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:23:59,340	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:00,050	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3067709)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008/trial-009b7_00006_6_alpha_d_ff=2,batch_size=64,d_core=64,d_model=64,dropout=0.0019,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-23-49/checkpoint_000004)[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3067709)[0m Epoch: 1 cost time: 1.7753565311431885[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3067709)[0m Epoch: 1, Steps: 133 | Train Loss: 0.5295357 Vali Loss: 0.2588045 Best vali loss: 0.2588045[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3065259)[0m 	iters: 500, epoch: 4 | loss: 0.5059720[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3065259)[0m 	speed: 0.0102s/iter; left time: 21.9610s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3065258)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3065258)[0m Early stopping
[36m(_train_fn pid=3067709)[0m Validation loss decreased (0.2588 --> 0.2409).  Saving model state dict ...
[36m(_train_fn pid=3067939)[0m Validation loss decreased (inf --> 0.2218).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3067939)[0m configuration
[36m(_train_fn pid=3067939)[0m Use GPU: cuda:0
[36m(_train_fn pid=3067939)[0m train 8449
[36m(_train_fn pid=3067939)[0m val 2785
[36m(_train_fn pid=3067939)[0m start_epoch 0
[36m(_train_fn pid=3067939)[0m max_epoch 8
[36m(_train_fn pid=3067939)[0m Updating learning rate to 1.31756678915239e-06[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3067939)[0m saving checkpoint...[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3067939)[0m Epoch: 7 cost time: 0.49141836166381836[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3067939)[0m Epoch: 7, Steps: 67 | Train Loss: 0.3842499 Vali Loss: 0.2136128 Best vali loss: 0.2135262[32m [repeated 12x across cluster][0m

Trial trial-009b7_00007 completed after 8 iterations at 2024-08-26 14:23:59. Total running time: 28s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                              0.6206 â”‚
â”‚ time_total_s                                 5.66291 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.21353 â”‚
â”‚ train_loss                                   0.38366 â”‚
â”‚ valid_loss                                   0.21369 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3065259)[0m 	iters: 400, epoch: 5 | loss: 0.1747523[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3065259)[0m 	speed: 0.0107s/iter; left time: 18.3918s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3067939)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3067709)[0m Validation loss decreased (0.2294 --> 0.2273).  Saving model state dict ...[32m [repeated 8x across cluster][0m

Trial trial-009b7_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00105 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00302 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 5 TERMINATED | 4 RUNNING
Current time: 2024-08-26 14:24:00. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 009b7_00007 with best_valid_loss=0.21352622060630222 and params={'batch_size': 128, 'learning_rate': 8.432427450575295e-05, 'd_model': 512, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0015418682030659719, 'lradj': 'type1', 'd_ff': 1536}
2024-08-26 14:24:01,614	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:01,926	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:03,332	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:03,454	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:05,112	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:07,634	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3069582)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008/trial-009b7_00009_9_alpha_d_ff=1,batch_size=128,d_core=32,d_model=32,dropout=0.0014,e_layers=1,learning_rate=0.0021,lradj=cosine_2024-08-26_14-24-05/checkpoint_000000)[32m [repeated 6x across cluster][0m
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-009b7_00002   RUNNING                16       0.000313846         512              3         32            4   0.00219474   cosine         4           23.4105        0.327296       0.224741            0.221104 â”‚
â”‚ trial-009b7_00005   RUNNING                16       0.00131916          256              2         64            3   0.00205778   cosine         2           12.4697        0.45886        0.227796            0.227796 â”‚
â”‚ trial-009b7_00006   RUNNING                64       6.19507e-05          64              2         64            4   0.00192746   cosine         5            8.82933       0.431944       0.227328            0.227328 â”‚
â”‚ trial-009b7_00008   RUNNING                 8       0.00301686           32              1         32            3   0.00105389   cosine                                                                                â”‚
â”‚ trial-009b7_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5           10.7848        0.347325       0.222158            0.214827 â”‚
â”‚ trial-009b7_00001   TERMINATED             16       0.00211384           32              3         64            1   0.0114659    type1          5           19.9569        0.340725       0.227518            0.21923  â”‚
â”‚ trial-009b7_00003   TERMINATED            128       0.000125414          32              4        128            1   0.00271787   type1          8            4.68412       0.483008       0.252657            0.252657 â”‚
â”‚ trial-009b7_00004   TERMINATED             16       0.000336347         512              1         64            3   0.00708027   cosine         4           11.2769        0.262916       0.238503            0.219522 â”‚
â”‚ trial-009b7_00007   TERMINATED            128       8.43243e-05         512              3        128            1   0.00154187   type1          8            5.66291       0.383663       0.213687            0.213526 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3069058)[0m configuration
[36m(_train_fn pid=3069058)[0m {'batch_size': 8, 'learning_rate': 0.003016859073121743, 'd_model': 32, 'd_core': 32, 'e_layers': 3, 'dropout': 0.001053891876991033, 'lradj': 'cosine', 'd_ff': 32}
[36m(_train_fn pid=3069058)[0m Use GPU: cuda:0
[36m(_train_fn pid=3069058)[0m train 8449
[36m(_train_fn pid=3069058)[0m val 2785
[36m(_train_fn pid=3069058)[0m start_epoch 0
[36m(_train_fn pid=3069058)[0m max_epoch 8
[36m(_train_fn pid=3067108)[0m Updating learning rate to 0.0009119879550799177[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3067108)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3067108)[0m Epoch: 3 cost time: 5.287228584289551[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3067108)[0m Epoch: 3, Steps: 529 | Train Loss: 0.4369687 Vali Loss: 0.2376527 Best vali loss: 0.2277958[32m [repeated 6x across cluster][0m

Trial trial-009b7_00006 completed after 8 iterations at 2024-08-26 14:24:05. Total running time: 34s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.77718 â”‚
â”‚ time_total_s                                13.88368 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22564 â”‚
â”‚ train_loss                                   0.42991 â”‚
â”‚ valid_loss                                   0.22564 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3067108)[0m 	iters: 100, epoch: 4 | loss: 0.5583245[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3067108)[0m 	speed: 0.0261s/iter; left time: 66.4837s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3067108)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3067709)[0m Validation loss decreased (0.2258 --> 0.2256).  Saving model state dict ...[32m [repeated 3x across cluster][0m

Trial trial-009b7_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                             0.0014 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00206 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3069582)[0m configuration
[36m(_train_fn pid=3069582)[0m {'batch_size': 128, 'learning_rate': 0.0020597636363955483, 'd_model': 32, 'd_core': 32, 'e_layers': 1, 'dropout': 0.0013996656540216062, 'lradj': 'cosine', 'd_ff': 32}
[36m(_train_fn pid=3069582)[0m Use GPU: cuda:0
[36m(_train_fn pid=3069582)[0m train 8449
[36m(_train_fn pid=3069582)[0m val 2785
[36m(_train_fn pid=3069582)[0m start_epoch 0
[36m(_train_fn pid=3069582)[0m max_epoch 8
2024-08-26 14:24:08,003	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:08,372	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:08,586	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:08,758	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:09,128	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:09,137	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:11,487	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:12,398	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:16,033	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3069058)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008/trial-009b7_00008_8_alpha_d_ff=1,batch_size=8,d_core=32,d_model=32,dropout=0.0011,e_layers=3,learning_rate=0.0030,lradj=cosine_2024-08-26_14-23-59/checkpoint_000001)[32m [repeated 9x across cluster][0m
2024-08-26 14:24:20,292	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:24,960	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:24,971	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008' in 0.0020s.
[36m(_train_fn pid=3069582)[0m Validation loss decreased (inf --> 0.2215).  Saving model state dict ...

Trial trial-009b7_00002 completed after 6 iterations at 2024-08-26 14:24:08. Total running time: 37s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             6.65726 â”‚
â”‚ time_total_s                                36.30498 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                               0.2211 â”‚
â”‚ train_loss                                   0.23996 â”‚
â”‚ valid_loss                                   0.24214 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3065259)[0m Early stopping

Trial trial-009b7_00009 completed after 5 iterations at 2024-08-26 14:24:09. Total running time: 38s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             0.36759 â”‚
â”‚ time_total_s                                 2.48666 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21822 â”‚
â”‚ train_loss                                   0.37206 â”‚
â”‚ valid_loss                                   0.22202 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3067108)[0m 
[36m(_train_fn pid=3069582)[0m Updating learning rate to 0.0006357631090794506[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3069582)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3069582)[0m Epoch: 5 cost time: 0.2590208053588867[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3069582)[0m Epoch: 5, Steps: 67 | Train Loss: 0.3720597 Vali Loss: 0.2220197 Best vali loss: 0.2182197[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3067108)[0m 	iters: 200, epoch: 5 | loss: 0.5510288[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3067108)[0m 	speed: 0.0053s/iter; left time: 10.0760s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3069582)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3069582)[0m Validation loss decreased (0.2215 --> 0.2182).  Saving model state dict ...

Trial trial-009b7_00005 completed after 5 iterations at 2024-08-26 14:24:12. Total running time: 41s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             3.25868 â”‚
â”‚ time_total_s                                27.75232 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                               0.2278 â”‚
â”‚ train_loss                                   0.40338 â”‚
â”‚ valid_loss                                   0.22845 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3069058)[0m Validation loss decreased (inf --> 0.2241).  Saving model state dict ...
[36m(_train_fn pid=3067108)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3067108)[0m Updating learning rate to 0.0004071685976531339[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3067108)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3067108)[0m Epoch: 5 cost time: 2.794933319091797[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3067108)[0m Epoch: 5, Steps: 529 | Train Loss: 0.4033777 Vali Loss: 0.2284452 Best vali loss: 0.2277958[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3069058)[0m 	iters: 1000, epoch: 2 | loss: 0.9076004[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3069058)[0m 	speed: 0.0034s/iter; left time: 22.0452s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3069058)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3069058)[0m Updating learning rate to 0.0020856805290928675[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3069058)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3069058)[0m Epoch: 3 cost time: 3.717750072479248[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3069058)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.3970167 Vali Loss: 0.2309571 Best vali loss: 0.2240694[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3069058)[0m 	iters: 200, epoch: 4 | loss: 0.2295716[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3069058)[0m 	speed: 0.0038s/iter; left time: 19.5798s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3069058)[0m EarlyStopping counter: 2 out of 3

Trial trial-009b7_00008 completed after 4 iterations at 2024-08-26 14:24:24. Total running time: 54s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-009b7_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             4.66502 â”‚
â”‚ time_total_s                                24.09475 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22407 â”‚
â”‚ train_loss                                   0.36745 â”‚
â”‚ valid_loss                                   0.22742 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:24:24. Total running time: 54s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 009b7_00007 with best_valid_loss=0.21352622060630222 and params={'batch_size': 128, 'learning_rate': 8.432427450575295e-05, 'd_model': 512, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0015418682030659719, 'lradj': 'type1', 'd_ff': 1536}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-009b7_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5           10.7848        0.347325       0.222158            0.214827 â”‚
â”‚ trial-009b7_00001   TERMINATED             16       0.00211384           32              3         64            1   0.0114659    type1          5           19.9569        0.340725       0.227518            0.21923  â”‚
â”‚ trial-009b7_00002   TERMINATED             16       0.000313846         512              3         32            4   0.00219474   cosine         6           36.305         0.239962       0.242138            0.221104 â”‚
â”‚ trial-009b7_00003   TERMINATED            128       0.000125414          32              4        128            1   0.00271787   type1          8            4.68412       0.483008       0.252657            0.252657 â”‚
â”‚ trial-009b7_00004   TERMINATED             16       0.000336347         512              1         64            3   0.00708027   cosine         4           11.2769        0.262916       0.238503            0.219522 â”‚
â”‚ trial-009b7_00005   TERMINATED             16       0.00131916          256              2         64            3   0.00205778   cosine         5           27.7523        0.403378       0.228445            0.227796 â”‚
â”‚ trial-009b7_00006   TERMINATED             64       6.19507e-05          64              2         64            4   0.00192746   cosine         8           13.8837        0.429913       0.225638            0.225638 â”‚
â”‚ trial-009b7_00007   TERMINATED            128       8.43243e-05         512              3        128            1   0.00154187   type1          8            5.66291       0.383663       0.213687            0.213526 â”‚
â”‚ trial-009b7_00008   TERMINATED              8       0.00301686           32              1         32            3   0.00105389   cosine         4           24.0947        0.36745        0.227423            0.224069 â”‚
â”‚ trial-009b7_00009   TERMINATED            128       0.00205976           32              1         32            1   0.00139967   cosine         5            2.48666       0.37206        0.22202             0.21822  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 128, 'learning_rate': 8.432427450575295e-05, 'd_model': 512, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0015418682030659719, 'lradj': 'type1', 'd_ff': 1536}
[36m(_train_fn pid=3069058)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1008/trial-009b7_00008_8_alpha_d_ff=1,batch_size=8,d_core=32,d_model=32,dropout=0.0011,e_layers=3,learning_rate=0.0030,lradj=cosine_2024-08-26_14-23-59/checkpoint_000003)[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3069058)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=3069058)[0m Early stopping
[36m(_train_fn pid=3069058)[0m Updating learning rate to 0.0015084295365608714
[36m(_train_fn pid=3069058)[0m saving checkpoint...
[36m(_train_fn pid=3069058)[0m Epoch: 4 cost time: 4.135168790817261
[36m(_train_fn pid=3069058)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3674496 Vali Loss: 0.2274234 Best vali loss: 0.2240694
[36m(_train_fn pid=3069058)[0m 	iters: 1000, epoch: 4 | loss: 0.2529941[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3069058)[0m 	speed: 0.0039s/iter; left time: 16.6171s[32m [repeated 8x across cluster][0m


Time taken (4 parallel trials): 59 seconds


2024-08-26 14:24:29,104	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:24:29,496	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:24:29,501	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:24:29,508	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:24:33,111	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3072737)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1009/trial-23b3e_00002_2_alpha_d_ff=1,batch_size=64,d_core=128,d_model=256,dropout=0.0012,e_layers=2,learning_rate=0.0007,lradj=cosine_2024-08-26_14-24-29/checkpoint_000000)
2024-08-26 14:24:33,284	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:33,539	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:34,409	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:34,508	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:34,679	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:35,167	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:35,670	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed1009   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator       â”‚
â”‚ Scheduler                        FIFOScheduler               â”‚
â”‚ Number of trials                 10                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1009
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-24-28_671120_3070512/artifacts/2024-08-26_14-24-29/ETTh2_96_96_test_seed1009/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:24:29. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-23b3e_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-23b3e_00001   PENDING              64       0.000225321          32              2         32            3   0.0074879    cosine  â”‚
â”‚ trial-23b3e_00002   PENDING              64       0.00066622          256              1        128            2   0.00120827   cosine  â”‚
â”‚ trial-23b3e_00003   PENDING              64       6.50339e-05         512              1        256            2   0.00169973   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-23b3e_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00121 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00067 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-23b3e_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-23b3e_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00749 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00023 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-23b3e_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                             0.0017 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00007 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3072735)[0m configuration
[36m(_train_fn pid=3072735)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=3072735)[0m Use GPU: cuda:0
[36m(_train_fn pid=3072735)[0m train 8449
[36m(_train_fn pid=3072737)[0m val 2785
[36m(_train_fn pid=3072735)[0m start_epoch 0
[36m(_train_fn pid=3072735)[0m max_epoch 8
[36m(_train_fn pid=3072737)[0m 	iters: 100, epoch: 1 | loss: 0.2970602
[36m(_train_fn pid=3072737)[0m 	speed: 0.0138s/iter; left time: 13.3137s
[36m(_train_fn pid=3072737)[0m Updating learning rate to 0.0006408633938289322
[36m(_train_fn pid=3072737)[0m saving checkpoint...
[36m(_train_fn pid=3072737)[0m Validation loss decreased (inf --> 0.2228).  Saving model state dict ...
[36m(_train_fn pid=3072737)[0m Epoch: 1 cost time: 1.3848285675048828
[36m(_train_fn pid=3072737)[0m Epoch: 1, Steps: 133 | Train Loss: 0.4393327 Vali Loss: 0.2227566 Best vali loss: 0.2227566
[36m(_train_fn pid=3072737)[0m Validation loss decreased (0.2228 --> 0.2203).  Saving model state dict ...
2024-08-26 14:24:36,091	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:36,821	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:37,000	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:37,203	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:37,427	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3072737)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1009/trial-23b3e_00002_2_alpha_d_ff=1,batch_size=64,d_core=128,d_model=256,dropout=0.0012,e_layers=2,learning_rate=0.0007,lradj=cosine_2024-08-26_14-24-29/checkpoint_000004)[32m [repeated 13x across cluster][0m
2024-08-26 14:24:38,274	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:38,467	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:38,709	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:39,533	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:39,770	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:39,779	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:40,664	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:40,824	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:40,987	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:41,397	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:41,453	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:41,795	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3072737)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3072738)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=3072738)[0m {'batch_size': 64, 'learning_rate': 6.503394511764766e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 2, 'dropout': 0.0016997344879154952, 'lradj': 'cosine', 'd_ff': 512}[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3072738)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3072738)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3072738)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3072738)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3072738)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3072737)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=3072737)[0m 	iters: 100, epoch: 5 | loss: 0.1942250[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3072737)[0m 	speed: 0.0127s/iter; left time: 5.5183s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3072737)[0m Updating learning rate to 0.00020563428422409042[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3072737)[0m saving checkpoint...[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3072735)[0m Validation loss decreased (inf --> 0.2166).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3072738)[0m Epoch: 4 cost time: 1.1188268661499023[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3072738)[0m Epoch: 4, Steps: 133 | Train Loss: 0.3731964 Vali Loss: 0.2167919 Best vali loss: 0.2150889[32m [repeated 12x across cluster][0m

Trial trial-23b3e_00002 completed after 5 iterations at 2024-08-26 14:24:38. Total running time: 8s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.27188 â”‚
â”‚ time_total_s                                 7.20014 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22033 â”‚
â”‚ train_loss                                   0.29635 â”‚
â”‚ valid_loss                                   0.23551 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3072737)[0m Early stopping
[36m(_train_fn pid=3072736)[0m Validation loss decreased (0.2274 --> 0.2249).  Saving model state dict ...[32m [repeated 5x across cluster][0m

Trial trial-23b3e_00003 completed after 6 iterations at 2024-08-26 14:24:39. Total running time: 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.05763 â”‚
â”‚ time_total_s                                 8.64249 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.21509 â”‚
â”‚ train_loss                                   0.35075 â”‚
â”‚ valid_loss                                   0.22177 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-23b3e_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                             128 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00192 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00167 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3072735)[0m EarlyStopping counter: 3 out of 3[32m [repeated 7x across cluster][0m

Trial trial-23b3e_00000 completed after 4 iterations at 2024-08-26 14:24:40. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.28912 â”‚
â”‚ time_total_s                                 9.72108 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                               0.2166 â”‚
â”‚ train_loss                                   0.36014 â”‚
â”‚ valid_loss                                   0.22133 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-23b3e_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00257 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00031 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3074314)[0m configuration
[36m(_train_fn pid=3074314)[0m {'batch_size': 128, 'learning_rate': 0.0016690211839602404, 'd_model': 256, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0019162161239354475, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=3074314)[0m Use GPU: cuda:0
[36m(_train_fn pid=3074630)[0m configuration
[36m(_train_fn pid=3074630)[0m {'batch_size': 64, 'learning_rate': 0.00031240532313722794, 'd_model': 32, 'd_core': 256, 'e_layers': 2, 'dropout': 0.0025656452675530354, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=3074630)[0m Use GPU: cuda:0
[36m(_train_fn pid=3074630)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3074630)[0m val 2785[32m [repeated 2x across cluster][0m
2024-08-26 14:24:42,196	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:42,206	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:42,578	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:42,740	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:43,475	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3074630)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1009/trial-23b3e_00005_5_alpha_d_ff=4,batch_size=64,d_core=256,d_model=32,dropout=0.0026,e_layers=2,learning_rate=0.0003,lradj=type1_2024-08-26_14-24-39/checkpoint_000001)[32m [repeated 16x across cluster][0m
2024-08-26 14:24:44,300	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:45,323	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:45,681	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:46,731	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:46,831	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:46,939	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:47,997	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:48,026	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:48,250	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:48,535	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3075506)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1009/trial-23b3e_00008_8_alpha_d_ff=4,batch_size=64,d_core=512,d_model=256,dropout=0.0111,e_layers=3,learning_rate=0.0026,lradj=type1_2024-08-26_14-24-42/checkpoint_000001)[32m [repeated 10x across cluster][0m
2024-08-26 14:24:49,356	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:49,592	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3074630)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3074630)[0m max_epoch 8[32m [repeated 2x across cluster][0m

Trial trial-23b3e_00001 completed after 8 iterations at 2024-08-26 14:24:42. Total running time: 12s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                              0.7508 â”‚
â”‚ time_total_s                                11.08025 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22268 â”‚
â”‚ train_loss                                   0.41724 â”‚
â”‚ valid_loss                                   0.22268 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-23b3e_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00098 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                       0.0002 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-23b3e_00004 completed after 5 iterations at 2024-08-26 14:24:42. Total running time: 13s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             0.37847 â”‚
â”‚ time_total_s                                 2.71986 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22316 â”‚
â”‚ train_loss                                   0.29149 â”‚
â”‚ valid_loss                                   0.23298 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3074630)[0m 	iters: 100, epoch: 1 | loss: 0.4236078[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3074630)[0m 	speed: 0.0095s/iter; left time: 9.2082s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3074630)[0m Updating learning rate to 0.00015620266156861397[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3074630)[0m saving checkpoint...[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3074630)[0m Validation loss decreased (inf --> 0.2427).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3074630)[0m Epoch: 2 cost time: 0.5704326629638672[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=3074630)[0m Epoch: 2, Steps: 133 | Train Loss: 0.4422449 Vali Loss: 0.2276365 Best vali loss: 0.2276365[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=3074314)[0m Early stopping[32m [repeated 3x across cluster][0m

Trial trial-23b3e_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00804 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00013 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3075394)[0m {'batch_size': 64, 'learning_rate': 0.00013438348975532976, 'd_model': 256, 'd_core': 32, 'e_layers': 2, 'dropout': 0.008038196809666323, 'lradj': 'cosine', 'd_ff': 512}

Trial trial-23b3e_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.01107 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00258 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3074630)[0m Validation loss decreased (0.2276 --> 0.2242).  Saving model state dict ...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3074314)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3075506)[0m configuration[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3075506)[0m {'batch_size': 64, 'learning_rate': 0.002581774108769759, 'd_model': 256, 'd_core': 512, 'e_layers': 3, 'dropout': 0.01106621475907545, 'lradj': 'type1', 'd_ff': 1024}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3075506)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3075506)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3075506)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3075506)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3075506)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3075394)[0m 	iters: 100, epoch: 3 | loss: 0.3939766[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3075394)[0m 	speed: 0.0119s/iter; left time: 8.3436s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3075506)[0m Updating learning rate to 0.0012908870543848795[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3075506)[0m saving checkpoint...[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3074886)[0m Validation loss decreased (inf --> 0.2207).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3075506)[0m Epoch: 2 cost time: 1.4717626571655273[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=3075506)[0m Epoch: 2, Steps: 133 | Train Loss: 0.5184646 Vali Loss: 0.2790054 Best vali loss: 0.2686429[32m [repeated 10x across cluster][0m
2024-08-26 14:24:50,387	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:50,739	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:50,926	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:51,996	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:52,149	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:52,801	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:54,698	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3074886)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1009/trial-23b3e_00006_6_alpha_d_ff=1,batch_size=16,d_core=128,d_model=512,dropout=0.0010,e_layers=2,learning_rate=0.0002,lradj=type1_2024-08-26_14-24-40/checkpoint_000002)[32m [repeated 9x across cluster][0m
2024-08-26 14:24:55,717	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:56,653	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:24:58,452	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3075394)[0m Validation loss decreased (0.2168 --> 0.2154).  Saving model state dict ...[32m [repeated 4x across cluster][0m

Trial trial-23b3e_00005 completed after 8 iterations at 2024-08-26 14:24:50. Total running time: 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.37998 â”‚
â”‚ time_total_s                                 9.32128 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22218 â”‚
â”‚ train_loss                                   0.41058 â”‚
â”‚ valid_loss                                   0.22219 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3075394)[0m EarlyStopping counter: 2 out of 3[32m [repeated 7x across cluster][0m

Trial trial-23b3e_00008 completed after 4 iterations at 2024-08-26 14:24:51. Total running time: 22s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.60491 â”‚
â”‚ time_total_s                                 7.84858 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.26864 â”‚
â”‚ train_loss                                   0.51477 â”‚
â”‚ valid_loss                                   0.27265 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3075506)[0m Early stopping

Trial trial-23b3e_00007 completed after 6 iterations at 2024-08-26 14:24:52. Total running time: 22s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.22011 â”‚
â”‚ time_total_s                                 8.31361 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                               0.2154 â”‚
â”‚ train_loss                                   0.35757 â”‚
â”‚ valid_loss                                   0.22162 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-23b3e_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00447 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00085 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3076903)[0m configuration
[36m(_train_fn pid=3076903)[0m {'batch_size': 8, 'learning_rate': 0.0008530343699588276, 'd_model': 256, 'd_core': 512, 'e_layers': 1, 'dropout': 0.004467190703793936, 'lradj': 'type1', 'd_ff': 512}
[36m(_train_fn pid=3076903)[0m Use GPU: cuda:0
[36m(_train_fn pid=3076903)[0m train 8449
[36m(_train_fn pid=3076903)[0m val 2785
[36m(_train_fn pid=3076903)[0m start_epoch 0
[36m(_train_fn pid=3076903)[0m max_epoch 8
[36m(_train_fn pid=3074886)[0m 	iters: 100, epoch: 3 | loss: 0.5416502[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3074886)[0m 	speed: 0.0074s/iter; left time: 22.8620s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=3074886)[0m Updating learning rate to 9.896065260726327e-05[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3074886)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3074886)[0m Epoch: 2 cost time: 4.470780372619629[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3074886)[0m Epoch: 2, Steps: 529 | Train Loss: 0.3688801 Vali Loss: 0.2230878 Best vali loss: 0.2207499[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=3076903)[0m Validation loss decreased (inf --> 0.2226).  Saving model state dict ...
[36m(_train_fn pid=3074886)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m

Trial trial-23b3e_00006 completed after 4 iterations at 2024-08-26 14:24:56. Total running time: 27s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.95208 â”‚
â”‚ time_total_s                                14.26703 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22075 â”‚
â”‚ train_loss                                   0.28802 â”‚
â”‚ valid_loss                                   0.22835 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3074886)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3076903)[0m 	iters: 1000, epoch: 2 | loss: 0.3858122[32m [repeated 29x across cluster][0m
[36m(_train_fn pid=3076903)[0m 	speed: 0.0020s/iter; left time: 12.7004s[32m [repeated 29x across cluster][0m
[36m(_train_fn pid=3076903)[0m Updating learning rate to 0.0004265171849794138[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3076903)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3076903)[0m Epoch: 2 cost time: 2.321190595626831[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3076903)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.4205309 Vali Loss: 0.2242412 Best vali loss: 0.2225669[32m [repeated 4x across cluster][0m

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2024-08-26 14:24:59. Total running time: 30s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
2024-08-26 14:25:01,161	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3076903)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1009/trial-23b3e_00009_9_alpha_d_ff=2,batch_size=8,d_core=512,d_model=256,dropout=0.0045,e_layers=1,learning_rate=0.0009,lradj=type1_2024-08-26_14-24-50/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 14:25:03,919	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:03,930	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1009' in 0.0089s.
Current best trial: 23b3e_00003 with best_valid_loss=0.21508889031367412 and params={'batch_size': 64, 'learning_rate': 6.503394511764766e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 2, 'dropout': 0.0016997344879154952, 'lradj': 'cosine', 'd_ff': 512}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-23b3e_00009   RUNNING                 8       0.000853034         256              2        512            1   0.00446719    type1          2            6.16591       0.420531       0.224241            0.222567 â”‚
â”‚ trial-23b3e_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         4            9.72108       0.360142       0.22133             0.216598 â”‚
â”‚ trial-23b3e_00001   TERMINATED             64       0.000225321          32              2         32            3   0.0074879     cosine         8           11.0803        0.417239       0.222678            0.222678 â”‚
â”‚ trial-23b3e_00002   TERMINATED             64       0.00066622          256              1        128            2   0.00120827    cosine         5            7.20014       0.296354       0.23551             0.220333 â”‚
â”‚ trial-23b3e_00003   TERMINATED             64       6.50339e-05         512              1        256            2   0.00169973    cosine         6            8.64249       0.350748       0.221768            0.215089 â”‚
â”‚ trial-23b3e_00004   TERMINATED            128       0.00166902          256              2        128            1   0.00191622    cosine         5            2.71986       0.291486       0.232978            0.223164 â”‚
â”‚ trial-23b3e_00005   TERMINATED             64       0.000312405          32              4        256            2   0.00256565    type1          8            9.32128       0.410585       0.222187            0.222184 â”‚
â”‚ trial-23b3e_00006   TERMINATED             16       0.000197921         512              1        128            2   0.000978737   type1          4           14.267         0.288021       0.22835             0.22075  â”‚
â”‚ trial-23b3e_00007   TERMINATED             64       0.000134383         256              2         32            2   0.0080382     cosine         6            8.31361       0.357567       0.221616            0.215398 â”‚
â”‚ trial-23b3e_00008   TERMINATED             64       0.00258177          256              4        512            3   0.0110662     type1          4            7.84858       0.514774       0.272649            0.268643 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3076903)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3076903)[0m 	iters: 1000, epoch: 4 | loss: 0.5304430[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=3076903)[0m 	speed: 0.0021s/iter; left time: 9.1888s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=3076903)[0m Updating learning rate to 0.00010662929624485345[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3076903)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3076903)[0m Epoch: 3 cost time: 2.2970380783081055
[36m(_train_fn pid=3076903)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.3677547 Vali Loss: 0.2283190 Best vali loss: 0.2225669
[36m(_train_fn pid=3076903)[0m Epoch: 4 cost time: 2.34291672706604
[36m(_train_fn pid=3076903)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3122982 Vali Loss: 0.2388020 Best vali loss: 0.2225669
[36m(_train_fn pid=3076903)[0m Early stopping

Trial trial-23b3e_00009 completed after 4 iterations at 2024-08-26 14:25:03. Total running time: 34s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-23b3e_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.75556 â”‚
â”‚ time_total_s                                11.62822 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22257 â”‚
â”‚ train_loss                                    0.3123 â”‚
â”‚ valid_loss                                    0.2388 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:25:03. Total running time: 34s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 23b3e_00003 with best_valid_loss=0.21508889031367412 and params={'batch_size': 64, 'learning_rate': 6.503394511764766e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 2, 'dropout': 0.0016997344879154952, 'lradj': 'cosine', 'd_ff': 512}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers       dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-23b3e_00000   TERMINATED             32       0.0003              128              1         64            2   0             cosine         4            9.72108       0.360142       0.22133             0.216598 â”‚
â”‚ trial-23b3e_00001   TERMINATED             64       0.000225321          32              2         32            3   0.0074879     cosine         8           11.0803        0.417239       0.222678            0.222678 â”‚
â”‚ trial-23b3e_00002   TERMINATED             64       0.00066622          256              1        128            2   0.00120827    cosine         5            7.20014       0.296354       0.23551             0.220333 â”‚
â”‚ trial-23b3e_00003   TERMINATED             64       6.50339e-05         512              1        256            2   0.00169973    cosine         6            8.64249       0.350748       0.221768            0.215089 â”‚
â”‚ trial-23b3e_00004   TERMINATED            128       0.00166902          256              2        128            1   0.00191622    cosine         5            2.71986       0.291486       0.232978            0.223164 â”‚
â”‚ trial-23b3e_00005   TERMINATED             64       0.000312405          32              4        256            2   0.00256565    type1          8            9.32128       0.410585       0.222187            0.222184 â”‚
â”‚ trial-23b3e_00006   TERMINATED             16       0.000197921         512              1        128            2   0.000978737   type1          4           14.267         0.288021       0.22835             0.22075  â”‚
â”‚ trial-23b3e_00007   TERMINATED             64       0.000134383         256              2         32            2   0.0080382     cosine         6            8.31361       0.357567       0.221616            0.215398 â”‚
â”‚ trial-23b3e_00008   TERMINATED             64       0.00258177          256              4        512            3   0.0110662     type1          4            7.84858       0.514774       0.272649            0.268643 â”‚
â”‚ trial-23b3e_00009   TERMINATED              8       0.000853034         256              2        512            1   0.00446719    type1          4           11.6282        0.312298       0.238802            0.222567 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 64, 'learning_rate': 6.503394511764766e-05, 'd_model': 512, 'd_core': 256, 'e_layers': 2, 'dropout': 0.0016997344879154952, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=3076903)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed1009/trial-23b3e_00009_9_alpha_d_ff=2,batch_size=8,d_core=512,d_model=256,dropout=0.0045,e_layers=1,learning_rate=0.0009,lradj=type1_2024-08-26_14-24-50/checkpoint_000003)
[36m(_train_fn pid=3076903)[0m EarlyStopping counter: 3 out of 3


Time taken (4 parallel trials): 39 seconds


2024-08-26 14:25:08,092	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 14:25:08,484	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 14:25:08,489	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 14:25:08,497	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 14:25:11,592	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_seed10010   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator        â”‚
â”‚ Scheduler                        FIFOScheduler                â”‚
â”‚ Number of trials                 10                           â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_14-25-07_660693_3077642/artifacts/2024-08-26_14-25-08/ETTh2_96_96_test_seed10010/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 14:25:08. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-3af17_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-3af17_00001   PENDING              64       0.000589633          32              1        512            1   0.00796855   type1   â”‚
â”‚ trial-3af17_00002   PENDING              32       0.000329468         256              2        128            3   0.00461211   cosine  â”‚
â”‚ trial-3af17_00003   PENDING               8       0.00279278           64              3         32            3   0.00522738   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-3af17_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00461 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00033 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-3af17_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00797 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00059 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-3af17_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-3af17_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00523 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00279 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3079864)[0m configuration
[36m(_train_fn pid=3079864)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=3079864)[0m Use GPU: cuda:0
[36m(_train_fn pid=3079865)[0m {'batch_size': 64, 'learning_rate': 0.0005896325724829134, 'd_model': 32, 'd_core': 512, 'e_layers': 1, 'dropout': 0.007968548632312068, 'lradj': 'type1', 'd_ff': 32}
[36m(_train_fn pid=3079864)[0m train 8449
[36m(_train_fn pid=3079864)[0m val 2785
[36m(_train_fn pid=3079864)[0m start_epoch 0
[36m(_train_fn pid=3079864)[0m max_epoch 8
[36m(_train_fn pid=3079865)[0m 	iters: 100, epoch: 1 | loss: 0.3345841
[36m(_train_fn pid=3079865)[0m 	speed: 0.0100s/iter; left time: 9.6432s
[36m(_train_fn pid=3079865)[0m Updating learning rate to 0.0005896325724829134
[36m(_train_fn pid=3079865)[0m saving checkpoint...
[36m(_train_fn pid=3079865)[0m Validation loss decreased (inf --> 0.2312).  Saving model state dict ...
[36m(_train_fn pid=3079865)[0m Epoch: 1 cost time: 0.8674194812774658
[36m(_train_fn pid=3079865)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010/trial-3af17_00001_1_alpha_d_ff=1,batch_size=64,d_core=512,d_model=32,dropout=0.0080,e_layers=1,learning_rate=0.0006,lradj=type1_2024-08-26_14-25-08/checkpoint_000000)
2024-08-26 14:25:12,341	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3079865)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010/trial-3af17_00001_1_alpha_d_ff=1,batch_size=64,d_core=512,d_model=32,dropout=0.0080,e_layers=1,learning_rate=0.0006,lradj=type1_2024-08-26_14-25-08/checkpoint_000001)
2024-08-26 14:25:12,879	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:13,106	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:13,115	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:13,893	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:14,668	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:14,899	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:15,390	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:15,423	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:16,206	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:16,825	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3079864)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010/trial-3af17_00000_0_2024-08-26_14-25-08/checkpoint_000002)[32m [repeated 10x across cluster][0m
2024-08-26 14:25:17,464	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:18,499	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:19,464	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:20,073	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:20,534	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:20,625	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3079865)[0m Epoch: 1, Steps: 133 | Train Loss: 0.4864621 Vali Loss: 0.2312483 Best vali loss: 0.2312483
[36m(_train_fn pid=3079865)[0m Updating learning rate to 0.0002948162862414567
[36m(_train_fn pid=3079865)[0m saving checkpoint...
[36m(_train_fn pid=3079865)[0m Validation loss decreased (0.2312 --> 0.2208).  Saving model state dict ...
[36m(_train_fn pid=3079865)[0m Epoch: 2 cost time: 0.5830016136169434
[36m(_train_fn pid=3079865)[0m Epoch: 2, Steps: 133 | Train Loss: 0.4254736 Vali Loss: 0.2208451 Best vali loss: 0.2208451
[36m(_train_fn pid=3079865)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3079865)[0m Validation loss decreased (0.2208 --> 0.2188).  Saving model state dict ...
[36m(_train_fn pid=3079865)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3079867)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=3079866)[0m {'batch_size': 32, 'learning_rate': 0.0003294680466086627, 'd_model': 256, 'd_core': 128, 'e_layers': 3, 'dropout': 0.004612106182441521, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=3079867)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3079867)[0m {'batch_size': 8, 'learning_rate': 0.00279277715023313, 'd_model': 64, 'd_core': 32, 'e_layers': 3, 'dropout': 0.00522738396441112, 'lradj': 'type1', 'd_ff': 192}
[36m(_train_fn pid=3079867)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3079867)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3079867)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3079867)[0m max_epoch 8[32m [repeated 3x across cluster][0m

Trial trial-3af17_00001 completed after 7 iterations at 2024-08-26 14:25:16. Total running time: 7s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             0.78024 â”‚
â”‚ time_total_s                                 6.10227 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.21883 â”‚
â”‚ train_loss                                   0.39623 â”‚
â”‚ valid_loss                                   0.21914 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3079865)[0m Early stopping
[36m(_train_fn pid=3079866)[0m 	iters: 100, epoch: 3 | loss: 0.3993757[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=3079866)[0m 	speed: 0.0156s/iter; left time: 23.2143s[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=3079865)[0m Updating learning rate to 9.213008945045522e-06[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3079865)[0m saving checkpoint...[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3079866)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3079865)[0m Epoch: 7 cost time: 0.6056230068206787[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3079865)[0m Epoch: 7, Steps: 133 | Train Loss: 0.3962269 Vali Loss: 0.2191388 Best vali loss: 0.2188322[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=3079864)[0m Validation loss decreased (0.2197 --> 0.2185).  Saving model state dict ...

Trial trial-3af17_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00127 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00131 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3081185)[0m {'batch_size': 32, 'learning_rate': 0.001313824248645093, 'd_model': 512, 'd_core': 32, 'e_layers': 2, 'dropout': 0.00126569510404579, 'lradj': 'cosine', 'd_ff': 1536}
[36m(_train_fn pid=3079866)[0m EarlyStopping counter: 2 out of 3[32m [repeated 5x across cluster][0m

Trial trial-3af17_00002 completed after 4 iterations at 2024-08-26 14:25:19. Total running time: 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.99677 â”‚
â”‚ time_total_s                                 9.36777 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22053 â”‚
â”‚ train_loss                                   0.31397 â”‚
â”‚ valid_loss                                   0.22639 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-3af17_00000 completed after 5 iterations at 2024-08-26 14:25:20. Total running time: 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.57089 â”‚
â”‚ time_total_s                                 9.96311 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21854 â”‚
â”‚ train_loss                                   0.34749 â”‚
â”‚ valid_loss                                   0.22122 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3081185)[0m configuration
[36m(_train_fn pid=3081185)[0m Use GPU: cuda:0
[36m(_train_fn pid=3081185)[0m train 8449
[36m(_train_fn pid=3081185)[0m val 2785

Trial trial-3af17_00005 started with configuration:
2024-08-26 14:25:22,574	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3081185)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010/trial-3af17_00004_4_alpha_d_ff=3,batch_size=32,d_core=32,d_model=512,dropout=0.0013,e_layers=2,learning_rate=0.0013,lradj=cosine_2024-08-26_14-25-16/checkpoint_000001)[32m [repeated 7x across cluster][0m
2024-08-26 14:25:24,595	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:26,611	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:26,954	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:29,369	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3081688)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010/trial-3af17_00006_6_alpha_d_ff=2,batch_size=8,d_core=32,d_model=128,dropout=0.0025,e_layers=1,learning_rate=0.0002,lradj=cosine_2024-08-26_14-25-20/checkpoint_000000)[32m [repeated 4x across cluster][0m
2024-08-26 14:25:29,932	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:30,597	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:32,395	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:33,955	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00192 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00302 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3081185)[0m start_epoch 0
[36m(_train_fn pid=3081185)[0m max_epoch 8
[36m(_train_fn pid=3081558)[0m configuration
[36m(_train_fn pid=3081558)[0m Use GPU: cuda:0
[36m(_train_fn pid=3081558)[0m train 8449
[36m(_train_fn pid=3081558)[0m val 2785
[36m(_train_fn pid=3079864)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3079867)[0m 	iters: 200, epoch: 2 | loss: 0.3329187[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=3079867)[0m 	speed: 0.0038s/iter; left time: 27.1756s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=3081558)[0m start_epoch 0
[36m(_train_fn pid=3081558)[0m max_epoch 8

Trial trial-3af17_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00249 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00017 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3079867)[0m Updating learning rate to 0.00279277715023313[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3079867)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3079867)[0m Validation loss decreased (inf --> 0.2643).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3079867)[0m Epoch: 1 cost time: 9.35304856300354[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3079867)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.4954686 Vali Loss: 0.2642813 Best vali loss: 0.2642813[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=3081688)[0m {'batch_size': 8, 'learning_rate': 0.0001678353491209946, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.002489494394656069, 'lradj': 'cosine', 'd_ff': 256}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3081185)[0m EarlyStopping counter: 1 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3081688)[0m configuration
[36m(_train_fn pid=3081688)[0m Use GPU: cuda:0
[36m(_train_fn pid=3081688)[0m train 8449
[36m(_train_fn pid=3081688)[0m val 2785
[36m(_train_fn pid=3081688)[0m 	iters: 800, epoch: 1 | loss: 0.2840390[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=3081688)[0m 	speed: 0.0039s/iter; left time: 29.5691s[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=3081688)[0m start_epoch 0
[36m(_train_fn pid=3081688)[0m max_epoch 8

Trial trial-3af17_00004 completed after 4 iterations at 2024-08-26 14:25:26. Total running time: 18s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.01334 â”‚
â”‚ time_total_s                                 8.81435 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.23957 â”‚
â”‚ train_loss                                   0.43714 â”‚
â”‚ valid_loss                                   0.23986 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3081185)[0m Early stopping
[36m(_train_fn pid=3081558)[0m Updating learning rate to 0.002908331099978839[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3081558)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3081558)[0m Validation loss decreased (inf --> 0.2577).  Saving model state dict ...
[36m(_train_fn pid=3081558)[0m Epoch: 1 cost time: 4.755793333053589[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3081558)[0m Epoch: 1, Steps: 529 | Train Loss: 0.5183579 Vali Loss: 0.2576905 Best vali loss: 0.2576905[32m [repeated 4x across cluster][0m

Trial trial-3af17_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00553 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00135 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3082210)[0m configuration
[36m(_train_fn pid=3082210)[0m Use GPU: cuda:0
[36m(_train_fn pid=3082210)[0m {'batch_size': 64, 'learning_rate': 0.0013454715314037066, 'd_model': 256, 'd_core': 64, 'e_layers': 4, 'dropout': 0.005533416088923823, 'lradj': 'cosine', 'd_ff': 512}
[36m(_train_fn pid=3082210)[0m train 8449
[36m(_train_fn pid=3082210)[0m val 2785
[36m(_train_fn pid=3081185)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3082210)[0m start_epoch 0
[36m(_train_fn pid=3082210)[0m max_epoch 8
[36m(_train_fn pid=3081688)[0m Validation loss decreased (inf --> 0.2165).  Saving model state dict ...
[36m(_train_fn pid=3082210)[0m 	iters: 100, epoch: 2 | loss: 0.2774682[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3082210)[0m 	speed: 0.0181s/iter; left time: 15.0707s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=3082210)[0m Updating learning rate to 0.001294262570422104[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3082210)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3082210)[0m Epoch: 1 cost time: 1.819084882736206[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3082210)[0m Epoch: 1, Steps: 133 | Train Loss: 0.4507705 Vali Loss: 0.2431099 Best vali loss: 0.2431099[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3082210)[0m Validation loss decreased (0.2431 --> 0.2412).  Saving model state dict ...
2024-08-26 14:25:34,048	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:35,859	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3082210)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010/trial-3af17_00007_7_alpha_d_ff=2,batch_size=64,d_core=64,d_model=256,dropout=0.0055,e_layers=4,learning_rate=0.0013,lradj=cosine_2024-08-26_14-25-26/checkpoint_000003)[32m [repeated 6x across cluster][0m
2024-08-26 14:25:37,637	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:39,376	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:39,853	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:41,235	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3081558)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010/trial-3af17_00005_5_alpha_d_ff=2,batch_size=16,d_core=32,d_model=512,dropout=0.0019,e_layers=4,learning_rate=0.0030,lradj=cosine_2024-08-26_14-25-19/checkpoint_000002)[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3081558)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3082210)[0m Validation loss decreased (0.2412 --> 0.2306).  Saving model state dict ...
[36m(_train_fn pid=3082210)[0m Validation loss decreased (inf --> 0.2431).  Saving model state dict ...
[36m(_train_fn pid=3082210)[0m 	iters: 100, epoch: 5 | loss: 0.4090783[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=3082210)[0m 	speed: 0.0180s/iter; left time: 7.7744s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=3082210)[0m Updating learning rate to 0.0006727357657018533[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3082210)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3082210)[0m Epoch: 4 cost time: 1.5906803607940674[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=3082210)[0m Epoch: 4, Steps: 133 | Train Loss: 0.3746930 Vali Loss: 0.2395777 Best vali loss: 0.2306311[32m [repeated 4x across cluster][0m

Trial status: 4 TERMINATED | 4 RUNNING
Current time: 2024-08-26 14:25:38. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 3af17_00006 with best_valid_loss=0.21653995555382444 and params={'batch_size': 8, 'learning_rate': 0.0001678353491209946, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.002489494394656069, 'lradj': 'cosine', 'd_ff': 256}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-3af17_00003   RUNNING                 8       0.00279278           64              3         32            3   0.00522738   type1          2           19.8034        0.510954       0.273385            0.264281 â”‚
â”‚ trial-3af17_00005   RUNNING                16       0.0030234           512              2         32            4   0.00192302   cosine         2           12.956         0.521535       0.267039            0.257691 â”‚
â”‚ trial-3af17_00006   RUNNING                 8       0.000167835         128              2         32            1   0.00248949   cosine         1            7.79438       0.442408       0.21654             0.21654  â”‚
â”‚ trial-3af17_00007   RUNNING                64       0.00134547          256              2         64            4   0.00553342   cosine         5            9.51096       0.341264       0.245617            0.230631 â”‚
â”‚ trial-3af17_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5            9.96311       0.347486       0.221219            0.218544 â”‚
â”‚ trial-3af17_00001   TERMINATED             64       0.000589633          32              1        512            1   0.00796855   type1          7            6.10227       0.396227       0.219139            0.218832 â”‚
â”‚ trial-3af17_00002   TERMINATED             32       0.000329468         256              2        128            3   0.00461211   cosine         4            9.36777       0.313967       0.226394            0.220534 â”‚
â”‚ trial-3af17_00004   TERMINATED             32       0.00131382          512              3         32            2   0.0012657    cosine         4            8.81435       0.437139       0.239857            0.239571 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-3af17_00007 completed after 6 iterations at 2024-08-26 14:25:39. Total running time: 30s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.73561 â”‚
â”‚ time_total_s                                11.24657 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.23063 â”‚
â”‚ train_loss                                   0.31187 â”‚
â”‚ valid_loss                                   0.23923 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3082210)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3082210)[0m Early stopping
[36m(_train_fn pid=3081688)[0m Validation loss decreased (0.2165 --> 0.2165).  Saving model state dict ...

Trial trial-3af17_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  32 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00381 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00014 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3083064)[0m configuration
[36m(_train_fn pid=3083064)[0m {'batch_size': 32, 'learning_rate': 0.00014188181133903137, 'd_model': 32, 'd_core': 32, 'e_layers': 4, 'dropout': 0.003814024667100337, 'lradj': 'cosine', 'd_ff': 32}
[36m(_train_fn pid=3083064)[0m Use GPU: cuda:0
[36m(_train_fn pid=3083064)[0m train 8449
[36m(_train_fn pid=3083064)[0m val 2785
[36m(_train_fn pid=3083064)[0m start_epoch 0
2024-08-26 14:25:42,572	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:43,157	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:44,618	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:46,079	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:46,378	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3081558)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010/trial-3af17_00005_5_alpha_d_ff=2,batch_size=16,d_core=32,d_model=512,dropout=0.0019,e_layers=4,learning_rate=0.0030,lradj=cosine_2024-08-26_14-25-19/checkpoint_000003)[32m [repeated 5x across cluster][0m
2024-08-26 14:25:46,516	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:47,612	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:49,148	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:50,609	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:50,681	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:51,338	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:52,089	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3083064)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010/trial-3af17_00008_8_alpha_d_ff=1,batch_size=32,d_core=32,d_model=32,dropout=0.0038,e_layers=4,learning_rate=0.0001,lradj=cosine_2024-08-26_14-25-39/checkpoint_000006)[32m [repeated 7x across cluster][0m
2024-08-26 14:25:53,348	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:53,567	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:56,778	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:25:57,789	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3083665)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010/trial-3af17_00009_9_alpha_d_ff=4,batch_size=16,d_core=128,d_model=64,dropout=0.0070,e_layers=4,learning_rate=0.0044,lradj=type1_2024-08-26_14-25-46/checkpoint_000001)[32m [repeated 4x across cluster][0m
2024-08-26 14:25:58,078	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3083064)[0m max_epoch 8
[36m(_train_fn pid=3081688)[0m 	iters: 500, epoch: 3 | loss: 0.6044260[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3081688)[0m 	speed: 0.0050s/iter; left time: 29.4790s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3079867)[0m Updating learning rate to 0.0006981942875582825[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3079867)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3079867)[0m Epoch: 3 cost time: 11.127353429794312[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3079867)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.5127980 Vali Loss: 0.2597115 Best vali loss: 0.2597115[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3083064)[0m Validation loss decreased (inf --> 0.2468).  Saving model state dict ...
[36m(_train_fn pid=3081558)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3083064)[0m Validation loss decreased (0.2468 --> 0.2322).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-3af17_00005 completed after 4 iterations at 2024-08-26 14:25:46. Total running time: 37s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             5.13968 â”‚
â”‚ time_total_s                                25.37337 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.25769 â”‚
â”‚ train_loss                                    0.5181 â”‚
â”‚ valid_loss                                   0.27112 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3081558)[0m Early stopping
[36m(_train_fn pid=3081688)[0m 	iters: 200, epoch: 4 | loss: 0.2164316[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=3081688)[0m 	speed: 0.0033s/iter; left time: 17.0165s[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=3083064)[0m Updating learning rate to 7.094090566951568e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3083064)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3083064)[0m Epoch: 4 cost time: 1.296525001525879[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=3083064)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4240663 Vali Loss: 0.2222648 Best vali loss: 0.2222648[32m [repeated 6x across cluster][0m

Trial trial-3af17_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00697 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00443 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3083665)[0m configuration
[36m(_train_fn pid=3083665)[0m {'batch_size': 16, 'learning_rate': 0.004431344121181138, 'd_model': 64, 'd_core': 128, 'e_layers': 4, 'dropout': 0.00697044652051838, 'lradj': 'type1', 'd_ff': 256}
[36m(_train_fn pid=3083665)[0m Use GPU: cuda:0
[36m(_train_fn pid=3083665)[0m train 8449
[36m(_train_fn pid=3083665)[0m val 2785
[36m(_train_fn pid=3083665)[0m start_epoch 0
[36m(_train_fn pid=3083665)[0m max_epoch 8
[36m(_train_fn pid=3081688)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3083064)[0m Validation loss decreased (0.2223 --> 0.2210).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3083665)[0m 	iters: 500, epoch: 1 | loss: 0.4112012[32m [repeated 27x across cluster][0m
[36m(_train_fn pid=3083665)[0m 	speed: 0.0079s/iter; left time: 29.6629s[32m [repeated 27x across cluster][0m
[36m(_train_fn pid=3083064)[0m Updating learning rate to 5.400054903636243e-06[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3083064)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3083064)[0m Epoch: 7 cost time: 1.2518069744110107[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3083064)[0m Epoch: 7, Steps: 265 | Train Loss: 0.4140552 Vali Loss: 0.2209194 Best vali loss: 0.2206102[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3083665)[0m Validation loss decreased (inf --> 0.2749).  Saving model state dict ...

Trial trial-3af17_00008 completed after 8 iterations at 2024-08-26 14:25:53. Total running time: 45s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.47457 â”‚
â”‚ time_total_s                                12.61545 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22061 â”‚
â”‚ train_loss                                   0.42044 â”‚
â”‚ valid_loss                                   0.22083 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3083064)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3079867)[0m Validation loss decreased (0.2597 --> 0.2565).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3081688)[0m Early stopping

Trial trial-3af17_00006 completed after 5 iterations at 2024-08-26 14:25:56. Total running time: 48s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.43829 â”‚
â”‚ time_total_s                                35.19264 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.21645 â”‚
â”‚ train_loss                                   0.35998 â”‚
â”‚ valid_loss                                   0.21995 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3083665)[0m 	iters: 500, epoch: 2 | loss: 0.2421849[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=3083665)[0m 	speed: 0.0066s/iter; left time: 21.1453s[32m [repeated 23x across cluster][0m
2024-08-26 14:26:01,442	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:26:04,704	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3079867)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010/trial-3af17_00003_3_alpha_d_ff=3,batch_size=8,d_core=32,d_model=64,dropout=0.0052,e_layers=3,learning_rate=0.0028,lradj=type1_2024-08-26_14-25-08/checkpoint_000005)[32m [repeated 3x across cluster][0m
2024-08-26 14:26:05,292	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:26:09,077	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:26:10,409	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=3079867)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010/trial-3af17_00003_3_alpha_d_ff=3,batch_size=8,d_core=32,d_model=64,dropout=0.0052,e_layers=3,learning_rate=0.0028,lradj=type1_2024-08-26_14-25-08/checkpoint_000006)[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3079867)[0m Updating learning rate to 0.00017454857188957062[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3079867)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3079867)[0m Epoch: 5 cost time: 6.547658443450928[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3079867)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.4935271 Vali Loss: 0.2709905 Best vali loss: 0.2565073[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=3079867)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3083665)[0m Validation loss decreased (0.2749 --> 0.2699).  Saving model state dict ...
[36m(_train_fn pid=3083665)[0m 	iters: 200, epoch: 4 | loss: 0.5049654[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3083665)[0m 	speed: 0.0065s/iter; left time: 15.8086s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=3083665)[0m Updating learning rate to 0.0011078360302952846
[36m(_train_fn pid=3083665)[0m saving checkpoint...
[36m(_train_fn pid=3083665)[0m Epoch: 3 cost time: 3.1385984420776367
[36m(_train_fn pid=3083665)[0m Epoch: 3, Steps: 529 | Train Loss: 0.5167633 Vali Loss: 0.2705020 Best vali loss: 0.2698549
[36m(_train_fn pid=3079867)[0m Updating learning rate to 8.727428594478531e-05
[36m(_train_fn pid=3079867)[0m saving checkpoint...
[36m(_train_fn pid=3079867)[0m Epoch: 6 cost time: 5.728781700134277
[36m(_train_fn pid=3079867)[0m Epoch: 6, Steps: 1057 | Train Loss: 0.5047006 Vali Loss: 0.2577608 Best vali loss: 0.2565073
[36m(_train_fn pid=3079867)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3079867)[0m 	iters: 700, epoch: 7 | loss: 1.0475069[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=3079867)[0m 	speed: 0.0053s/iter; left time: 7.4325s[32m [repeated 16x across cluster][0m

Trial status: 8 TERMINATED | 2 RUNNING
Current time: 2024-08-26 14:26:08. Total running time: 1min 0s
Logical resource usage: 2.0/32 CPUs, 0.5/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 3af17_00006 with best_valid_loss=0.21645101033806588 and params={'batch_size': 8, 'learning_rate': 0.0001678353491209946, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.002489494394656069, 'lradj': 'cosine', 'd_ff': 256}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-3af17_00003   RUNNING                 8       0.00279278           64              3         32            3   0.00522738   type1          6           54.565         0.504701       0.257761            0.256507 â”‚
â”‚ trial-3af17_00009   RUNNING                16       0.00443134           64              4        128            4   0.00697045   type1          4           17.3569        0.516411       0.270439            0.269855 â”‚
â”‚ trial-3af17_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5            9.96311       0.347486       0.221219            0.218544 â”‚
â”‚ trial-3af17_00001   TERMINATED             64       0.000589633          32              1        512            1   0.00796855   type1          7            6.10227       0.396227       0.219139            0.218832 â”‚
â”‚ trial-3af17_00002   TERMINATED             32       0.000329468         256              2        128            3   0.00461211   cosine         4            9.36777       0.313967       0.226394            0.220534 â”‚
â”‚ trial-3af17_00004   TERMINATED             32       0.00131382          512              3         32            2   0.0012657    cosine         4            8.81435       0.437139       0.239857            0.239571 â”‚
â”‚ trial-3af17_00005   TERMINATED             16       0.0030234           512              2         32            4   0.00192302   cosine         4           25.3734        0.5181         0.271119            0.257691 â”‚
â”‚ trial-3af17_00006   TERMINATED              8       0.000167835         128              2         32            1   0.00248949   cosine         5           35.1926        0.359984       0.219949            0.216451 â”‚
â”‚ trial-3af17_00007   TERMINATED             64       0.00134547          256              2         64            4   0.00553342   cosine         6           11.2466        0.311868       0.239226            0.230631 â”‚
â”‚ trial-3af17_00008   TERMINATED             32       0.000141882          32              1         32            4   0.00381402   cosine         8           12.6155        0.420442       0.220826            0.22061  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-3af17_00009 completed after 5 iterations at 2024-08-26 14:26:09. Total running time: 1min 0s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             3.78206 â”‚
â”‚ time_total_s                                21.13894 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.26985 â”‚
â”‚ train_loss                                   0.51659 â”‚
â”‚ valid_loss                                   0.27093 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=3083665)[0m Early stopping
[36m(_train_fn pid=3079867)[0m Validation loss decreased (0.2565 --> 0.2552).  Saving model state dict ...
[36m(_train_fn pid=3079867)[0m Updating learning rate to 4.3637142972392655e-05[32m [repeated 3x across cluster][0m
2024-08-26 14:26:15,004	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 14:26:15,016	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010' in 0.0022s.
[36m(_train_fn pid=3079867)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3079867)[0m Epoch: 7 cost time: 5.165693759918213[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3079867)[0m Epoch: 7, Steps: 1057 | Train Loss: 0.4791079 Vali Loss: 0.2551614 Best vali loss: 0.2551614[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=3083665)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3079867)[0m 	iters: 800, epoch: 8 | loss: 0.3139154[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=3079867)[0m 	speed: 0.0037s/iter; left time: 0.9424s[32m [repeated 12x across cluster][0m

Trial trial-3af17_00003 completed after 8 iterations at 2024-08-26 14:26:15. Total running time: 1min 6s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-3af17_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             4.59276 â”‚
â”‚ time_total_s                                64.86055 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.25516 â”‚
â”‚ train_loss                                   0.47762 â”‚
â”‚ valid_loss                                    0.2553 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 14:26:15. Total running time: 1min 6s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 3af17_00006 with best_valid_loss=0.21645101033806588 and params={'batch_size': 8, 'learning_rate': 0.0001678353491209946, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.002489494394656069, 'lradj': 'cosine', 'd_ff': 256}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-3af17_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         5            9.96311       0.347486       0.221219            0.218544 â”‚
â”‚ trial-3af17_00001   TERMINATED             64       0.000589633          32              1        512            1   0.00796855   type1          7            6.10227       0.396227       0.219139            0.218832 â”‚
â”‚ trial-3af17_00002   TERMINATED             32       0.000329468         256              2        128            3   0.00461211   cosine         4            9.36777       0.313967       0.226394            0.220534 â”‚
â”‚ trial-3af17_00003   TERMINATED              8       0.00279278           64              3         32            3   0.00522738   type1          8           64.8605        0.477617       0.255303            0.255161 â”‚
â”‚ trial-3af17_00004   TERMINATED             32       0.00131382          512              3         32            2   0.0012657    cosine         4            8.81435       0.437139       0.239857            0.239571 â”‚
â”‚ trial-3af17_00005   TERMINATED             16       0.0030234           512              2         32            4   0.00192302   cosine         4           25.3734        0.5181         0.271119            0.257691 â”‚
â”‚ trial-3af17_00006   TERMINATED              8       0.000167835         128              2         32            1   0.00248949   cosine         5           35.1926        0.359984       0.219949            0.216451 â”‚
â”‚ trial-3af17_00007   TERMINATED             64       0.00134547          256              2         64            4   0.00553342   cosine         6           11.2466        0.311868       0.239226            0.230631 â”‚
â”‚ trial-3af17_00008   TERMINATED             32       0.000141882          32              1         32            4   0.00381402   cosine         8           12.6155        0.420442       0.220826            0.22061  â”‚
â”‚ trial-3af17_00009   TERMINATED             16       0.00443134           64              4        128            4   0.00697045   type1          5           21.1389        0.516585       0.270929            0.269855 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 0.0001678353491209946, 'd_model': 128, 'd_core': 32, 'e_layers': 1, 'dropout': 0.002489494394656069, 'lradj': 'cosine', 'd_ff': 256}
[36m(_train_fn pid=3079867)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_seed10010/trial-3af17_00003_3_alpha_d_ff=3,batch_size=8,d_core=32,d_model=64,dropout=0.0052,e_layers=3,learning_rate=0.0028,lradj=type1_2024-08-26_14-25-08/checkpoint_000007)
[36m(_train_fn pid=3079867)[0m Updating learning rate to 2.1818571486196328e-05
[36m(_train_fn pid=3079867)[0m saving checkpoint...
[36m(_train_fn pid=3079867)[0m Epoch: 8 cost time: 4.0400660037994385
[36m(_train_fn pid=3079867)[0m Epoch: 8, Steps: 1057 | Train Loss: 0.4776174 Vali Loss: 0.2553026 Best vali loss: 0.2551614
[36m(_train_fn pid=3079867)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=3079867)[0m 	iters: 1000, epoch: 8 | loss: 0.5811961[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=3079867)[0m 	speed: 0.0037s/iter; left time: 0.2140s[32m [repeated 2x across cluster][0m


Time taken (4 parallel trials): 71 seconds


