N=10
for ((maxconcurrent=1; maxconcurrent<=N; maxconcurrent++))
do
horizon=96
# maxconcurrent=2
gpu_fraction=$(echo "scale=2; 1/$maxconcurrent" | bc)  # Calculate GPU fraction with 2 decimal places
start_time=$(date +%s)  # Get the current time in seconds
python3 tune_softs.py \
    --model SOFTS \
    --data ETTh2 \
    --root_path ./dataset/ETT-small/ \
    --data_path ETTh2.csv \
    --features M \
    --seq_len 96 \
    --label_len 48 \
    --pred_len $horizon \
    --enc_in 7 \
    --dec_in 7 \
    --c_out 7 \
    --embed timeF \
    --activation gelu \
    --train_epochs 8 \
    --patience 3 \
    --loss MSE \
    --num_workers 1 \
    --gpu 0 \
    --tune_search_algorithm random_search \
    --tune_trial_scheduler fifo \
    --tune_storage_path ./checkpoints/hptunning/random_search/ \
    --tune_experiment_name ETTh2_96_${horizon}_test_${maxconcurrent}_parallel_trials \
    --tune_objective best_valid_loss \
    --tune_num_samples 10 \
    --tune_max_trial_time_s 70 \
    --tune_time_budget_s 14400 \
    --tune_max_concurrent $maxconcurrent \
    --tune_gpu_resources $gpu_fraction \
    --tune_cpu_resources 1 \
    --tune_default_config "{
        \"batch_size\": 32, \
        \"learning_rate\": 0.0003, \
        \"d_model\": 128, \
        \"alpha_d_ff\": 1, \
        \"d_core\": 64, \
        \"e_layers\": 2, \
        \"dropout\": 0.0, \
        \"lradj\": \"cosine\"
    }" \
    --tune_param_space "{
        \"batch_size\": [\"choice\", [8, 16, 32, 64, 128]], \
        \"learning_rate\": [\"loguniform\", [0.00005, 0.005]], \
        \"d_model\": [\"choice\", [32, 64, 128, 256, 512]], \
        \"alpha_d_ff\": [\"choice\", [1, 2, 3, 4]], \
        \"d_core\": [\"choice\", [32, 64, 128, 256, 512]], \
        \"e_layers\": [\"choice\", [1, 2, 3, 4]], \
        \"dropout\": [\"loguniform\", [0.0008, 0.012]], \
        \"lradj\": [\"choice\", [\"cosine\", \"type1\"]]
    }" \
    --seed 123;
end_time=$(date +%s)  # Get the current time in seconds
elapsed_time=$((end_time - start_time))  # Calculate the elapsed time
echo ""
echo ""
echo "Time taken ($maxconcurrent parallel trials): $elapsed_time seconds"
echo ""
echo ""
done2024-08-26 13:30:34,062	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:30:34,441	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:30:34,447	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:30:34,454	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:30:37,601	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2788308)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00000_0_2024-08-26_13-30-34/checkpoint_000000)
2024-08-26 13:30:38,625	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2788308)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00000_0_2024-08-26_13-30-34/checkpoint_000001)
2024-08-26 13:30:39,594	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2788308)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00000_0_2024-08-26_13-30-34/checkpoint_000002)
2024-08-26 13:30:40,572	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_1_parallel_trials   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator                â”‚
â”‚ Scheduler                        FIFOScheduler                        â”‚
â”‚ Number of trials                 10                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-30-33_629157_2786096/artifacts/2024-08-26_13-30-34/ETTh2_96_96_test_1_parallel_trials/driver_artifacts`

Trial status: 1 PENDING
Current time: 2024-08-26 13:30:34. Total running time: 0s
Logical resource usage: 1.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers     dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-9b77c_00000   PENDING              32            0.0003         128              1         64            2           0   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-9b77c_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2788308)[0m configuration
[36m(_train_fn pid=2788308)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2788308)[0m Use GPU: cuda:0
[36m(_train_fn pid=2788308)[0m train 8449
[36m(_train_fn pid=2788308)[0m val 2785
[36m(_train_fn pid=2788308)[0m start_epoch 0
[36m(_train_fn pid=2788308)[0m max_epoch 8
[36m(_train_fn pid=2788308)[0m 	iters: 100, epoch: 1 | loss: 0.5344234
[36m(_train_fn pid=2788308)[0m 	speed: 0.0083s/iter; left time: 16.7806s
[36m(_train_fn pid=2788308)[0m 	iters: 200, epoch: 1 | loss: 0.5150301
[36m(_train_fn pid=2788308)[0m 	speed: 0.0030s/iter; left time: 5.7361s
[36m(_train_fn pid=2788308)[0m Updating learning rate to 0.000288581929876693
[36m(_train_fn pid=2788308)[0m saving checkpoint...
[36m(_train_fn pid=2788308)[0m Validation loss decreased (inf --> 0.2195).  Saving model state dict ...
[36m(_train_fn pid=2788308)[0m Epoch: 1 cost time: 1.0670125484466553
[36m(_train_fn pid=2788308)[0m Epoch: 1, Steps: 265 | Train Loss: 0.5247267 Vali Loss: 0.2195345 Best vali loss: 0.2195345
[36m(_train_fn pid=2788308)[0m 	iters: 100, epoch: 2 | loss: 0.5224623
[36m(_train_fn pid=2788308)[0m 	speed: 0.0073s/iter; left time: 12.8565s
[36m(_train_fn pid=2788308)[0m 	iters: 200, epoch: 2 | loss: 0.3515588
[36m(_train_fn pid=2788308)[0m 	speed: 0.0030s/iter; left time: 5.0384s
[36m(_train_fn pid=2788308)[0m Updating learning rate to 0.00025606601717798207
[36m(_train_fn pid=2788308)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2788308)[0m saving checkpoint...
[36m(_train_fn pid=2788308)[0m Epoch: 2 cost time: 0.8530561923980713
[36m(_train_fn pid=2788308)[0m Epoch: 2, Steps: 265 | Train Loss: 0.4370105 Vali Loss: 0.2229692 Best vali loss: 0.2195345
[36m(_train_fn pid=2788308)[0m 	iters: 100, epoch: 3 | loss: 0.3006290
[36m(_train_fn pid=2788308)[0m 	speed: 0.0069s/iter; left time: 10.3031s
[36m(_train_fn pid=2788308)[0m 	iters: 200, epoch: 3 | loss: 0.3384072
[36m(_train_fn pid=2788308)[0m 	speed: 0.0028s/iter; left time: 3.9635s
[36m(_train_fn pid=2788308)[0m Updating learning rate to 0.00020740251485476345
[36m(_train_fn pid=2788308)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2788308)[0m saving checkpoint...
[36m(_train_fn pid=2788308)[0m Epoch: 3 cost time: 0.7968699932098389
[36m(_train_fn pid=2788308)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3195181 Vali Loss: 0.2231508 Best vali loss: 0.2195345
[36m(_train_fn pid=2788308)[0m 	iters: 100, epoch: 4 | loss: 0.5657514
[36m(_train_fn pid=2788308)[0m 	speed: 0.0069s/iter; left time: 8.4575s
[36m(_train_fn pid=2788308)[0m 	iters: 200, epoch: 4 | loss: 0.3015919
[36m(_train_fn pid=2788308)[0m 	speed: 0.0029s/iter; left time: 3.2863s

Trial trial-9b77c_00000 completed after 4 iterations at 2024-08-26 13:30:40. Total running time: 6s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                              0.9754 â”‚
â”‚ time_total_s                                 4.60712 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21953 â”‚
â”‚ train_loss                                   0.43367 â”‚
â”‚ valid_loss                                   0.23297 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2788308)[0m Updating learning rate to 0.00015
[36m(_train_fn pid=2788308)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2788308)[0m saving checkpoint...
[36m(_train_fn pid=2788308)[0m Epoch: 4 cost time: 0.8141510486602783
[36m(_train_fn pid=2788308)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00000_0_2024-08-26_13-30-34/checkpoint_000003)
[36m(_train_fn pid=2788778)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-30-40/checkpoint_000000)
2024-08-26 13:30:49,763	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2788778)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-30-40/checkpoint_000001)
2024-08-26 13:30:53,506	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2788778)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-30-40/checkpoint_000002)
[36m(_train_fn pid=2788308)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4336717 Vali Loss: 0.2329654 Best vali loss: 0.2195345
[36m(_train_fn pid=2788308)[0m Early stopping

Trial trial-9b77c_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00129 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2788778)[0m configuration
[36m(_train_fn pid=2788778)[0m {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2788778)[0m Use GPU: cuda:0
[36m(_train_fn pid=2788778)[0m train 8449
[36m(_train_fn pid=2788778)[0m val 2785
[36m(_train_fn pid=2788778)[0m start_epoch 0
[36m(_train_fn pid=2788778)[0m max_epoch 8
[36m(_train_fn pid=2788778)[0m 	iters: 100, epoch: 1 | loss: 0.1373757
[36m(_train_fn pid=2788778)[0m 	speed: 0.0079s/iter; left time: 66.3133s
[36m(_train_fn pid=2788778)[0m 	iters: 200, epoch: 1 | loss: 0.4189163
[36m(_train_fn pid=2788778)[0m 	speed: 0.0028s/iter; left time: 22.8228s
[36m(_train_fn pid=2788778)[0m 	iters: 300, epoch: 1 | loss: 0.2796506
[36m(_train_fn pid=2788778)[0m 	speed: 0.0029s/iter; left time: 23.3824s
[36m(_train_fn pid=2788778)[0m 	iters: 400, epoch: 1 | loss: 0.1596564
[36m(_train_fn pid=2788778)[0m 	speed: 0.0029s/iter; left time: 23.4443s
[36m(_train_fn pid=2788778)[0m 	iters: 500, epoch: 1 | loss: 0.3762820
[36m(_train_fn pid=2788778)[0m 	speed: 0.0028s/iter; left time: 22.5767s
[36m(_train_fn pid=2788778)[0m 	iters: 600, epoch: 1 | loss: 0.2901162
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 23.5182s
[36m(_train_fn pid=2788778)[0m 	iters: 700, epoch: 1 | loss: 0.3194359
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 23.5628s
[36m(_train_fn pid=2788778)[0m 	iters: 800, epoch: 1 | loss: 0.3678788
[36m(_train_fn pid=2788778)[0m 	speed: 0.0031s/iter; left time: 23.3789s
[36m(_train_fn pid=2788778)[0m 	iters: 900, epoch: 1 | loss: 0.2575698
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 22.6593s
[36m(_train_fn pid=2788778)[0m 	iters: 1000, epoch: 1 | loss: 0.5904244
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 22.0420s
[36m(_train_fn pid=2788778)[0m Updating learning rate to 6.162542403792038e-05
[36m(_train_fn pid=2788778)[0m saving checkpoint...
[36m(_train_fn pid=2788778)[0m Validation loss decreased (inf --> 0.2160).  Saving model state dict ...
[36m(_train_fn pid=2788778)[0m Epoch: 1 cost time: 3.340484380722046
[36m(_train_fn pid=2788778)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.3197306 Vali Loss: 0.2159520 Best vali loss: 0.2159520
[36m(_train_fn pid=2788778)[0m 	iters: 100, epoch: 2 | loss: 0.1362932
[36m(_train_fn pid=2788778)[0m 	speed: 0.0097s/iter; left time: 71.0446s
[36m(_train_fn pid=2788778)[0m 	iters: 200, epoch: 2 | loss: 0.9777021
[36m(_train_fn pid=2788778)[0m 	speed: 0.0029s/iter; left time: 21.0025s
[36m(_train_fn pid=2788778)[0m 	iters: 300, epoch: 2 | loss: 0.4114753
[36m(_train_fn pid=2788778)[0m 	speed: 0.0029s/iter; left time: 20.2511s
[36m(_train_fn pid=2788778)[0m 	iters: 400, epoch: 2 | loss: 0.2054970
[36m(_train_fn pid=2788778)[0m 	speed: 0.0028s/iter; left time: 19.8155s
[36m(_train_fn pid=2788778)[0m 	iters: 500, epoch: 2 | loss: 0.2110277
[36m(_train_fn pid=2788778)[0m 	speed: 0.0028s/iter; left time: 19.4628s
[36m(_train_fn pid=2788778)[0m 	iters: 600, epoch: 2 | loss: 0.1726706
[36m(_train_fn pid=2788778)[0m 	speed: 0.0029s/iter; left time: 19.4189s
[36m(_train_fn pid=2788778)[0m 	iters: 700, epoch: 2 | loss: 0.1599204
[36m(_train_fn pid=2788778)[0m 	speed: 0.0029s/iter; left time: 19.6323s
[36m(_train_fn pid=2788778)[0m 	iters: 800, epoch: 2 | loss: 0.1914527
[36m(_train_fn pid=2788778)[0m 	speed: 0.0029s/iter; left time: 18.8998s
[36m(_train_fn pid=2788778)[0m 	iters: 900, epoch: 2 | loss: 0.3837073
[36m(_train_fn pid=2788778)[0m 	speed: 0.0028s/iter; left time: 18.4543s
[36m(_train_fn pid=2788778)[0m 	iters: 1000, epoch: 2 | loss: 0.2932326
[36m(_train_fn pid=2788778)[0m 	speed: 0.0028s/iter; left time: 18.0100s
[36m(_train_fn pid=2788778)[0m Updating learning rate to 5.4681791396423194e-05
[36m(_train_fn pid=2788778)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2788778)[0m saving checkpoint...
[36m(_train_fn pid=2788778)[0m Epoch: 2 cost time: 3.0532965660095215
[36m(_train_fn pid=2788778)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.3142979 Vali Loss: 0.2210757 Best vali loss: 0.2159520
[36m(_train_fn pid=2788778)[0m 	iters: 100, epoch: 3 | loss: 0.2875986
[36m(_train_fn pid=2788778)[0m 	speed: 0.0097s/iter; left time: 60.4230s
[36m(_train_fn pid=2788778)[0m 	iters: 200, epoch: 3 | loss: 0.6528748
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 18.3592s
[36m(_train_fn pid=2788778)[0m 	iters: 300, epoch: 3 | loss: 0.1613897
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 18.0812s
[36m(_train_fn pid=2788778)[0m 	iters: 400, epoch: 3 | loss: 0.1030525
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 17.8292s
[36m(_train_fn pid=2788778)[0m 	iters: 500, epoch: 3 | loss: 0.1672853
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 17.5856s
[36m(_train_fn pid=2788778)[0m 	iters: 600, epoch: 3 | loss: 1.0164441
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 17.3467s
[36m(_train_fn pid=2788778)[0m 	iters: 700, epoch: 3 | loss: 0.1807255
[36m(_train_fn pid=2788778)[0m 	speed: 0.0031s/iter; left time: 17.2563s
[36m(_train_fn pid=2788778)[0m 	iters: 800, epoch: 3 | loss: 1.2156129
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 16.8918s
[36m(_train_fn pid=2788778)[0m 	iters: 900, epoch: 3 | loss: 0.1498966
[36m(_train_fn pid=2788778)[0m 	speed: 0.0031s/iter; left time: 16.6255s
[36m(_train_fn pid=2788778)[0m 	iters: 1000, epoch: 3 | loss: 0.3225371
[36m(_train_fn pid=2788778)[0m 	speed: 0.0031s/iter; left time: 16.8114s
[36m(_train_fn pid=2788778)[0m Updating learning rate to 4.428991077132631e-05
[36m(_train_fn pid=2788778)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2788778)[0m saving checkpoint...
[36m(_train_fn pid=2788778)[0m Epoch: 3 cost time: 3.2482404708862305
[36m(_train_fn pid=2788778)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4257417 Vali Loss: 0.2197143 Best vali loss: 0.2159520
[36m(_train_fn pid=2788778)[0m 	iters: 100, epoch: 4 | loss: 0.3174965
[36m(_train_fn pid=2788778)[0m 	speed: 0.0101s/iter; left time: 52.4515s
[36m(_train_fn pid=2788778)[0m 	iters: 200, epoch: 4 | loss: 0.1810564
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 15.2048s
[36m(_train_fn pid=2788778)[0m 	iters: 300, epoch: 4 | loss: 0.2372465
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 14.8949s
[36m(_train_fn pid=2788778)[0m 	iters: 400, epoch: 4 | loss: 0.1639731
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 14.6749s
[36m(_train_fn pid=2788778)[0m 	iters: 500, epoch: 4 | loss: 0.2771734
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 14.4064s
[36m(_train_fn pid=2788778)[0m 	iters: 600, epoch: 4 | loss: 0.3275895
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 14.0585s
2024-08-26 13:30:57,185	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2788778)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-30-40/checkpoint_000003)
[36m(_train_fn pid=2789206)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-30-57/checkpoint_000000)
[36m(_train_fn pid=2788778)[0m 	iters: 700, epoch: 4 | loss: 0.9402995
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 13.6598s
[36m(_train_fn pid=2788778)[0m 	iters: 800, epoch: 4 | loss: 0.1081746
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 13.4098s
[36m(_train_fn pid=2788778)[0m 	iters: 900, epoch: 4 | loss: 0.3779389
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 13.2133s
[36m(_train_fn pid=2788778)[0m 	iters: 1000, epoch: 4 | loss: 0.4967451
[36m(_train_fn pid=2788778)[0m 	speed: 0.0030s/iter; left time: 12.7260s

Trial trial-9b77c_00001 completed after 4 iterations at 2024-08-26 13:30:57. Total running time: 22s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             3.67584 â”‚
â”‚ time_total_s                                15.19115 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21595 â”‚
â”‚ train_loss                                   0.34277 â”‚
â”‚ valid_loss                                   0.22282 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2788778)[0m Updating learning rate to 3.203185178516828e-05
[36m(_train_fn pid=2788778)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2788778)[0m saving checkpoint...
[36m(_train_fn pid=2788778)[0m Epoch: 4 cost time: 3.198946714401245
[36m(_train_fn pid=2788778)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3427693 Vali Loss: 0.2228175 Best vali loss: 0.2159520
[36m(_train_fn pid=2788778)[0m Early stopping

Trial trial-9b77c_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00891 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00351 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2789206)[0m configuration
[36m(_train_fn pid=2789206)[0m {'batch_size': 16, 'learning_rate': 0.0035128531928926765, 'd_model': 512, 'd_core': 64, 'e_layers': 4, 'dropout': 0.00890604745419256, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2789206)[0m Use GPU: cuda:0
[36m(_train_fn pid=2789206)[0m train 8449
[36m(_train_fn pid=2789206)[0m val 2785
[36m(_train_fn pid=2789206)[0m start_epoch 0
[36m(_train_fn pid=2789206)[0m max_epoch 8
[36m(_train_fn pid=2789206)[0m 	iters: 100, epoch: 1 | loss: 0.9502878
[36m(_train_fn pid=2789206)[0m 	speed: 0.0102s/iter; left time: 42.1355s
[36m(_train_fn pid=2789206)[0m 	iters: 200, epoch: 1 | loss: 0.4507644
[36m(_train_fn pid=2789206)[0m 	speed: 0.0050s/iter; left time: 20.1104s
[36m(_train_fn pid=2789206)[0m 	iters: 300, epoch: 1 | loss: 1.3000757
[36m(_train_fn pid=2789206)[0m 	speed: 0.0049s/iter; left time: 19.1614s
[36m(_train_fn pid=2789206)[0m 	iters: 400, epoch: 1 | loss: 0.8396374
[36m(_train_fn pid=2789206)[0m 	speed: 0.0050s/iter; left time: 19.0866s
[36m(_train_fn pid=2789206)[0m 	iters: 500, epoch: 1 | loss: 0.2699364
[36m(_train_fn pid=2789206)[0m 	speed: 0.0049s/iter; left time: 18.4718s
[36m(_train_fn pid=2789206)[0m Updating learning rate to 0.0035128531928926765
[36m(_train_fn pid=2789206)[0m saving checkpoint...
[36m(_train_fn pid=2789206)[0m Validation loss decreased (inf --> 0.2859).  Saving model state dict ...
[36m(_train_fn pid=2789206)[0m Epoch: 1 cost time: 2.904961109161377
[36m(_train_fn pid=2789206)[0m Epoch: 1, Steps: 529 | Train Loss: 0.7621403 Vali Loss: 0.2858527 Best vali loss: 0.2858527
[36m(_train_fn pid=2789206)[0m 	iters: 100, epoch: 2 | loss: 0.4133271
[36m(_train_fn pid=2789206)[0m 	speed: 0.0107s/iter; left time: 38.4301s
[36m(_train_fn pid=2789206)[0m 	iters: 200, epoch: 2 | loss: 0.5005943
[36m(_train_fn pid=2789206)[0m 	speed: 0.0047s/iter; left time: 16.5518s
[36m(_train_fn pid=2789206)[0m 	iters: 300, epoch: 2 | loss: 0.4131463
[36m(_train_fn pid=2789206)[0m 	speed: 0.0048s/iter; left time: 16.3283s
[36m(_train_fn pid=2789206)[0m 	iters: 400, epoch: 2 | loss: 0.5075887
[36m(_train_fn pid=2789206)[0m 	speed: 0.0045s/iter; left time: 14.9649s

Trial status: 2 TERMINATED | 1 RUNNING
Current time: 2024-08-26 13:31:04. Total running time: 30s
Logical resource usage: 1.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 9b77c_00001 with best_valid_loss=0.21595204482707772 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
2024-08-26 13:31:05,245	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2789206)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-30-57/checkpoint_000001)
2024-08-26 13:31:08,299	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2789206)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-30-57/checkpoint_000002)
2024-08-26 13:31:11,223	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2789206)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-30-57/checkpoint_000003)
2024-08-26 13:31:14,051	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2789206)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-30-57/checkpoint_000004)
2024-08-26 13:31:17,104	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2789206)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-30-57/checkpoint_000005)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-9b77c_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          1            3.70954       0.76214        0.285853            0.285853 â”‚
â”‚ trial-9b77c_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            4.60712       0.433672       0.232965            0.219535 â”‚
â”‚ trial-9b77c_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           15.1912        0.342769       0.222818            0.215952 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2789206)[0m 	iters: 500, epoch: 2 | loss: 0.3070810
[36m(_train_fn pid=2789206)[0m 	speed: 0.0046s/iter; left time: 14.6288s
[36m(_train_fn pid=2789206)[0m Updating learning rate to 0.0017564265964463382
[36m(_train_fn pid=2789206)[0m saving checkpoint...
[36m(_train_fn pid=2789206)[0m Validation loss decreased (0.2859 --> 0.2783).  Saving model state dict ...
[36m(_train_fn pid=2789206)[0m Epoch: 2 cost time: 2.514723539352417
[36m(_train_fn pid=2789206)[0m Epoch: 2, Steps: 529 | Train Loss: 0.4283475 Vali Loss: 0.2783087 Best vali loss: 0.2783087
[36m(_train_fn pid=2789206)[0m 	iters: 100, epoch: 3 | loss: 0.5801491
[36m(_train_fn pid=2789206)[0m 	speed: 0.0107s/iter; left time: 33.0420s
[36m(_train_fn pid=2789206)[0m 	iters: 200, epoch: 3 | loss: 0.6031475
[36m(_train_fn pid=2789206)[0m 	speed: 0.0051s/iter; left time: 15.1639s
[36m(_train_fn pid=2789206)[0m 	iters: 300, epoch: 3 | loss: 1.3186543
[36m(_train_fn pid=2789206)[0m 	speed: 0.0050s/iter; left time: 14.4820s
[36m(_train_fn pid=2789206)[0m 	iters: 400, epoch: 3 | loss: 0.4205400
[36m(_train_fn pid=2789206)[0m 	speed: 0.0049s/iter; left time: 13.6433s
[36m(_train_fn pid=2789206)[0m 	iters: 500, epoch: 3 | loss: 0.3148034
[36m(_train_fn pid=2789206)[0m 	speed: 0.0048s/iter; left time: 12.7809s
[36m(_train_fn pid=2789206)[0m Updating learning rate to 0.0008782132982231691
[36m(_train_fn pid=2789206)[0m saving checkpoint...
[36m(_train_fn pid=2789206)[0m Validation loss decreased (0.2783 --> 0.2703).  Saving model state dict ...
[36m(_train_fn pid=2789206)[0m Epoch: 3 cost time: 2.654911994934082
[36m(_train_fn pid=2789206)[0m Epoch: 3, Steps: 529 | Train Loss: 0.6474589 Vali Loss: 0.2703472 Best vali loss: 0.2703472
[36m(_train_fn pid=2789206)[0m 	iters: 100, epoch: 4 | loss: 0.4897015
[36m(_train_fn pid=2789206)[0m 	speed: 0.0104s/iter; left time: 26.5789s
[36m(_train_fn pid=2789206)[0m 	iters: 200, epoch: 4 | loss: 0.3712242
[36m(_train_fn pid=2789206)[0m 	speed: 0.0048s/iter; left time: 11.6875s
[36m(_train_fn pid=2789206)[0m 	iters: 300, epoch: 4 | loss: 0.2992301
[36m(_train_fn pid=2789206)[0m 	speed: 0.0048s/iter; left time: 11.2476s
[36m(_train_fn pid=2789206)[0m 	iters: 400, epoch: 4 | loss: 0.5143488
[36m(_train_fn pid=2789206)[0m 	speed: 0.0046s/iter; left time: 10.2368s
[36m(_train_fn pid=2789206)[0m 	iters: 500, epoch: 4 | loss: 0.3747051
[36m(_train_fn pid=2789206)[0m 	speed: 0.0045s/iter; left time: 9.7630s
[36m(_train_fn pid=2789206)[0m Updating learning rate to 0.00043910664911158456
[36m(_train_fn pid=2789206)[0m saving checkpoint...
[36m(_train_fn pid=2789206)[0m Validation loss decreased (0.2703 --> 0.2689).  Saving model state dict ...
[36m(_train_fn pid=2789206)[0m Epoch: 4 cost time: 2.505413770675659
[36m(_train_fn pid=2789206)[0m Epoch: 4, Steps: 529 | Train Loss: 0.4098419 Vali Loss: 0.2689202 Best vali loss: 0.2689202
[36m(_train_fn pid=2789206)[0m 	iters: 100, epoch: 5 | loss: 0.6292289
[36m(_train_fn pid=2789206)[0m 	speed: 0.0105s/iter; left time: 21.1262s
[36m(_train_fn pid=2789206)[0m 	iters: 200, epoch: 5 | loss: 0.2823420
[36m(_train_fn pid=2789206)[0m 	speed: 0.0048s/iter; left time: 9.1257s
[36m(_train_fn pid=2789206)[0m 	iters: 300, epoch: 5 | loss: 0.5293031
[36m(_train_fn pid=2789206)[0m 	speed: 0.0048s/iter; left time: 8.7306s
[36m(_train_fn pid=2789206)[0m 	iters: 400, epoch: 5 | loss: 0.5447793
[36m(_train_fn pid=2789206)[0m 	speed: 0.0044s/iter; left time: 7.5675s
[36m(_train_fn pid=2789206)[0m 	iters: 500, epoch: 5 | loss: 0.5121201
[36m(_train_fn pid=2789206)[0m 	speed: 0.0044s/iter; left time: 7.1582s
[36m(_train_fn pid=2789206)[0m Updating learning rate to 0.00021955332455579228
[36m(_train_fn pid=2789206)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2789206)[0m saving checkpoint...
[36m(_train_fn pid=2789206)[0m Epoch: 5 cost time: 2.4659676551818848
[36m(_train_fn pid=2789206)[0m Epoch: 5, Steps: 529 | Train Loss: 0.4995547 Vali Loss: 0.2704001 Best vali loss: 0.2689202
[36m(_train_fn pid=2789206)[0m 	iters: 100, epoch: 6 | loss: 0.3024983
[36m(_train_fn pid=2789206)[0m 	speed: 0.0102s/iter; left time: 15.1892s
[36m(_train_fn pid=2789206)[0m 	iters: 200, epoch: 6 | loss: 0.3787350
[36m(_train_fn pid=2789206)[0m 	speed: 0.0052s/iter; left time: 7.1755s
[36m(_train_fn pid=2789206)[0m 	iters: 300, epoch: 6 | loss: 0.4137888
[36m(_train_fn pid=2789206)[0m 	speed: 0.0051s/iter; left time: 6.6237s
[36m(_train_fn pid=2789206)[0m 	iters: 400, epoch: 6 | loss: 0.3279505
[36m(_train_fn pid=2789206)[0m 	speed: 0.0050s/iter; left time: 5.9134s
[36m(_train_fn pid=2789206)[0m 	iters: 500, epoch: 6 | loss: 0.3727880
[36m(_train_fn pid=2789206)[0m 	speed: 0.0049s/iter; left time: 5.3662s
[36m(_train_fn pid=2789206)[0m Updating learning rate to 0.00010977666227789614
[36m(_train_fn pid=2789206)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2789206)[0m saving checkpoint...
[36m(_train_fn pid=2789206)[0m Epoch: 6 cost time: 2.702033281326294
[36m(_train_fn pid=2789206)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3591521 Vali Loss: 0.2706647 Best vali loss: 0.2689202
[36m(_train_fn pid=2789206)[0m 	iters: 100, epoch: 7 | loss: 0.4110825
2024-08-26 13:31:19,933	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2789206)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-30-57/checkpoint_000006)
[36m(_train_fn pid=2789870)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-31-19/checkpoint_000000)
[36m(_train_fn pid=2789870)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-31-19/checkpoint_000001)
[36m(_train_fn pid=2789206)[0m 	speed: 0.0099s/iter; left time: 9.5127s
[36m(_train_fn pid=2789206)[0m 	iters: 200, epoch: 7 | loss: 0.2671881
[36m(_train_fn pid=2789206)[0m 	speed: 0.0046s/iter; left time: 3.9148s
[36m(_train_fn pid=2789206)[0m 	iters: 300, epoch: 7 | loss: 0.2132674
[36m(_train_fn pid=2789206)[0m 	speed: 0.0046s/iter; left time: 3.4743s
[36m(_train_fn pid=2789206)[0m 	iters: 400, epoch: 7 | loss: 0.4284273
[36m(_train_fn pid=2789206)[0m 	speed: 0.0049s/iter; left time: 3.2109s
[36m(_train_fn pid=2789206)[0m 	iters: 500, epoch: 7 | loss: 0.4790372
[36m(_train_fn pid=2789206)[0m 	speed: 0.0045s/iter; left time: 2.5237s

Trial trial-9b77c_00002 completed after 7 iterations at 2024-08-26 13:31:19. Total running time: 45s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             2.82717 â”‚
â”‚ time_total_s                                21.28365 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26892 â”‚
â”‚ train_loss                                    0.3598 â”‚
â”‚ valid_loss                                    0.2707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2789206)[0m Updating learning rate to 5.488833113894807e-05
[36m(_train_fn pid=2789206)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2789206)[0m saving checkpoint...
[36m(_train_fn pid=2789206)[0m Epoch: 7 cost time: 2.4785499572753906
[36m(_train_fn pid=2789206)[0m Epoch: 7, Steps: 529 | Train Loss: 0.3598005 Vali Loss: 0.2706989 Best vali loss: 0.2689202
[36m(_train_fn pid=2789206)[0m Early stopping

Trial trial-9b77c_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00596 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00015 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2789870)[0m configuration
[36m(_train_fn pid=2789870)[0m {'batch_size': 8, 'learning_rate': 0.00015448958497378217, 'd_model': 128, 'd_core': 512, 'e_layers': 4, 'dropout': 0.005958307406826554, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2789870)[0m Use GPU: cuda:0
[36m(_train_fn pid=2789870)[0m train 8449
[36m(_train_fn pid=2789870)[0m val 2785
[36m(_train_fn pid=2789870)[0m start_epoch 0
[36m(_train_fn pid=2789870)[0m max_epoch 8
[36m(_train_fn pid=2789870)[0m 	iters: 200, epoch: 1 | loss: 0.6132952[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2789870)[0m 	speed: 0.0043s/iter; left time: 35.2218s[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2789870)[0m Updating learning rate to 0.00014860967525861138
[36m(_train_fn pid=2789870)[0m saving checkpoint...
[36m(_train_fn pid=2789870)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...
[36m(_train_fn pid=2789870)[0m Epoch: 1 cost time: 4.804574728012085
[36m(_train_fn pid=2789870)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.4098796 Vali Loss: 0.2205437 Best vali loss: 0.2205437
[36m(_train_fn pid=2789870)[0m 	iters: 200, epoch: 2 | loss: 0.0968299[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2789870)[0m 	speed: 0.0043s/iter; left time: 30.8304s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2789870)[0m Updating learning rate to 0.00013186510906571945
[36m(_train_fn pid=2789870)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2789870)[0m saving checkpoint...
[36m(_train_fn pid=2789870)[0m Epoch: 2 cost time: 4.492945432662964
[36m(_train_fn pid=2789870)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.2872207 Vali Loss: 0.2243823 Best vali loss: 0.2205437
[36m(_train_fn pid=2789870)[0m 	iters: 200, epoch: 3 | loss: 0.2540271[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2789870)[0m 	speed: 0.0047s/iter; left time: 29.0964s[32m [repeated 10x across cluster][0m

Trial status: 3 TERMINATED | 1 RUNNING
Current time: 2024-08-26 13:31:34. Total running time: 1min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 9b77c_00001 with best_valid_loss=0.21595204482707772 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2789870)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-31-19/checkpoint_000002)
[36m(_train_fn pid=2789870)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-31-19/checkpoint_000003)
[36m(_train_fn pid=2790318)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-31-43/checkpoint_000000)
[36m(_train_fn pid=2790318)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-31-43/checkpoint_000001)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-9b77c_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         2           10.9484        0.287221       0.224382            0.220544 â”‚
â”‚ trial-9b77c_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            4.60712       0.433672       0.232965            0.219535 â”‚
â”‚ trial-9b77c_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           15.1912        0.342769       0.222818            0.215952 â”‚
â”‚ trial-9b77c_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           21.2837        0.359801       0.270699            0.26892  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2789870)[0m Updating learning rate to 0.00010680509480810367
[36m(_train_fn pid=2789870)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2789870)[0m saving checkpoint...
[36m(_train_fn pid=2789870)[0m Epoch: 3 cost time: 5.033541679382324
[36m(_train_fn pid=2789870)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4134459 Vali Loss: 0.2232880 Best vali loss: 0.2205437
[36m(_train_fn pid=2789870)[0m 	iters: 100, epoch: 4 | loss: 0.2336146[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2789870)[0m 	speed: 0.0135s/iter; left time: 70.0840s[32m [repeated 9x across cluster][0m

Trial trial-9b77c_00003 completed after 4 iterations at 2024-08-26 13:31:43. Total running time: 1min 8s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             5.25092 â”‚
â”‚ time_total_s                                21.84307 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22054 â”‚
â”‚ train_loss                                   0.24563 â”‚
â”‚ valid_loss                                   0.22743 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2789870)[0m Updating learning rate to 7.724479248689109e-05
[36m(_train_fn pid=2789870)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2789870)[0m saving checkpoint...
[36m(_train_fn pid=2789870)[0m Epoch: 4 cost time: 4.642263889312744
[36m(_train_fn pid=2789870)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.2456316 Vali Loss: 0.2274289 Best vali loss: 0.2205437
[36m(_train_fn pid=2789870)[0m Early stopping

Trial trial-9b77c_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00325 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00358 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2790318)[0m configuration
[36m(_train_fn pid=2790318)[0m {'batch_size': 8, 'learning_rate': 0.0035791880881606486, 'd_model': 256, 'd_core': 64, 'e_layers': 4, 'dropout': 0.003254613020686493, 'lradj': 'type1', 'd_ff': 768}
[36m(_train_fn pid=2789870)[0m 	iters: 1000, epoch: 4 | loss: 0.2348267[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2789870)[0m 	speed: 0.0044s/iter; left time: 18.7012s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2790318)[0m Use GPU: cuda:0
[36m(_train_fn pid=2790318)[0m train 8449
[36m(_train_fn pid=2790318)[0m val 2785
[36m(_train_fn pid=2790318)[0m start_epoch 0
[36m(_train_fn pid=2790318)[0m max_epoch 8
[36m(_train_fn pid=2790318)[0m 	iters: 900, epoch: 1 | loss: 1.5788442[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2790318)[0m 	speed: 0.0050s/iter; left time: 37.4248s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2790318)[0m Updating learning rate to 0.0035791880881606486
[36m(_train_fn pid=2790318)[0m saving checkpoint...
[36m(_train_fn pid=2790318)[0m Validation loss decreased (inf --> 0.2711).  Saving model state dict ...
[36m(_train_fn pid=2790318)[0m Epoch: 1 cost time: 5.514461040496826
[36m(_train_fn pid=2790318)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.5489847 Vali Loss: 0.2711374 Best vali loss: 0.2711374
[36m(_train_fn pid=2790318)[0m 	iters: 800, epoch: 2 | loss: 0.3385489[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2790318)[0m 	speed: 0.0044s/iter; left time: 28.9423s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2790318)[0m Updating learning rate to 0.0017895940440803243
[36m(_train_fn pid=2790318)[0m saving checkpoint...
[36m(_train_fn pid=2790318)[0m Validation loss decreased (0.2711 --> 0.2668).  Saving model state dict ...
[36m(_train_fn pid=2790318)[0m Epoch: 2 cost time: 4.796646595001221
[36m(_train_fn pid=2790318)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-31-43/checkpoint_000002)
[36m(_train_fn pid=2790318)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-31-43/checkpoint_000003)
[36m(_train_fn pid=2790318)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-31-43/checkpoint_000004)
[36m(_train_fn pid=2790318)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.5709005 Vali Loss: 0.2667734 Best vali loss: 0.2667734
[36m(_train_fn pid=2790318)[0m 	iters: 700, epoch: 3 | loss: 0.3934609[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2790318)[0m 	speed: 0.0050s/iter; left time: 27.9632s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2790318)[0m Updating learning rate to 0.0008947970220401621
[36m(_train_fn pid=2790318)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2790318)[0m saving checkpoint...
[36m(_train_fn pid=2790318)[0m Epoch: 3 cost time: 5.226138114929199
[36m(_train_fn pid=2790318)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4845674 Vali Loss: 0.2714944 Best vali loss: 0.2667734

Trial status: 4 TERMINATED | 1 RUNNING
Current time: 2024-08-26 13:32:04. Total running time: 1min 30s
Logical resource usage: 1.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 9b77c_00001 with best_valid_loss=0.21595204482707772 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-9b77c_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          3           17.8569        0.484567       0.271494            0.266773 â”‚
â”‚ trial-9b77c_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            4.60712       0.433672       0.232965            0.219535 â”‚
â”‚ trial-9b77c_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           15.1912        0.342769       0.222818            0.215952 â”‚
â”‚ trial-9b77c_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           21.2837        0.359801       0.270699            0.26892  â”‚
â”‚ trial-9b77c_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           21.8431        0.245632       0.227429            0.220544 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2790318)[0m 	iters: 600, epoch: 4 | loss: 1.5296119[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2790318)[0m 	speed: 0.0043s/iter; left time: 20.3795s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2790318)[0m Updating learning rate to 0.00044739851102008107
[36m(_train_fn pid=2790318)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2790318)[0m saving checkpoint...
[36m(_train_fn pid=2790318)[0m Epoch: 4 cost time: 4.687740802764893
[36m(_train_fn pid=2790318)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.6146849 Vali Loss: 0.2705928 Best vali loss: 0.2667734
[36m(_train_fn pid=2790318)[0m 	iters: 600, epoch: 5 | loss: 0.4263970[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2790318)[0m 	speed: 0.0044s/iter; left time: 15.9973s[32m [repeated 10x across cluster][0m

Trial trial-9b77c_00004 completed after 5 iterations at 2024-08-26 13:32:13. Total running time: 1min 38s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.38505 â”‚
â”‚ time_total_s                                28.54632 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.26677 â”‚
â”‚ train_loss                                   0.43419 â”‚
â”‚ valid_loss                                   0.27142 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2790318)[0m Updating learning rate to 0.00022369925551004054
[36m(_train_fn pid=2790318)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2790318)[0m saving checkpoint...
[36m(_train_fn pid=2790318)[0m Epoch: 5 cost time: 4.7737250328063965
[36m(_train_fn pid=2790318)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.4341850 Vali Loss: 0.2714180 Best vali loss: 0.2667734
[36m(_train_fn pid=2790318)[0m Early stopping

Trial trial-9b77c_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00132 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2790861)[0m configuration
[36m(_train_fn pid=2790861)[0m {'batch_size': 64, 'learning_rate': 0.00010734405345907071, 'd_model': 64, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0013179176757202608, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=2790861)[0m Use GPU: cuda:0
[36m(_train_fn pid=2790861)[0m train 8449
2024-08-26 13:32:15,816	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2790861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-32-13/checkpoint_000000)
2024-08-26 13:32:16,303	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2790861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-32-13/checkpoint_000001)
2024-08-26 13:32:16,789	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2790861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-32-13/checkpoint_000002)
2024-08-26 13:32:17,273	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2790861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-32-13/checkpoint_000003)
2024-08-26 13:32:17,759	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2790861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-32-13/checkpoint_000004)
2024-08-26 13:32:18,237	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2790861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-32-13/checkpoint_000005)
2024-08-26 13:32:18,727	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2790861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-32-13/checkpoint_000006)
2024-08-26 13:32:19,217	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2790861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-32-13/checkpoint_000007)
2024-08-26 13:32:23,215	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2791533)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-32-19/checkpoint_000000)
2024-08-26 13:32:25,111	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2791533)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-32-19/checkpoint_000001)
2024-08-26 13:32:26,992	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2791533)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-32-19/checkpoint_000002)
2024-08-26 13:32:28,852	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2791533)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-32-19/checkpoint_000003)
2024-08-26 13:32:30,712	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2791533)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-32-19/checkpoint_000004)
2024-08-26 13:32:32,540	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2791533)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-32-19/checkpoint_000005)
[36m(_train_fn pid=2790861)[0m val 2785
[36m(_train_fn pid=2790861)[0m start_epoch 0
[36m(_train_fn pid=2790861)[0m max_epoch 8
[36m(_train_fn pid=2790861)[0m 	iters: 100, epoch: 1 | loss: 0.6778051[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2790861)[0m 	speed: 0.0077s/iter; left time: 7.4528s[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2790861)[0m Validation loss decreased (inf --> 0.2578).  Saving model state dict ...
[36m(_train_fn pid=2790861)[0m Validation loss decreased (0.2578 --> 0.2387).  Saving model state dict ...
[36m(_train_fn pid=2790861)[0m Validation loss decreased (0.2387 --> 0.2336).  Saving model state dict ...
[36m(_train_fn pid=2790861)[0m Validation loss decreased (0.2336 --> 0.2315).  Saving model state dict ...
[36m(_train_fn pid=2790861)[0m Validation loss decreased (0.2315 --> 0.2305).  Saving model state dict ...
[36m(_train_fn pid=2790861)[0m Validation loss decreased (0.2305 --> 0.2300).  Saving model state dict ...
[36m(_train_fn pid=2790861)[0m Updating learning rate to 3.35450167059596e-06[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2790861)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2790861)[0m Epoch: 6 cost time: 0.3548901081085205[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2790861)[0m Epoch: 6, Steps: 133 | Train Loss: 0.3472632 Vali Loss: 0.2300409 Best vali loss: 0.2300409[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2790861)[0m Validation loss decreased (0.2300 --> 0.2298).  Saving model state dict ...

Trial trial-9b77c_00005 completed after 8 iterations at 2024-08-26 13:32:19. Total running time: 1min 44s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.48768 â”‚
â”‚ time_total_s                                 4.52117 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22969 â”‚
â”‚ train_loss                                   0.36795 â”‚
â”‚ valid_loss                                   0.22969 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2790861)[0m Validation loss decreased (0.2298 --> 0.2297).  Saving model state dict ...

Trial trial-9b77c_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00435 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00044 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2791533)[0m configuration
[36m(_train_fn pid=2791533)[0m {'batch_size': 16, 'learning_rate': 0.0004377603894965504, 'd_model': 32, 'd_core': 256, 'e_layers': 2, 'dropout': 0.00435286364938959, 'lradj': 'type1', 'd_ff': 32}
[36m(_train_fn pid=2791533)[0m Use GPU: cuda:0
[36m(_train_fn pid=2790861)[0m 	iters: 100, epoch: 8 | loss: 0.3679461[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2790861)[0m 	speed: 0.0048s/iter; left time: 0.1648s[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2791533)[0m train 8449
[36m(_train_fn pid=2791533)[0m val 2785
[36m(_train_fn pid=2791533)[0m start_epoch 0
[36m(_train_fn pid=2791533)[0m max_epoch 8
[36m(_train_fn pid=2791533)[0m Validation loss decreased (inf --> 0.2230).  Saving model state dict ...
[36m(_train_fn pid=2791533)[0m Updating learning rate to 0.0004377603894965504[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2791533)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2791533)[0m Epoch: 1 cost time: 1.885209560394287[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2791533)[0m Epoch: 1, Steps: 529 | Train Loss: 0.5443157 Vali Loss: 0.2229993 Best vali loss: 0.2229993[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2791533)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2791533)[0m 	iters: 200, epoch: 3 | loss: 0.3344912[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2791533)[0m 	speed: 0.0029s/iter; left time: 8.5063s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2791533)[0m Validation loss decreased (0.2230 --> 0.2218).  Saving model state dict ...
[36m(_train_fn pid=2791533)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2791533)[0m Updating learning rate to 5.47200486870688e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2791533)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2791533)[0m Epoch: 4 cost time: 1.5749564170837402[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2791533)[0m Epoch: 4, Steps: 529 | Train Loss: 0.3405288 Vali Loss: 0.2232343 Best vali loss: 0.2217501[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2791533)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2791533)[0m 	iters: 100, epoch: 6 | loss: 0.4815346[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2791533)[0m 	speed: 0.0068s/iter; left time: 10.1625s[32m [repeated 14x across cluster][0m

Trial trial-9b77c_00006 completed after 6 iterations at 2024-08-26 13:32:32. Total running time: 1min 58s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.82498 â”‚
â”‚ time_total_s                                11.87333 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22175 â”‚
â”‚ train_loss                                   0.36244 â”‚
â”‚ valid_loss                                   0.22489 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2791533)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2791533)[0m Early stopping

Trial trial-9b77c_00007 started with configuration:
2024-08-26 13:32:35,628	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:32:36,599	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:32:37,593	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2792095)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00007_7_alpha_d_ff=2,batch_size=32,d_core=64,d_model=512,dropout=0.0058,e_layers=1,learning_rate=0.0027,lradj=type1_2024-08-26_13-32-32/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 13:32:38,573	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00579 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00268 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2792095)[0m configuration
[36m(_train_fn pid=2792095)[0m {'batch_size': 32, 'learning_rate': 0.002681331476927111, 'd_model': 512, 'd_core': 64, 'e_layers': 1, 'dropout': 0.005787899105404264, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2792095)[0m Use GPU: cuda:0
[36m(_train_fn pid=2791533)[0m Updating learning rate to 1.36800121717672e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2791533)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2791533)[0m Epoch: 6 cost time: 1.5573558807373047[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2791533)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3624397 Vali Loss: 0.2248932 Best vali loss: 0.2217501[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2792095)[0m train 8449
[36m(_train_fn pid=2792095)[0m val 2785
[36m(_train_fn pid=2792095)[0m start_epoch 0
[36m(_train_fn pid=2792095)[0m max_epoch 8

Trial status: 7 TERMINATED | 1 RUNNING
Current time: 2024-08-26 13:32:34. Total running time: 2min 0s
Logical resource usage: 1.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 9b77c_00001 with best_valid_loss=0.21595204482707772 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-9b77c_00007   RUNNING                32       0.00268133          512              2         64            1   0.0057879    type1                                                                                 â”‚
â”‚ trial-9b77c_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            4.60712       0.433672       0.232965            0.219535 â”‚
â”‚ trial-9b77c_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           15.1912        0.342769       0.222818            0.215952 â”‚
â”‚ trial-9b77c_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           21.2837        0.359801       0.270699            0.26892  â”‚
â”‚ trial-9b77c_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           21.8431        0.245632       0.227429            0.220544 â”‚
â”‚ trial-9b77c_00004   TERMINATED              8       0.00357919          256              3         64            4   0.00325461   type1          5           28.5463        0.434185       0.271418            0.266773 â”‚
â”‚ trial-9b77c_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            4.52117       0.367946       0.229691            0.229691 â”‚
â”‚ trial-9b77c_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           11.8733        0.36244        0.224893            0.22175  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2792095)[0m Validation loss decreased (inf --> 0.2258).  Saving model state dict ...
[36m(_train_fn pid=2792095)[0m 	iters: 200, epoch: 2 | loss: 0.2969854[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2792095)[0m 	speed: 0.0029s/iter; left time: 4.8450s[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2792095)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2792095)[0m EarlyStopping counter: 2 out of 3

Trial trial-9b77c_00007 completed after 4 iterations at 2024-08-26 13:32:38. Total running time: 2min 4s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.97754 â”‚
â”‚ time_total_s                                 4.58993 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22575 â”‚
â”‚ train_loss                                   0.20782 â”‚
â”‚ valid_loss                                   0.25053 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2792095)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2792095)[0m Early stopping

Trial trial-9b77c_00008 started with configuration:
2024-08-26 13:32:42,219	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:32:43,770	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2792487)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-32-38/checkpoint_000001)[32m [repeated 3x across cluster][0m
2024-08-26 13:32:45,345	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:32:46,922	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:32:48,520	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:32:50,108	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2792487)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-32-38/checkpoint_000005)[32m [repeated 4x across cluster][0m
2024-08-26 13:32:53,459	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:32:54,640	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:32:55,937	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2793039)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-32-50/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 13:32:57,139	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:32:58,350	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00333 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00196 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2792487)[0m {'batch_size': 32, 'learning_rate': 0.001963618480113192, 'd_model': 64, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0033338002527237612, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2792487)[0m configuration
[36m(_train_fn pid=2792487)[0m Use GPU: cuda:0
[36m(_train_fn pid=2792095)[0m Updating learning rate to 0.0003351664346158889[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2792095)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2792095)[0m Epoch: 4 cost time: 0.8177480697631836[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2792095)[0m Epoch: 4, Steps: 265 | Train Loss: 0.2078220 Vali Loss: 0.2505336 Best vali loss: 0.2257504[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2792487)[0m train 8449
[36m(_train_fn pid=2792487)[0m val 2785
[36m(_train_fn pid=2792487)[0m start_epoch 0
[36m(_train_fn pid=2792487)[0m max_epoch 8
[36m(_train_fn pid=2792487)[0m 	iters: 200, epoch: 1 | loss: 0.4194447[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2792487)[0m 	speed: 0.0049s/iter; left time: 9.4104s[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2792487)[0m Validation loss decreased (inf --> 0.2248).  Saving model state dict ...
[36m(_train_fn pid=2792487)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2792487)[0m Validation loss decreased (0.2248 --> 0.2229).  Saving model state dict ...
[36m(_train_fn pid=2792487)[0m Updating learning rate to 0.0013575313699692147[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2792487)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2792487)[0m Epoch: 3 cost time: 1.3736896514892578[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2792487)[0m Epoch: 3, Steps: 265 | Train Loss: 0.4804408 Vali Loss: 0.2229292 Best vali loss: 0.2229292[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2792487)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2792487)[0m 	iters: 200, epoch: 4 | loss: 0.2010550[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2792487)[0m 	speed: 0.0051s/iter; left time: 5.7831s[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2792487)[0m EarlyStopping counter: 2 out of 3

Trial trial-9b77c_00008 completed after 6 iterations at 2024-08-26 13:32:50. Total running time: 2min 15s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.58532 â”‚
â”‚ time_total_s                                10.08372 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22293 â”‚
â”‚ train_loss                                   0.20661 â”‚
â”‚ valid_loss                                   0.24217 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2792487)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2792487)[0m Early stopping

Trial trial-9b77c_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00128 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00073 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2793039)[0m configuration
[36m(_train_fn pid=2793039)[0m {'batch_size': 32, 'learning_rate': 0.0007335788889365637, 'd_model': 32, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0012802554597501359, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2793039)[0m Use GPU: cuda:0
[36m(_train_fn pid=2792487)[0m Updating learning rate to 0.00028756526858096616[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2792487)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2792487)[0m Epoch: 6 cost time: 1.3873076438903809[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2792487)[0m Epoch: 6, Steps: 265 | Train Loss: 0.2066133 Vali Loss: 0.2421666 Best vali loss: 0.2229292[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2793039)[0m train 8449
[36m(_train_fn pid=2793039)[0m val 2785
[36m(_train_fn pid=2793039)[0m start_epoch 0
[36m(_train_fn pid=2793039)[0m max_epoch 8
[36m(_train_fn pid=2792487)[0m 	iters: 200, epoch: 6 | loss: 0.2011863[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2792487)[0m 	speed: 0.0052s/iter; left time: 3.0777s[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2793039)[0m Validation loss decreased (inf --> 0.2216).  Saving model state dict ...
[36m(_train_fn pid=2793039)[0m Validation loss decreased (0.2216 --> 0.2208).  Saving model state dict ...
[36m(_train_fn pid=2793039)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2793039)[0m Updating learning rate to 0.00018339472223414093[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2793039)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2793039)[0m Epoch: 3 cost time: 1.1096687316894531[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2793039)[0m Epoch: 3, Steps: 265 | Train Loss: 0.2450095 Vali Loss: 0.2231636 Best vali loss: 0.2207911[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2793039)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2793039)[0m 	iters: 200, epoch: 4 | loss: 0.4004461[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2793039)[0m 	speed: 0.0037s/iter; left time: 4.1279s[32m [repeated 8x across cluster][0m

Trial trial-9b77c_00009 completed after 5 iterations at 2024-08-26 13:32:58. Total running time: 2min 23s
2024-08-26 13:32:58,359	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials' in 0.0073s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-9b77c_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                              1.2083 â”‚
â”‚ time_total_s                                 6.78285 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22079 â”‚
â”‚ train_loss                                   0.31516 â”‚
â”‚ valid_loss                                   0.22207 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:32:58. Total running time: 2min 23s
Logical resource usage: 0/32 CPUs, 0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 9b77c_00001 with best_valid_loss=0.21595204482707772 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-9b77c_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            4.60712       0.433672       0.232965            0.219535 â”‚
â”‚ trial-9b77c_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           15.1912        0.342769       0.222818            0.215952 â”‚
â”‚ trial-9b77c_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           21.2837        0.359801       0.270699            0.26892  â”‚
â”‚ trial-9b77c_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           21.8431        0.245632       0.227429            0.220544 â”‚
â”‚ trial-9b77c_00004   TERMINATED              8       0.00357919          256              3         64            4   0.00325461   type1          5           28.5463        0.434185       0.271418            0.266773 â”‚
â”‚ trial-9b77c_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            4.52117       0.367946       0.229691            0.229691 â”‚
â”‚ trial-9b77c_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           11.8733        0.36244        0.224893            0.22175  â”‚
â”‚ trial-9b77c_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          4            4.58993       0.207822       0.250534            0.22575  â”‚
â”‚ trial-9b77c_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6           10.0837        0.206613       0.242167            0.222929 â”‚
â”‚ trial-9b77c_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            6.78285       0.315164       0.222069            0.220791 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2793039)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_1_parallel_trials/trial-9b77c_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-32-50/checkpoint_000004)[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2793039)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2793039)[0m Early stopping
[36m(_train_fn pid=2793039)[0m Updating learning rate to 4.584868055853523e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2793039)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2793039)[0m Epoch: 5 cost time: 1.0231757164001465[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2793039)[0m Epoch: 5, Steps: 265 | Train Loss: 0.3151644 Vali Loss: 0.2220689 Best vali loss: 0.2207911[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2793039)[0m 	iters: 200, epoch: 5 | loss: 0.3968734[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2793039)[0m 	speed: 0.0036s/iter; left time: 3.1276s[32m [repeated 2x across cluster][0m


Time taken (1 parallel trials): 148 seconds


2024-08-26 13:33:02,286	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:33:02,643	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:33:02,649	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:33:02,656	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:33:06,183	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2795742)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00000_0_2024-08-26_13-33-02/checkpoint_000000)
2024-08-26 13:33:07,544	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2795742)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00000_0_2024-08-26_13-33-02/checkpoint_000001)
2024-08-26 13:33:08,887	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2795742)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00000_0_2024-08-26_13-33-02/checkpoint_000002)
2024-08-26 13:33:09,870	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:10,081	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_2_parallel_trials   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator                â”‚
â”‚ Scheduler                        FIFOScheduler                        â”‚
â”‚ Number of trials                 10                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-33-01_851324_2793536/artifacts/2024-08-26_13-33-02/ETTh2_96_96_test_2_parallel_trials/driver_artifacts`

Trial status: 2 PENDING
Current time: 2024-08-26 13:33:02. Total running time: 0s
Logical resource usage: 2.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-f3d0e_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-f3d0e_00001   PENDING               8       6.40637e-05         256              4         64            2   0.00128816   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-f3d0e_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-f3d0e_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00129 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2795742)[0m configuration
[36m(_train_fn pid=2795742)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2795742)[0m Use GPU: cuda:0
[36m(_train_fn pid=2795742)[0m train 8449
[36m(_train_fn pid=2795742)[0m val 2785
[36m(_train_fn pid=2795742)[0m start_epoch 0
[36m(_train_fn pid=2795742)[0m max_epoch 8
[36m(_train_fn pid=2795742)[0m 	iters: 100, epoch: 1 | loss: 0.5344234
[36m(_train_fn pid=2795742)[0m 	speed: 0.0095s/iter; left time: 19.2257s
[36m(_train_fn pid=2795742)[0m Updating learning rate to 0.000288581929876693
[36m(_train_fn pid=2795742)[0m saving checkpoint...
[36m(_train_fn pid=2795742)[0m Validation loss decreased (inf --> 0.2195).  Saving model state dict ...
[36m(_train_fn pid=2795742)[0m Epoch: 1 cost time: 1.3596017360687256
[36m(_train_fn pid=2795742)[0m Epoch: 1, Steps: 265 | Train Loss: 0.5247267 Vali Loss: 0.2195345 Best vali loss: 0.2195345
[36m(_train_fn pid=2795742)[0m Updating learning rate to 0.00025606601717798207
[36m(_train_fn pid=2795742)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2795742)[0m saving checkpoint...
[36m(_train_fn pid=2795742)[0m Epoch: 2 cost time: 1.1290462017059326
[36m(_train_fn pid=2795742)[0m Epoch: 2, Steps: 265 | Train Loss: 0.4370105 Vali Loss: 0.2229692 Best vali loss: 0.2195345
[36m(_train_fn pid=2795742)[0m Updating learning rate to 0.00020740251485476345
[36m(_train_fn pid=2795742)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2795742)[0m saving checkpoint...
[36m(_train_fn pid=2795742)[0m Epoch: 3 cost time: 1.1091842651367188
[36m(_train_fn pid=2795742)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3195181 Vali Loss: 0.2231508 Best vali loss: 0.2195345
[36m(_train_fn pid=2795743)[0m configuration
[36m(_train_fn pid=2795743)[0m {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2795743)[0m Use GPU: cuda:0
[36m(_train_fn pid=2795743)[0m train 8449
[36m(_train_fn pid=2795743)[0m val 2785
[36m(_train_fn pid=2795743)[0m start_epoch 0
[36m(_train_fn pid=2795743)[0m max_epoch 8

Trial trial-f3d0e_00000 completed after 4 iterations at 2024-08-26 13:33:10. Total running time: 7s
2024-08-26 13:33:13,389	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2795743)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-33-02/checkpoint_000001)[32m [repeated 3x across cluster][0m
2024-08-26 13:33:16,532	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:16,887	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:20,395	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2795743)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-33-02/checkpoint_000003)[32m [repeated 3x across cluster][0m
2024-08-26 13:33:20,798	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:25,033	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.19193 â”‚
â”‚ time_total_s                                 5.90876 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21953 â”‚
â”‚ train_loss                                   0.43367 â”‚
â”‚ valid_loss                                   0.23297 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2795742)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2795742)[0m Early stopping
[36m(_train_fn pid=2795743)[0m 	iters: 200, epoch: 2 | loss: 0.9777274[32m [repeated 19x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2795743)[0m 	speed: 0.0028s/iter; left time: 19.8764s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2795742)[0m Updating learning rate to 0.00015[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2795742)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2795743)[0m Validation loss decreased (inf --> 0.2160).  Saving model state dict ...
[36m(_train_fn pid=2795742)[0m Epoch: 4 cost time: 0.9654181003570557[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2795742)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4336717 Vali Loss: 0.2329654 Best vali loss: 0.2195345[32m [repeated 2x across cluster][0m

Trial trial-f3d0e_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00891 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00351 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2796413)[0m configuration
[36m(_train_fn pid=2796413)[0m {'batch_size': 16, 'learning_rate': 0.0035128531928926765, 'd_model': 512, 'd_core': 64, 'e_layers': 4, 'dropout': 0.00890604745419256, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2796413)[0m Use GPU: cuda:0
[36m(_train_fn pid=2796413)[0m train 8449
[36m(_train_fn pid=2796413)[0m val 2785
[36m(_train_fn pid=2796413)[0m start_epoch 0
[36m(_train_fn pid=2796413)[0m max_epoch 8
[36m(_train_fn pid=2795743)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2795743)[0m 	iters: 800, epoch: 3 | loss: 1.2098777[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2795743)[0m 	speed: 0.0028s/iter; left time: 15.2911s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2796413)[0m Validation loss decreased (inf --> 0.2884).  Saving model state dict ...
[36m(_train_fn pid=2796413)[0m Updating learning rate to 0.0035128531928926765[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2796413)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2796413)[0m Epoch: 1 cost time: 4.049784898757935[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2796413)[0m Epoch: 1, Steps: 529 | Train Loss: 0.7850501 Vali Loss: 0.2883907 Best vali loss: 0.2883907[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2795743)[0m EarlyStopping counter: 2 out of 3

Trial trial-f3d0e_00001 completed after 4 iterations at 2024-08-26 13:33:20. Total running time: 17s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             3.50595 â”‚
â”‚ time_total_s                                  16.221 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21595 â”‚
â”‚ train_loss                                   0.34303 â”‚
â”‚ valid_loss                                   0.22278 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2795743)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2795743)[0m Early stopping
[36m(_train_fn pid=2796413)[0m Validation loss decreased (0.2884 --> 0.2775).  Saving model state dict ...
[36m(_train_fn pid=2796413)[0m 	iters: 500, epoch: 2 | loss: 0.3073497[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2796413)[0m 	speed: 0.0071s/iter; left time: 22.8936s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2796413)[0m Updating learning rate to 0.0017564265964463382[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2796413)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2796413)[0m Epoch: 2 cost time: 3.8139991760253906[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2796413)[0m Epoch: 2, Steps: 529 | Train Loss: 0.4281371 Vali Loss: 0.2774843 Best vali loss: 0.2774843[32m [repeated 3x across cluster][0m

Trial trial-f3d0e_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00596 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00015 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2796854)[0m configuration
[36m(_train_fn pid=2796854)[0m {'batch_size': 8, 'learning_rate': 0.00015448958497378217, 'd_model': 128, 'd_core': 512, 'e_layers': 4, 'dropout': 0.005958307406826554, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2796854)[0m Use GPU: cuda:0
[36m(_train_fn pid=2796854)[0m train 8449
[36m(_train_fn pid=2796854)[0m val 2785
[36m(_train_fn pid=2796854)[0m start_epoch 0
[36m(_train_fn pid=2796854)[0m max_epoch 8
[36m(_train_fn pid=2796413)[0m Validation loss decreased (0.2775 --> 0.2703).  Saving model state dict ...
[36m(_train_fn pid=2796854)[0m 	iters: 800, epoch: 1 | loss: 0.3597501[32m [repeated 14x across cluster][0m
2024-08-26 13:33:28,049	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2796854)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-33-20/checkpoint_000000)[32m [repeated 3x across cluster][0m
2024-08-26 13:33:29,307	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:33,512	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:33,552	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2796413)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-33-10/checkpoint_000004)[32m [repeated 2x across cluster][0m
2024-08-26 13:33:37,757	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2796854)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-33-20/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 13:33:38,861	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:41,998	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2796854)[0m 	speed: 0.0046s/iter; left time: 35.0011s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2796413)[0m Updating learning rate to 0.0008782132982231691
[36m(_train_fn pid=2796413)[0m saving checkpoint...
[36m(_train_fn pid=2796413)[0m Epoch: 3 cost time: 3.796029567718506
[36m(_train_fn pid=2796413)[0m Epoch: 3, Steps: 529 | Train Loss: 0.6475151 Vali Loss: 0.2702828 Best vali loss: 0.2702828
[36m(_train_fn pid=2796854)[0m Updating learning rate to 0.00014860967525861138
[36m(_train_fn pid=2796854)[0m saving checkpoint...
[36m(_train_fn pid=2796854)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...
[36m(_train_fn pid=2796854)[0m Epoch: 1 cost time: 5.146520614624023
[36m(_train_fn pid=2796854)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.4099332 Vali Loss: 0.2205494 Best vali loss: 0.2205494
[36m(_train_fn pid=2796413)[0m Validation loss decreased (0.2703 --> 0.2689).  Saving model state dict ...
[36m(_train_fn pid=2796854)[0m 	iters: 700, epoch: 2 | loss: 0.2889208[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2796854)[0m 	speed: 0.0044s/iter; left time: 29.4635s[32m [repeated 15x across cluster][0m

Trial status: 2 TERMINATED | 2 RUNNING
Current time: 2024-08-26 13:33:32. Total running time: 30s
Logical resource usage: 2.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: f3d0e_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-f3d0e_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          4           17.749         0.409946       0.268922            0.268922 â”‚
â”‚ trial-f3d0e_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         1            6.18823       0.409933       0.220549            0.220549 â”‚
â”‚ trial-f3d0e_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            5.90876       0.433672       0.232965            0.219535 â”‚
â”‚ trial-f3d0e_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           16.221         0.343032       0.222783            0.215952 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2796413)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2796413)[0m Updating learning rate to 0.00021955332455579228[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2796413)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2796413)[0m Epoch: 5 cost time: 3.8316922187805176[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2796413)[0m Epoch: 5, Steps: 529 | Train Loss: 0.4993637 Vali Loss: 0.2703723 Best vali loss: 0.2689222[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2796413)[0m 	iters: 400, epoch: 6 | loss: 0.3279820[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2796413)[0m 	speed: 0.0071s/iter; left time: 8.4496s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2796854)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2796854)[0m Updating learning rate to 0.00010680509480810367[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2796854)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2796413)[0m Epoch: 6 cost time: 3.799487829208374[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2796413)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3591892 Vali Loss: 0.2706392 Best vali loss: 0.2689222[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2796854)[0m 	iters: 600, epoch: 4 | loss: 0.2013022[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2796854)[0m 	speed: 0.0043s/iter; left time: 20.1374s[32m [repeated 16x across cluster][0m

Trial trial-f3d0e_00002 completed after 7 iterations at 2024-08-26 13:33:41. Total running time: 39s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                              4.2386 â”‚
â”‚ time_total_s                                 30.4278 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26892 â”‚
â”‚ train_loss                                   0.35979 â”‚
â”‚ valid_loss                                   0.27069 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2796413)[0m Early stopping

Trial trial-f3d0e_00004 started with configuration:
2024-08-26 13:33:44,325	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2796854)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-33-20/checkpoint_000003)[32m [repeated 2x across cluster][0m
2024-08-26 13:33:46,854	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:47,317	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:47,792	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:48,257	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:48,736	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:49,217	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:49,685	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:49,707	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2797620)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-33-42/checkpoint_000000)[32m [repeated 7x across cluster][0m
2024-08-26 13:33:50,222	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00325 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00358 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2797620)[0m configuration
[36m(_train_fn pid=2797620)[0m {'batch_size': 8, 'learning_rate': 0.0035791880881606486, 'd_model': 256, 'd_core': 64, 'e_layers': 4, 'dropout': 0.003254613020686493, 'lradj': 'type1', 'd_ff': 768}
[36m(_train_fn pid=2797620)[0m Use GPU: cuda:0
[36m(_train_fn pid=2797620)[0m train 8449
[36m(_train_fn pid=2797620)[0m val 2785
[36m(_train_fn pid=2797620)[0m start_epoch 0
[36m(_train_fn pid=2797620)[0m max_epoch 8
[36m(_train_fn pid=2796413)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2796413)[0m Updating learning rate to 5.488833113894807e-05
[36m(_train_fn pid=2796413)[0m saving checkpoint...
[36m(_train_fn pid=2796413)[0m Epoch: 7 cost time: 3.8165700435638428[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2796413)[0m Epoch: 7, Steps: 529 | Train Loss: 0.3597851 Vali Loss: 0.2706914 Best vali loss: 0.2689222[32m [repeated 2x across cluster][0m

Trial trial-f3d0e_00003 completed after 4 iterations at 2024-08-26 13:33:44. Total running time: 41s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             5.46182 â”‚
â”‚ time_total_s                                22.45631 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22055 â”‚
â”‚ train_loss                                   0.24506 â”‚
â”‚ valid_loss                                   0.22707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2796854)[0m Updating learning rate to 7.724479248689109e-05
[36m(_train_fn pid=2796854)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2796854)[0m saving checkpoint...

Trial trial-f3d0e_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00132 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2797771)[0m 	iters: 100, epoch: 1 | loss: 0.6778051[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2797771)[0m 	speed: 0.0076s/iter; left time: 7.3132s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2797771)[0m Validation loss decreased (inf --> 0.2578).  Saving model state dict ...
[36m(_train_fn pid=2796854)[0m Early stopping
[36m(_train_fn pid=2797771)[0m Validation loss decreased (0.2578 --> 0.2387).  Saving model state dict ...
[36m(_train_fn pid=2797771)[0m Validation loss decreased (0.2387 --> 0.2336).  Saving model state dict ...
[36m(_train_fn pid=2797771)[0m Validation loss decreased (0.2336 --> 0.2315).  Saving model state dict ...
[36m(_train_fn pid=2797771)[0m configuration
[36m(_train_fn pid=2797771)[0m {'batch_size': 64, 'learning_rate': 0.00010734405345907071, 'd_model': 64, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0013179176757202608, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=2797771)[0m Use GPU: cuda:0
[36m(_train_fn pid=2797771)[0m train 8449
[36m(_train_fn pid=2797771)[0m val 2785
[36m(_train_fn pid=2797771)[0m Validation loss decreased (0.2315 --> 0.2305).  Saving model state dict ...
[36m(_train_fn pid=2797771)[0m start_epoch 0
[36m(_train_fn pid=2797771)[0m max_epoch 8
[36m(_train_fn pid=2797771)[0m Epoch: 5 cost time: 0.3640625476837158[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2797771)[0m Epoch: 5, Steps: 133 | Train Loss: 0.4646299 Vali Loss: 0.2304958 Best vali loss: 0.2304958[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2797771)[0m Validation loss decreased (0.2305 --> 0.2300).  Saving model state dict ...
[36m(_train_fn pid=2797771)[0m Updating learning rate to 3.35450167059596e-06[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2797771)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2797771)[0m Validation loss decreased (0.2300 --> 0.2298).  Saving model state dict ...

Trial trial-f3d0e_00005 completed after 8 iterations at 2024-08-26 13:33:50. Total running time: 47s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.51197 â”‚
â”‚ time_total_s                                 4.45631 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22969 â”‚
â”‚ train_loss                                   0.36795 â”‚
â”‚ valid_loss                                   0.22969 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2797771)[0m Validation loss decreased (0.2298 --> 0.2297).  Saving model state dict ...

Trial trial-f3d0e_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00435 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00044 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2798519)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-33-50/checkpoint_000000)[32m [repeated 3x across cluster][0m
2024-08-26 13:33:56,293	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:33:58,190	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2798519)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-33-50/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 13:34:01,094	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:03,654	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:03,830	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:06,644	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2798519)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-33-50/checkpoint_000004)[32m [repeated 3x across cluster][0m
2024-08-26 13:34:09,730	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2798519)[0m configuration
[36m(_train_fn pid=2798519)[0m {'batch_size': 16, 'learning_rate': 0.0004377603894965504, 'd_model': 32, 'd_core': 256, 'e_layers': 2, 'dropout': 0.00435286364938959, 'lradj': 'type1', 'd_ff': 32}
[36m(_train_fn pid=2798519)[0m Use GPU: cuda:0
[36m(_train_fn pid=2797620)[0m 	iters: 400, epoch: 2 | loss: 0.2784161[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2797620)[0m 	speed: 0.0048s/iter; left time: 33.7623s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2798519)[0m train 8449
[36m(_train_fn pid=2798519)[0m val 2785
[36m(_train_fn pid=2797620)[0m Validation loss decreased (inf --> 0.2711).  Saving model state dict ...
[36m(_train_fn pid=2798519)[0m start_epoch 0
[36m(_train_fn pid=2798519)[0m max_epoch 8
[36m(_train_fn pid=2797771)[0m Epoch: 8 cost time: 0.390671968460083[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2797771)[0m Epoch: 8, Steps: 133 | Train Loss: 0.3679461 Vali Loss: 0.2296906 Best vali loss: 0.2296906[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2797771)[0m Updating learning rate to 8.3862541764899e-07[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2797771)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2798519)[0m Validation loss decreased (inf --> 0.2230).  Saving model state dict ...
[36m(_train_fn pid=2797620)[0m Validation loss decreased (0.2711 --> 0.2668).  Saving model state dict ...
[36m(_train_fn pid=2797620)[0m 	iters: 100, epoch: 3 | loss: 0.4695274[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2797620)[0m 	speed: 0.0194s/iter; left time: 121.2150s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2798519)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2798519)[0m Epoch: 2 cost time: 2.3775012493133545[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2798519)[0m Epoch: 2, Steps: 529 | Train Loss: 0.3621016 Vali Loss: 0.2247194 Best vali loss: 0.2229993[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2798519)[0m Updating learning rate to 0.0002188801947482752[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2798519)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2798519)[0m Validation loss decreased (0.2230 --> 0.2218).  Saving model state dict ...
[36m(_train_fn pid=2798519)[0m 	iters: 200, epoch: 4 | loss: 0.1557815[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2798519)[0m 	speed: 0.0048s/iter; left time: 11.8082s[32m [repeated 17x across cluster][0m

Trial status: 5 TERMINATED | 2 RUNNING
Current time: 2024-08-26 13:34:02. Total running time: 1min 0s
Logical resource usage: 2.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: f3d0e_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-f3d0e_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          2           12.83          0.5709         0.266773            0.266773 â”‚
â”‚ trial-f3d0e_00006   RUNNING                16       0.00043776           32              1        256            2   0.00435286   type1          3            9.37024       0.380841       0.22175             0.22175  â”‚
â”‚ trial-f3d0e_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            5.90876       0.433672       0.232965            0.219535 â”‚
â”‚ trial-f3d0e_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           16.221         0.343032       0.222783            0.215952 â”‚
â”‚ trial-f3d0e_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           30.4278        0.359785       0.270691            0.268922 â”‚
â”‚ trial-f3d0e_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           22.4563        0.245055       0.22707             0.220549 â”‚
â”‚ trial-f3d0e_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            4.45631       0.367946       0.229691            0.229691 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2797620)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2798519)[0m Epoch: 4 cost time: 2.2987096309661865[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2798519)[0m Epoch: 4, Steps: 529 | Train Loss: 0.3405288 Vali Loss: 0.2232343 Best vali loss: 0.2217501[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2798519)[0m Updating learning rate to 5.47200486870688e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2798519)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2798519)[0m 	iters: 100, epoch: 6 | loss: 0.4815346[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2798519)[0m 	speed: 0.0112s/iter; left time: 16.6736s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2798519)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m

Trial trial-f3d0e_00006 completed after 6 iterations at 2024-08-26 13:34:09. Total running time: 1min 7s
2024-08-26 13:34:10,755	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:12,929	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2799248)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00007_7_alpha_d_ff=2,batch_size=32,d_core=64,d_model=512,dropout=0.0058,e_layers=1,learning_rate=0.0027,lradj=type1_2024-08-26_13-34-09/checkpoint_000000)[32m [repeated 3x across cluster][0m
2024-08-26 13:34:13,900	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:14,874	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:15,853	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:16,232	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             3.08384 â”‚
â”‚ time_total_s                                17.99868 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22175 â”‚
â”‚ train_loss                                   0.36244 â”‚
â”‚ valid_loss                                   0.22489 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2798519)[0m Epoch: 5 cost time: 2.347910165786743
[36m(_train_fn pid=2798519)[0m Epoch: 5, Steps: 529 | Train Loss: 0.3294062 Vali Loss: 0.2245405 Best vali loss: 0.2217501
[36m(_train_fn pid=2798519)[0m Epoch: 6 cost time: 2.5689163208007812
[36m(_train_fn pid=2798519)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3624397 Vali Loss: 0.2248932 Best vali loss: 0.2217501
[36m(_train_fn pid=2798519)[0m Early stopping
[36m(_train_fn pid=2797620)[0m Updating learning rate to 0.00044739851102008107[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2797620)[0m saving checkpoint...[32m [repeated 3x across cluster][0m

Trial trial-f3d0e_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00579 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00268 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2799248)[0m configuration
[36m(_train_fn pid=2799248)[0m {'batch_size': 32, 'learning_rate': 0.002681331476927111, 'd_model': 512, 'd_core': 64, 'e_layers': 1, 'dropout': 0.005787899105404264, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2799248)[0m Use GPU: cuda:0
[36m(_train_fn pid=2799248)[0m train 8449
[36m(_train_fn pid=2799248)[0m val 2785
[36m(_train_fn pid=2799248)[0m start_epoch 0
[36m(_train_fn pid=2799248)[0m max_epoch 8
[36m(_train_fn pid=2797620)[0m 	iters: 300, epoch: 5 | loss: 1.4156616[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2797620)[0m 	speed: 0.0044s/iter; left time: 17.4198s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2799248)[0m Validation loss decreased (inf --> 0.2258).  Saving model state dict ...
[36m(_train_fn pid=2799248)[0m EarlyStopping counter: 1 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2799248)[0m Epoch: 2 cost time: 0.8075368404388428[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2799248)[0m Epoch: 2, Steps: 265 | Train Loss: 0.3472182 Vali Loss: 0.2326834 Best vali loss: 0.2257504[32m [repeated 3x across cluster][0m

Trial trial-f3d0e_00007 completed after 4 iterations at 2024-08-26 13:34:15. Total running time: 1min 13s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             0.97704 â”‚
â”‚ time_total_s                                 4.59918 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22575 â”‚
â”‚ train_loss                                   0.20782 â”‚
â”‚ valid_loss                                   0.25053 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2799248)[0m Early stopping
[36m(_train_fn pid=2799248)[0m Updating learning rate to 0.0003351664346158889[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2799248)[0m saving checkpoint...[32m [repeated 4x across cluster][0m

Trial trial-f3d0e_00004 completed after 5 iterations at 2024-08-26 13:34:16. Total running time: 1min 13s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.47396 â”‚
â”‚ time_total_s                                32.75939 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.26677 â”‚
â”‚ train_loss                                   0.43419 â”‚
â”‚ valid_loss                                   0.27142 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-f3d0e_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00333 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00196 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2799743)[0m {'batch_size': 32, 'learning_rate': 0.001963618480113192, 'd_model': 64, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0033338002527237612, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2799743)[0m configuration
[36m(_train_fn pid=2799248)[0m 	iters: 200, epoch: 4 | loss: 0.2150291[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2799248)[0m 	speed: 0.0029s/iter; left time: 3.2655s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2799743)[0m Use GPU: cuda:0
[36m(_train_fn pid=2799743)[0m train 8449
[36m(_train_fn pid=2799743)[0m val 2785

Trial trial-f3d0e_00009 started with configuration:
2024-08-26 13:34:19,494	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:19,514	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2799743)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-34-15/checkpoint_000000)[32m [repeated 5x across cluster][0m
2024-08-26 13:34:20,688	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:20,984	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:21,867	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:22,477	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:23,134	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:23,988	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:24,317	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:25,458	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2799743)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-34-15/checkpoint_000004)[32m [repeated 9x across cluster][0m
2024-08-26 13:34:26,937	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:26,946	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials' in 0.0081s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00128 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00073 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2799803)[0m configuration
[36m(_train_fn pid=2799803)[0m {'batch_size': 32, 'learning_rate': 0.0007335788889365637, 'd_model': 32, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0012802554597501359, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2799743)[0m start_epoch 0
[36m(_train_fn pid=2799743)[0m max_epoch 8
[36m(_train_fn pid=2799743)[0m Validation loss decreased (inf --> 0.2248).  Saving model state dict ...
[36m(_train_fn pid=2797620)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2799803)[0m Epoch: 1 cost time: 1.214216947555542[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2799803)[0m Epoch: 1, Steps: 265 | Train Loss: 0.6565596 Vali Loss: 0.2215726 Best vali loss: 0.2215726[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2799803)[0m Validation loss decreased (0.2216 --> 0.2208).  Saving model state dict ...
[36m(_train_fn pid=2797620)[0m Early stopping
[36m(_train_fn pid=2799743)[0m Updating learning rate to 0.001676053211532226[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2799743)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2799803)[0m 	iters: 100, epoch: 4 | loss: 0.4661205[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2799803)[0m 	speed: 0.0083s/iter; left time: 10.2083s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2799803)[0m Use GPU: cuda:0
[36m(_train_fn pid=2799803)[0m train 8449
[36m(_train_fn pid=2799803)[0m val 2785
[36m(_train_fn pid=2799803)[0m start_epoch 0
[36m(_train_fn pid=2799803)[0m max_epoch 8

Trial trial-f3d0e_00009 completed after 5 iterations at 2024-08-26 13:34:24. Total running time: 1min 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                              1.1813 â”‚
â”‚ time_total_s                                 6.59943 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22079 â”‚
â”‚ train_loss                                   0.31516 â”‚
â”‚ valid_loss                                   0.22207 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2799803)[0m Early stopping
[36m(_train_fn pid=2799803)[0m Validation loss decreased (inf --> 0.2216).  Saving model state dict ...
[36m(_train_fn pid=2799803)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2799743)[0m Epoch: 5 cost time: 1.261885404586792[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2799743)[0m Epoch: 5, Steps: 265 | Train Loss: 0.2362683 Vali Loss: 0.2301794 Best vali loss: 0.2229292[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2799743)[0m Validation loss decreased (0.2248 --> 0.2229).  Saving model state dict ...
[36m(_train_fn pid=2799743)[0m Updating learning rate to 0.0006060871101439775[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2799743)[0m saving checkpoint...[32m [repeated 6x across cluster][0m

Trial trial-f3d0e_00008 completed after 6 iterations at 2024-08-26 13:34:26. Total running time: 1min 24s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-f3d0e_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.47645 â”‚
â”‚ time_total_s                                 9.59528 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22293 â”‚
â”‚ train_loss                                   0.20661 â”‚
â”‚ valid_loss                                   0.24217 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:34:26. Total running time: 1min 24s
Logical resource usage: 1.0/32 CPUs, 0.5/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: f3d0e_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-f3d0e_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            5.90876       0.433672       0.232965            0.219535 â”‚
â”‚ trial-f3d0e_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           16.221         0.343032       0.222783            0.215952 â”‚
â”‚ trial-f3d0e_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           30.4278        0.359785       0.270691            0.268922 â”‚
â”‚ trial-f3d0e_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           22.4563        0.245055       0.22707             0.220549 â”‚
â”‚ trial-f3d0e_00004   TERMINATED              8       0.00357919          256              3         64            4   0.00325461   type1          5           32.7594        0.434185       0.271418            0.266773 â”‚
â”‚ trial-f3d0e_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            4.45631       0.367946       0.229691            0.229691 â”‚
â”‚ trial-f3d0e_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           17.9987        0.36244        0.224893            0.22175  â”‚
â”‚ trial-f3d0e_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          4            4.59918       0.207822       0.250534            0.22575  â”‚
â”‚ trial-f3d0e_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6            9.59528       0.206613       0.242167            0.222929 â”‚
â”‚ trial-f3d0e_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            6.59943       0.315164       0.222069            0.220791 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2799743)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_2_parallel_trials/trial-f3d0e_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-34-15/checkpoint_000005)
[36m(_train_fn pid=2799743)[0m 	iters: 200, epoch: 6 | loss: 0.2011863[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2799743)[0m 	speed: 0.0047s/iter; left time: 2.7786s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2799743)[0m Early stopping
[36m(_train_fn pid=2799743)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2799743)[0m Epoch: 6 cost time: 1.2735776901245117
[36m(_train_fn pid=2799743)[0m Epoch: 6, Steps: 265 | Train Loss: 0.2066133 Vali Loss: 0.2421666 Best vali loss: 0.2229292
[36m(_train_fn pid=2799743)[0m Updating learning rate to 0.00028756526858096616
[36m(_train_fn pid=2799743)[0m saving checkpoint...


Time taken (2 parallel trials): 89 seconds


2024-08-26 13:34:30,860	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:34:31,226	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:34:31,232	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:34:31,239	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:34:35,359	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2802979)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00000_0_2024-08-26_13-34-31/checkpoint_000000)
2024-08-26 13:34:37,347	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2802979)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00000_0_2024-08-26_13-34-31/checkpoint_000001)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_3_parallel_trials   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator                â”‚
â”‚ Scheduler                        FIFOScheduler                        â”‚
â”‚ Number of trials                 10                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-34-30_425840_2800758/artifacts/2024-08-26_13-34-31/ETTh2_96_96_test_3_parallel_trials/driver_artifacts`

Trial status: 3 PENDING
Current time: 2024-08-26 13:34:31. Total running time: 0s
Logical resource usage: 3.0/32 CPUs, 0.99/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-289c4_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-289c4_00001   PENDING               8       6.40637e-05         256              4         64            2   0.00128816   cosine  â”‚
â”‚ trial-289c4_00002   PENDING              16       0.00351285          512              2         64            4   0.00890605   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-289c4_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00891 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00351 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-289c4_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-289c4_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00129 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2802979)[0m configuration
[36m(_train_fn pid=2802979)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2802979)[0m Use GPU: cuda:0
[36m(_train_fn pid=2802981)[0m {'batch_size': 16, 'learning_rate': 0.0035128531928926765, 'd_model': 512, 'd_core': 64, 'e_layers': 4, 'dropout': 0.00890604745419256, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2802979)[0m train 8449
[36m(_train_fn pid=2802979)[0m val 2785
[36m(_train_fn pid=2802979)[0m start_epoch 0
[36m(_train_fn pid=2802979)[0m max_epoch 8
[36m(_train_fn pid=2802979)[0m 	iters: 100, epoch: 1 | loss: 0.5344234
[36m(_train_fn pid=2802979)[0m 	speed: 0.0113s/iter; left time: 22.7366s
[36m(_train_fn pid=2802979)[0m Updating learning rate to 0.000288581929876693
[36m(_train_fn pid=2802979)[0m saving checkpoint...
[36m(_train_fn pid=2802979)[0m Validation loss decreased (inf --> 0.2195).  Saving model state dict ...
[36m(_train_fn pid=2802979)[0m Epoch: 1 cost time: 1.839325189590454
[36m(_train_fn pid=2802979)[0m Epoch: 1, Steps: 265 | Train Loss: 0.5247267 Vali Loss: 0.2195345 Best vali loss: 0.2195345
[36m(_train_fn pid=2802979)[0m Updating learning rate to 0.00025606601717798207
[36m(_train_fn pid=2802979)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2802979)[0m saving checkpoint...
[36m(_train_fn pid=2802979)[0m Epoch: 2 cost time: 1.6471498012542725
[36m(_train_fn pid=2802979)[0m Epoch: 2, Steps: 265 | Train Loss: 0.4370105 Vali Loss: 0.2229692 Best vali loss: 0.2195345
[36m(_train_fn pid=2802980)[0m configuration[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
2024-08-26 13:34:39,136	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2802979)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00000_0_2024-08-26_13-34-31/checkpoint_000002)
2024-08-26 13:34:39,214	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:41,051	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2802979)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00000_0_2024-08-26_13-34-31/checkpoint_000003)[32m [repeated 2x across cluster][0m
2024-08-26 13:34:42,123	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:43,995	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:48,343	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:48,356	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2802981)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-34-31/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 13:34:48,668	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:52,606	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:53,515	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2803797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-34-41/checkpoint_000001)[32m [repeated 4x across cluster][0m
2024-08-26 13:34:56,079	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:56,739	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:34:58,923	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2803797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-34-41/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 13:35:01,114	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2802980)[0m {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2802980)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2802980)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2802980)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2802980)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2802980)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2802979)[0m Updating learning rate to 0.00020740251485476345
[36m(_train_fn pid=2802979)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2802979)[0m saving checkpoint...
[36m(_train_fn pid=2802979)[0m Epoch: 3 cost time: 1.4855713844299316
[36m(_train_fn pid=2802979)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3195181 Vali Loss: 0.2231508 Best vali loss: 0.2195345
[36m(_train_fn pid=2802980)[0m 	iters: 800, epoch: 1 | loss: 0.3678733[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2802980)[0m 	speed: 0.0053s/iter; left time: 40.2533s[32m [repeated 18x across cluster][0m

Trial trial-289c4_00000 completed after 4 iterations at 2024-08-26 13:34:41. Total running time: 9s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.91179 â”‚
â”‚ time_total_s                                 8.26129 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21953 â”‚
â”‚ train_loss                                   0.43367 â”‚
â”‚ valid_loss                                   0.23297 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2802979)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2802979)[0m Early stopping
[36m(_train_fn pid=2802979)[0m Updating learning rate to 0.00015[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2802979)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2802981)[0m Validation loss decreased (inf --> 0.2859).  Saving model state dict ...
[36m(_train_fn pid=2802979)[0m Epoch: 4 cost time: 1.5968153476715088[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2802979)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4336717 Vali Loss: 0.2329654 Best vali loss: 0.2195345[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2802980)[0m Validation loss decreased (inf --> 0.2160).  Saving model state dict ...

Trial trial-289c4_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00596 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00015 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2803797)[0m {'batch_size': 8, 'learning_rate': 0.00015448958497378217, 'd_model': 128, 'd_core': 512, 'e_layers': 4, 'dropout': 0.005958307406826554, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2803797)[0m configuration
[36m(_train_fn pid=2803797)[0m Use GPU: cuda:0
[36m(_train_fn pid=2803797)[0m train 8449
[36m(_train_fn pid=2803797)[0m val 2785
[36m(_train_fn pid=2803797)[0m start_epoch 0
[36m(_train_fn pid=2803797)[0m max_epoch 8
[36m(_train_fn pid=2802981)[0m Validation loss decreased (0.2859 --> 0.2783).  Saving model state dict ...
[36m(_train_fn pid=2803797)[0m 	iters: 300, epoch: 1 | loss: 0.2197938[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2803797)[0m 	speed: 0.0042s/iter; left time: 33.9509s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2802981)[0m Updating learning rate to 0.0017564265964463382[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2802981)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2802981)[0m Epoch: 2 cost time: 4.079469203948975[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2802981)[0m Epoch: 2, Steps: 529 | Train Loss: 0.4283475 Vali Loss: 0.2783087 Best vali loss: 0.2783087[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2802981)[0m Validation loss decreased (0.2783 --> 0.2703).  Saving model state dict ...
[36m(_train_fn pid=2803797)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...
[36m(_train_fn pid=2802980)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2802981)[0m 	iters: 200, epoch: 4 | loss: 0.3712242[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2802981)[0m 	speed: 0.0067s/iter; left time: 16.3272s[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2802980)[0m Updating learning rate to 5.4681791396423194e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802980)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802980)[0m Epoch: 2 cost time: 5.692073345184326[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802980)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.3143075 Vali Loss: 0.2210888 Best vali loss: 0.2159521[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802981)[0m Validation loss decreased (0.2703 --> 0.2689).  Saving model state dict ...
[36m(_train_fn pid=2803797)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2802981)[0m 	iters: 300, epoch: 5 | loss: 0.5293031[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2802981)[0m 	speed: 0.0070s/iter; left time: 12.7299s[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2802980)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2802980)[0m Updating learning rate to 4.428991077132631e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802980)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802980)[0m Epoch: 3 cost time: 6.098382949829102[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802980)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4252212 Vali Loss: 0.2197178 Best vali loss: 0.2159521[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802980)[0m 	iters: 700, epoch: 4 | loss: 0.9401140[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2802980)[0m 	speed: 0.0059s/iter; left time: 26.8633s[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2802981)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m

Trial status: 1 TERMINATED | 3 RUNNING
Current time: 2024-08-26 13:35:01. Total running time: 30s
Logical resource usage: 3.0/32 CPUs, 0.99/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 289c4_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
2024-08-26 13:35:02,739	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:04,262	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2803797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-34-41/checkpoint_000003)[32m [repeated 3x across cluster][0m
2024-08-26 13:35:04,355	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-289c4_00001   RUNNING                 8       6.40637e-05         256              4         64            2   0.00128816   cosine         3           23.2389        0.425221       0.219718            0.215952 â”‚
â”‚ trial-289c4_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          6           28.3145        0.359152       0.270665            0.26892  â”‚
â”‚ trial-289c4_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         3           16.3472        0.413446       0.223288            0.220544 â”‚
â”‚ trial-289c4_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            8.26129       0.433672       0.232965            0.219535 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2802981)[0m Updating learning rate to 0.00010977666227789614[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802981)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802981)[0m Epoch: 6 cost time: 3.724965810775757[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802981)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3591521 Vali Loss: 0.2706647 Best vali loss: 0.2689202[32m [repeated 3x across cluster][0m

Trial trial-289c4_00001 completed after 4 iterations at 2024-08-26 13:35:02. Total running time: 31s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             6.65706 â”‚
â”‚ time_total_s                                29.89595 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21595 â”‚
â”‚ train_loss                                   0.34303 â”‚
â”‚ valid_loss                                   0.22278 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2802980)[0m Early stopping

Trial trial-289c4_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00325 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00358 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2804790)[0m configuration
[36m(_train_fn pid=2804790)[0m {'batch_size': 8, 'learning_rate': 0.0035791880881606486, 'd_model': 256, 'd_core': 64, 'e_layers': 4, 'dropout': 0.003254613020686493, 'lradj': 'type1', 'd_ff': 768}

Trial trial-289c4_00003 completed after 4 iterations at 2024-08-26 13:35:04. Total running time: 33s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             5.33656 â”‚
â”‚ time_total_s                                21.68376 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22054 â”‚
â”‚ train_loss                                   0.24563 â”‚
â”‚ valid_loss                                   0.22743 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2804790)[0m Use GPU: cuda:0
[36m(_train_fn pid=2804790)[0m train 8449

Trial trial-289c4_00002 completed after 7 iterations at 2024-08-26 13:35:04. Total running time: 33s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                              3.2378 â”‚
â”‚ time_total_s                                 31.5523 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26892 â”‚
â”‚ train_loss                                    0.3598 â”‚
â”‚ valid_loss                                    0.2707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
2024-08-26 13:35:06,981	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:07,564	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:08,180	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:08,369	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:08,773	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:09,382	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2804935)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-35-04/checkpoint_000004)[32m [repeated 7x across cluster][0m
2024-08-26 13:35:09,972	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:10,297	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:10,594	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:11,156	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:11,388	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:12,164	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:14,007	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:14,399	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:15,450	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2805951)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00007_7_alpha_d_ff=2,batch_size=32,d_core=64,d_model=512,dropout=0.0058,e_layers=1,learning_rate=0.0027,lradj=type1_2024-08-26_13-35-11/checkpoint_000001)[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2804790)[0m val 2785
[36m(_train_fn pid=2804790)[0m start_epoch 0
[36m(_train_fn pid=2804790)[0m max_epoch 8
[36m(_train_fn pid=2804790)[0m 	iters: 100, epoch: 1 | loss: 1.3651259[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2804790)[0m 	speed: 0.0103s/iter; left time: 86.0685s[32m [repeated 19x across cluster][0m

Trial trial-289c4_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00132 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-289c4_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00435 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00044 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2802981)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802981)[0m Updating learning rate to 5.488833113894807e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802981)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802981)[0m Epoch: 7 cost time: 2.8766119480133057[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2802981)[0m Epoch: 7, Steps: 529 | Train Loss: 0.3598005 Vali Loss: 0.2706989 Best vali loss: 0.2689202[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2804935)[0m Validation loss decreased (inf --> 0.2578).  Saving model state dict ...
[36m(_train_fn pid=2804935)[0m Validation loss decreased (0.2578 --> 0.2387).  Saving model state dict ...
[36m(_train_fn pid=2802981)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2804935)[0m Validation loss decreased (0.2387 --> 0.2336).  Saving model state dict ...
[36m(_train_fn pid=2804935)[0m Validation loss decreased (0.2336 --> 0.2315).  Saving model state dict ...
[36m(_train_fn pid=2804939)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2804939)[0m {'batch_size': 16, 'learning_rate': 0.0004377603894965504, 'd_model': 32, 'd_core': 256, 'e_layers': 2, 'dropout': 0.00435286364938959, 'lradj': 'type1', 'd_ff': 32}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2804935)[0m Validation loss decreased (0.2315 --> 0.2305).  Saving model state dict ...
[36m(_train_fn pid=2804939)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2804939)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2804939)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2804939)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2804939)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2804935)[0m Validation loss decreased (0.2305 --> 0.2300).  Saving model state dict ...
[36m(_train_fn pid=2804935)[0m Validation loss decreased (0.2300 --> 0.2298).  Saving model state dict ...
[36m(_train_fn pid=2804935)[0m 	iters: 100, epoch: 7 | loss: 0.7282920[32m [repeated 26x across cluster][0m
[36m(_train_fn pid=2804935)[0m 	speed: 0.0062s/iter; left time: 1.0298s[32m [repeated 26x across cluster][0m
[36m(_train_fn pid=2804935)[0m Validation loss decreased (0.2298 --> 0.2297).  Saving model state dict ...

Trial trial-289c4_00005 completed after 8 iterations at 2024-08-26 13:35:11. Total running time: 39s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.55979 â”‚
â”‚ time_total_s                                 5.39134 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22969 â”‚
â”‚ train_loss                                   0.36879 â”‚
â”‚ valid_loss                                   0.22969 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2804939)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2804790)[0m Updating learning rate to 0.0035791880881606486[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2804790)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2804790)[0m Epoch: 1 cost time: 5.918479919433594[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2804790)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.5489847 Vali Loss: 0.2711374 Best vali loss: 0.2711374[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2804790)[0m Validation loss decreased (inf --> 0.2711).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-289c4_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00579 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00268 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2804939)[0m Validation loss decreased (0.2230 --> 0.2217).  Saving model state dict ...
[36m(_train_fn pid=2804939)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2805951)[0m configuration
[36m(_train_fn pid=2805951)[0m {'batch_size': 32, 'learning_rate': 0.002681331476927111, 'd_model': 512, 'd_core': 64, 'e_layers': 1, 'dropout': 0.005787899105404264, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2805951)[0m Use GPU: cuda:0
[36m(_train_fn pid=2805951)[0m train 8449
[36m(_train_fn pid=2805951)[0m val 2785
[36m(_train_fn pid=2805951)[0m start_epoch 0
[36m(_train_fn pid=2805951)[0m max_epoch 8
2024-08-26 13:35:15,916	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:16,554	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:17,607	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:17,853	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:18,770	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:18,853	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:19,999	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:22,186	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2806663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-35-17/checkpoint_000000)[32m [repeated 8x across cluster][0m
2024-08-26 13:35:23,529	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:24,252	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:24,787	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:25,995	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:26,292	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:26,801	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:27,238	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:28,329	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2806663)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-35-17/checkpoint_000003)[32m [repeated 8x across cluster][0m
2024-08-26 13:35:28,493	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2805951)[0m 	iters: 100, epoch: 3 | loss: 0.2991765[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=2805951)[0m 	speed: 0.0079s/iter; left time: 11.7803s[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=2805951)[0m Validation loss decreased (0.2288 --> 0.2243).  Saving model state dict ...
[36m(_train_fn pid=2805951)[0m Updating learning rate to 0.0006703328692317777[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2805951)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2805951)[0m Epoch: 3 cost time: 0.8703880310058594[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2805951)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3544751 Vali Loss: 0.2242911 Best vali loss: 0.2242911[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2805951)[0m Validation loss decreased (inf --> 0.2288).  Saving model state dict ...

Trial trial-289c4_00006 completed after 6 iterations at 2024-08-26 13:35:17. Total running time: 46s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.93499 â”‚
â”‚ time_total_s                                11.98577 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22172 â”‚
â”‚ train_loss                                   0.36204 â”‚
â”‚ valid_loss                                   0.22492 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2804939)[0m Early stopping
[36m(_train_fn pid=2805951)[0m EarlyStopping counter: 2 out of 3[32m [repeated 5x across cluster][0m

Trial trial-289c4_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00333 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00196 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2806663)[0m configuration
[36m(_train_fn pid=2806663)[0m {'batch_size': 32, 'learning_rate': 0.001963618480113192, 'd_model': 64, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0033338002527237612, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2806663)[0m Use GPU: cuda:0
[36m(_train_fn pid=2806663)[0m train 8449
[36m(_train_fn pid=2806663)[0m val 2785
[36m(_train_fn pid=2806663)[0m start_epoch 0
[36m(_train_fn pid=2806663)[0m max_epoch 8

Trial trial-289c4_00007 completed after 6 iterations at 2024-08-26 13:35:19. Total running time: 48s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.22712 â”‚
â”‚ time_total_s                                 7.32755 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22429 â”‚
â”‚ train_loss                                   0.37774 â”‚
â”‚ valid_loss                                   0.23335 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2804790)[0m 	iters: 300, epoch: 3 | loss: 0.2605102[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2804790)[0m 	speed: 0.0068s/iter; left time: 41.3782s[32m [repeated 19x across cluster][0m

Trial trial-289c4_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00128 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00073 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2806955)[0m {'batch_size': 32, 'learning_rate': 0.0007335788889365637, 'd_model': 32, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0012802554597501359, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2804790)[0m Validation loss decreased (0.2711 --> 0.2668).  Saving model state dict ...
[36m(_train_fn pid=2805951)[0m Updating learning rate to 8.379160865397222e-05[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2805951)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2805951)[0m Epoch: 6 cost time: 0.9034233093261719[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2805951)[0m Epoch: 6, Steps: 265 | Train Loss: 0.3777368 Vali Loss: 0.2333461 Best vali loss: 0.2242911[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2806663)[0m Validation loss decreased (inf --> 0.2248).  Saving model state dict ...
[36m(_train_fn pid=2805951)[0m Early stopping
[36m(_train_fn pid=2806663)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2806955)[0m Validation loss decreased (0.2216 --> 0.2208).  Saving model state dict ...
[36m(_train_fn pid=2806955)[0m configuration
[36m(_train_fn pid=2806955)[0m Use GPU: cuda:0
[36m(_train_fn pid=2806955)[0m train 8449
[36m(_train_fn pid=2806955)[0m val 2785
[36m(_train_fn pid=2806955)[0m start_epoch 0
[36m(_train_fn pid=2806955)[0m max_epoch 8
[36m(_train_fn pid=2806955)[0m 	iters: 200, epoch: 3 | loss: 0.2014026[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2806955)[0m 	speed: 0.0036s/iter; left time: 5.0496s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2806955)[0m Updating learning rate to 9.169736111707046e-05[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2806955)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2804790)[0m Epoch: 3 cost time: 7.037057399749756[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2804790)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4845674 Vali Loss: 0.2714944 Best vali loss: 0.2667734[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2806955)[0m Validation loss decreased (inf --> 0.2216).  Saving model state dict ...

Trial trial-289c4_00009 completed after 5 iterations at 2024-08-26 13:35:28. Total running time: 57s
2024-08-26 13:35:30,425	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:32,520	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:33,910	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2804790)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-35-02/checkpoint_000003)[32m [repeated 4x across cluster][0m
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.25349 â”‚
â”‚ time_total_s                                 6.98957 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22079 â”‚
â”‚ train_loss                                   0.31516 â”‚
â”‚ valid_loss                                   0.22207 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2806955)[0m Early stopping
[36m(_train_fn pid=2806955)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2806663)[0m Validation loss decreased (0.2248 --> 0.2229).  Saving model state dict ...

Trial status: 8 TERMINATED | 2 RUNNING
Current time: 2024-08-26 13:35:31. Total running time: 1min 0s
Logical resource usage: 2.0/32 CPUs, 0.66/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 289c4_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-289c4_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          3           22.5728        0.484567       0.271494            0.266773 â”‚
â”‚ trial-289c4_00008   RUNNING                32       0.00196362           64              2        512            4   0.0033338    cosine         5           11.0509        0.236268       0.230179            0.222929 â”‚
â”‚ trial-289c4_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            8.26129       0.433672       0.232965            0.219535 â”‚
â”‚ trial-289c4_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           29.8959        0.343032       0.222783            0.215952 â”‚
â”‚ trial-289c4_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           31.5523        0.359801       0.270699            0.26892  â”‚
â”‚ trial-289c4_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           21.6838        0.245632       0.227429            0.220544 â”‚
â”‚ trial-289c4_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            5.39134       0.368788       0.229693            0.229693 â”‚
â”‚ trial-289c4_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           11.9858        0.362042       0.224921            0.22172  â”‚
â”‚ trial-289c4_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6            7.32755       0.377737       0.233346            0.224291 â”‚
â”‚ trial-289c4_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            6.98957       0.315164       0.222069            0.220791 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2806663)[0m 	iters: 200, epoch: 6 | loss: 0.2011863[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2806663)[0m 	speed: 0.0067s/iter; left time: 4.0108s[32m [repeated 17x across cluster][0m

Trial trial-289c4_00008 completed after 6 iterations at 2024-08-26 13:35:32. Total running time: 1min 1s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             2.09271 â”‚
â”‚ time_total_s                                13.14359 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22293 â”‚
â”‚ train_loss                                   0.20661 â”‚
â”‚ valid_loss                                   0.24217 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2806663)[0m Updating learning rate to 0.00028756526858096616[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2806663)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2806663)[0m Epoch: 6 cost time: 1.8261332511901855[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2806663)[0m Epoch: 6, Steps: 265 | Train Loss: 0.2066133 Vali Loss: 0.2421666 Best vali loss: 0.2229292[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2806663)[0m Early stopping
[36m(_train_fn pid=2804790)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m
2024-08-26 13:35:39,230	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials' in 0.0022s.
[36m(_train_fn pid=2804790)[0m 	iters: 700, epoch: 5 | loss: 0.3472791[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2804790)[0m 	speed: 0.0044s/iter; left time: 15.4237s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2804790)[0m Updating learning rate to 0.00044739851102008107
[36m(_train_fn pid=2804790)[0m saving checkpoint...
[36m(_train_fn pid=2804790)[0m Epoch: 4 cost time: 6.470044851303101
[36m(_train_fn pid=2804790)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.6146849 Vali Loss: 0.2705928 Best vali loss: 0.2667734

Trial trial-289c4_00004 completed after 5 iterations at 2024-08-26 13:35:39. Total running time: 1min 7s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-289c4_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.31363 â”‚
â”‚ time_total_s                                34.99321 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.26677 â”‚
â”‚ train_loss                                   0.43419 â”‚
â”‚ valid_loss                                   0.27142 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:35:39. Total running time: 1min 7s
Logical resource usage: 1.0/32 CPUs, 0.33/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 289c4_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-289c4_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            8.26129       0.433672       0.232965            0.219535 â”‚
â”‚ trial-289c4_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           29.8959        0.343032       0.222783            0.215952 â”‚
â”‚ trial-289c4_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           31.5523        0.359801       0.270699            0.26892  â”‚
â”‚ trial-289c4_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           21.6838        0.245632       0.227429            0.220544 â”‚
â”‚ trial-289c4_00004   TERMINATED              8       0.00357919          256              3         64            4   0.00325461   type1          5           34.9932        0.434185       0.271418            0.266773 â”‚
â”‚ trial-289c4_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            5.39134       0.368788       0.229693            0.229693 â”‚
â”‚ trial-289c4_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           11.9858        0.362042       0.224921            0.22172  â”‚
â”‚ trial-289c4_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6            7.32755       0.377737       0.233346            0.224291 â”‚
â”‚ trial-289c4_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6           13.1436        0.206613       0.242167            0.222929 â”‚
â”‚ trial-289c4_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            6.98957       0.315164       0.222069            0.220791 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2804790)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_3_parallel_trials/trial-289c4_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-35-02/checkpoint_000004)
[36m(_train_fn pid=2804790)[0m Updating learning rate to 0.00022369925551004054
[36m(_train_fn pid=2804790)[0m saving checkpoint...
[36m(_train_fn pid=2804790)[0m Epoch: 5 cost time: 4.679598808288574
[36m(_train_fn pid=2804790)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.4341850 Vali Loss: 0.2714180 Best vali loss: 0.2667734
[36m(_train_fn pid=2804790)[0m Early stopping
[36m(_train_fn pid=2804790)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2804790)[0m 	iters: 1000, epoch: 5 | loss: 0.3214265[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2804790)[0m 	speed: 0.0043s/iter; left time: 14.0444s[32m [repeated 3x across cluster][0m


Time taken (3 parallel trials): 72 seconds


2024-08-26 13:35:43,202	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:35:43,560	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:35:43,565	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:35:43,573	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:35:48,090	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_4_parallel_trials   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator                â”‚
â”‚ Scheduler                        FIFOScheduler                        â”‚
â”‚ Number of trials                 10                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-35-42_771087_2808073/artifacts/2024-08-26_13-35-43/ETTh2_96_96_test_4_parallel_trials/driver_artifacts`

Trial status: 4 PENDING
Current time: 2024-08-26 13:35:43. Total running time: 0s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-53bb4_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-53bb4_00001   PENDING               8       6.40637e-05         256              4         64            2   0.00128816   cosine  â”‚
â”‚ trial-53bb4_00002   PENDING              16       0.00351285          512              2         64            4   0.00890605   type1   â”‚
â”‚ trial-53bb4_00003   PENDING               8       0.00015449          128              1        512            4   0.00595831   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-53bb4_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00129 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-53bb4_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00891 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00351 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-53bb4_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-53bb4_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00596 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00015 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2810297)[0m configuration
[36m(_train_fn pid=2810297)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2810297)[0m Use GPU: cuda:0
[36m(_train_fn pid=2810299)[0m {'batch_size': 16, 'learning_rate': 0.0035128531928926765, 'd_model': 512, 'd_core': 64, 'e_layers': 4, 'dropout': 0.00890604745419256, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2810297)[0m train 8449
[36m(_train_fn pid=2810298)[0m val 2785
[36m(_train_fn pid=2810297)[0m start_epoch 0
[36m(_train_fn pid=2810297)[0m max_epoch 8
[36m(_train_fn pid=2810297)[0m 	iters: 100, epoch: 1 | loss: 0.5344234
[36m(_train_fn pid=2810297)[0m 	speed: 0.0126s/iter; left time: 25.4100s
[36m(_train_fn pid=2810297)[0m Updating learning rate to 0.000288581929876693
[36m(_train_fn pid=2810297)[0m saving checkpoint...
[36m(_train_fn pid=2810297)[0m Validation loss decreased (inf --> 0.2195).  Saving model state dict ...
[36m(_train_fn pid=2810297)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00000_0_2024-08-26_13-35-43/checkpoint_000000)
2024-08-26 13:35:50,300	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2810297)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00000_0_2024-08-26_13-35-43/checkpoint_000001)
2024-08-26 13:35:52,490	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2810297)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00000_0_2024-08-26_13-35-43/checkpoint_000002)
2024-08-26 13:35:53,159	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2810299)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-35-43/checkpoint_000000)
2024-08-26 13:35:54,513	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2810297)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00000_0_2024-08-26_13-35-43/checkpoint_000003)
2024-08-26 13:35:56,358	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:58,888	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:35:58,897	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:02,222	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2811196)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-35-54/checkpoint_000000)[32m [repeated 4x across cluster][0m
2024-08-26 13:36:04,658	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:05,397	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:07,864	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2811196)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-35-54/checkpoint_000001)[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2810297)[0m Epoch: 1 cost time: 2.1322574615478516
[36m(_train_fn pid=2810297)[0m Epoch: 1, Steps: 265 | Train Loss: 0.5247267 Vali Loss: 0.2195345 Best vali loss: 0.2195345
[36m(_train_fn pid=2810297)[0m Updating learning rate to 0.00025606601717798207
[36m(_train_fn pid=2810297)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2810297)[0m saving checkpoint...
[36m(_train_fn pid=2810297)[0m Epoch: 2 cost time: 1.8634698390960693
[36m(_train_fn pid=2810297)[0m Epoch: 2, Steps: 265 | Train Loss: 0.4370105 Vali Loss: 0.2229692 Best vali loss: 0.2195345
[36m(_train_fn pid=2810299)[0m configuration[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2810300)[0m {'batch_size': 8, 'learning_rate': 0.00015448958497378217, 'd_model': 128, 'd_core': 512, 'e_layers': 4, 'dropout': 0.005958307406826554, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2810299)[0m Use GPU: cuda:0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2810299)[0m train 8449[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2810299)[0m val 2785[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2810300)[0m start_epoch 0[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2810300)[0m max_epoch 8[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2810297)[0m 	iters: 200, epoch: 3 | loss: 0.3384072[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2810297)[0m 	speed: 0.0068s/iter; left time: 9.4037s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2810297)[0m Updating learning rate to 0.00020740251485476345
[36m(_train_fn pid=2810297)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2810297)[0m saving checkpoint...
[36m(_train_fn pid=2810297)[0m Epoch: 3 cost time: 1.8398137092590332
[36m(_train_fn pid=2810297)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3195181 Vali Loss: 0.2231508 Best vali loss: 0.2195345
[36m(_train_fn pid=2810299)[0m Updating learning rate to 0.0035128531928926765
[36m(_train_fn pid=2810299)[0m saving checkpoint...
[36m(_train_fn pid=2810299)[0m Validation loss decreased (inf --> 0.2859).  Saving model state dict ...
[36m(_train_fn pid=2810299)[0m Epoch: 1 cost time: 6.546612501144409
[36m(_train_fn pid=2810299)[0m Epoch: 1, Steps: 529 | Train Loss: 0.7621403 Vali Loss: 0.2858527 Best vali loss: 0.2858527

Trial trial-53bb4_00000 completed after 4 iterations at 2024-08-26 13:35:54. Total running time: 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.02895 â”‚
â”‚ time_total_s                                 9.37033 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21953 â”‚
â”‚ train_loss                                   0.43367 â”‚
â”‚ valid_loss                                   0.23297 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2810297)[0m Updating learning rate to 0.00015
[36m(_train_fn pid=2810297)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2810297)[0m saving checkpoint...
[36m(_train_fn pid=2810297)[0m Epoch: 4 cost time: 1.6617870330810547
[36m(_train_fn pid=2810297)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4336717 Vali Loss: 0.2329654 Best vali loss: 0.2195345
[36m(_train_fn pid=2810297)[0m Early stopping

Trial trial-53bb4_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00325 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00358 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2811196)[0m configuration
[36m(_train_fn pid=2811196)[0m {'batch_size': 8, 'learning_rate': 0.0035791880881606486, 'd_model': 256, 'd_core': 64, 'e_layers': 4, 'dropout': 0.003254613020686493, 'lradj': 'type1', 'd_ff': 768}
[36m(_train_fn pid=2811196)[0m Use GPU: cuda:0
[36m(_train_fn pid=2811196)[0m train 8449
[36m(_train_fn pid=2811196)[0m val 2785
[36m(_train_fn pid=2810298)[0m Validation loss decreased (inf --> 0.2160).  Saving model state dict ...
[36m(_train_fn pid=2811196)[0m start_epoch 0
[36m(_train_fn pid=2811196)[0m max_epoch 8
[36m(_train_fn pid=2810299)[0m 	iters: 400, epoch: 2 | loss: 0.5075887[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2810299)[0m 	speed: 0.0088s/iter; left time: 28.9452s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2810299)[0m Validation loss decreased (0.2859 --> 0.2783).  Saving model state dict ...
[36m(_train_fn pid=2810299)[0m Updating learning rate to 0.0017564265964463382[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2810299)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2810300)[0m Epoch: 1 cost time: 11.69923734664917[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2810300)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.4099332 Vali Loss: 0.2205494 Best vali loss: 0.2205494[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2810300)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...
[36m(_train_fn pid=2811196)[0m Validation loss decreased (inf --> 0.2776).  Saving model state dict ...
[36m(_train_fn pid=2810298)[0m 	iters: 800, epoch: 2 | loss: 0.1914457[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2810298)[0m 	speed: 0.0076s/iter; left time: 50.2681s[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2810299)[0m Validation loss decreased (0.2783 --> 0.2703).  Saving model state dict ...
[36m(_train_fn pid=2810299)[0m Updating learning rate to 0.0008782132982231691[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2810299)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2810299)[0m Epoch: 3 cost time: 5.031835556030273[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2810299)[0m Epoch: 3, Steps: 529 | Train Loss: 0.6474589 Vali Loss: 0.2703472 Best vali loss: 0.2703472[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2810298)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2810299)[0m 	iters: 300, epoch: 4 | loss: 0.2992301[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2810299)[0m 	speed: 0.0093s/iter; left time: 21.8994s[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2811196)[0m Validation loss decreased (0.2776 --> 0.2673).  Saving model state dict ...
[36m(_train_fn pid=2810299)[0m Validation loss decreased (0.2703 --> 0.2689).  Saving model state dict ...
[36m(_train_fn pid=2810299)[0m Updating learning rate to 0.00043910664911158456[32m [repeated 3x across cluster][0m
2024-08-26 13:36:10,291	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:10,322	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:13,493	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2811196)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-35-54/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 13:36:14,541	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:16,012	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:19,099	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2811196)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-35-54/checkpoint_000003)[32m [repeated 3x across cluster][0m
2024-08-26 13:36:21,859	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:21,977	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:23,322	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:24,659	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2810299)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2811196)[0m Epoch: 2 cost time: 4.9768476486206055[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2811196)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.5843853 Vali Loss: 0.2672879 Best vali loss: 0.2672879[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2810300)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2810298)[0m 	iters: 1000, epoch: 3 | loss: 0.3227447[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=2810298)[0m 	speed: 0.0072s/iter; left time: 38.4852s[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=2811196)[0m EarlyStopping counter: 1 out of 3

Trial status: 1 TERMINATED | 4 RUNNING
Current time: 2024-08-26 13:36:13. Total running time: 30s
Logical resource usage: 4.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 53bb4_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-53bb4_00001   RUNNING                 8       6.40637e-05         256              4         64            2   0.00128816   cosine         2           20.2899        0.314308       0.221089            0.215952 â”‚
â”‚ trial-53bb4_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          4           25.1455        0.409842       0.26892             0.26892  â”‚
â”‚ trial-53bb4_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         2           25.1756        0.287397       0.224377            0.220549 â”‚
â”‚ trial-53bb4_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          3           17.4858        0.48428        0.271376            0.267288 â”‚
â”‚ trial-53bb4_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            9.37033       0.433672       0.232965            0.219535 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2810298)[0m Updating learning rate to 4.428991077132631e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2810298)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2810298)[0m Epoch: 3 cost time: 7.503875017166138[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2810298)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4252212 Vali Loss: 0.2197178 Best vali loss: 0.2159521[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2811196)[0m 	iters: 900, epoch: 4 | loss: 1.6203796[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2811196)[0m 	speed: 0.0045s/iter; left time: 19.9384s[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2810299)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2811196)[0m Updating learning rate to 0.00044739851102008107[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2811196)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2811196)[0m Epoch: 4 cost time: 4.961594581604004[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2811196)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.6166318 Vali Loss: 0.2703438 Best vali loss: 0.2672879[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2810300)[0m 	iters: 100, epoch: 4 | loss: 0.2328789[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2810300)[0m 	speed: 0.0301s/iter; left time: 156.0662s[32m [repeated 23x across cluster][0m

Trial trial-53bb4_00001 completed after 4 iterations at 2024-08-26 13:36:23. Total running time: 39s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             8.77864 â”‚
â”‚ time_total_s                                 38.2094 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21595 â”‚
â”‚ train_loss                                   0.34303 â”‚
â”‚ valid_loss                                   0.22278 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2810298)[0m Early stopping
[36m(_train_fn pid=2810298)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m

Trial trial-53bb4_00004 completed after 5 iterations at 2024-08-26 13:36:24. Total running time: 41s
[36m(_train_fn pid=2811196)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-35-54/checkpoint_000004)[32m [repeated 4x across cluster][0m
2024-08-26 13:36:26,337	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:26,678	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:27,006	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:27,628	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:28,233	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:28,877	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:29,521	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:29,984	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2812913)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00007_7_alpha_d_ff=2,batch_size=32,d_core=64,d_model=512,dropout=0.0058,e_layers=1,learning_rate=0.0027,lradj=type1_2024-08-26_13-36-26/checkpoint_000000)[32m [repeated 8x across cluster][0m
2024-08-26 13:36:30,167	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:30,174	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:30,828	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.55778 â”‚
â”‚ time_total_s                                28.64702 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.26729 â”‚
â”‚ train_loss                                   0.43398 â”‚
â”‚ valid_loss                                   0.27155 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-53bb4_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00132 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2812539)[0m configuration
[36m(_train_fn pid=2812539)[0m {'batch_size': 64, 'learning_rate': 0.00010734405345907071, 'd_model': 64, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0013179176757202608, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=2812539)[0m Use GPU: cuda:0
[36m(_train_fn pid=2812539)[0m train 8449
[36m(_train_fn pid=2812539)[0m val 2785
[36m(_train_fn pid=2812539)[0m start_epoch 0
[36m(_train_fn pid=2812539)[0m max_epoch 8
[36m(_train_fn pid=2811196)[0m Updating learning rate to 0.00022369925551004054[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2811196)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2811196)[0m Epoch: 5 cost time: 4.897154808044434[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2811196)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.4339838 Vali Loss: 0.2715512 Best vali loss: 0.2672879[32m [repeated 4x across cluster][0m

Trial trial-53bb4_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00435 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00044 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2812539)[0m Validation loss decreased (inf --> 0.2578).  Saving model state dict ...

Trial trial-53bb4_00002 completed after 7 iterations at 2024-08-26 13:36:26. Total running time: 43s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             4.81581 â”‚
â”‚ time_total_s                                41.52054 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26892 â”‚
â”‚ train_loss                                    0.3598 â”‚
â”‚ valid_loss                                    0.2707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2812539)[0m Validation loss decreased (0.2578 --> 0.2387).  Saving model state dict ...
[36m(_train_fn pid=2812539)[0m Validation loss decreased (0.2387 --> 0.2336).  Saving model state dict ...
[36m(_train_fn pid=2812539)[0m 	iters: 100, epoch: 4 | loss: 0.3614123[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2812539)[0m 	speed: 0.0063s/iter; left time: 3.5514s[32m [repeated 18x across cluster][0m

Trial trial-53bb4_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00579 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00268 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2812539)[0m Validation loss decreased (0.2336 --> 0.2315).  Saving model state dict ...
[36m(_train_fn pid=2810299)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2810299)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2812539)[0m Validation loss decreased (0.2315 --> 0.2305).  Saving model state dict ...
[36m(_train_fn pid=2812539)[0m Validation loss decreased (0.2305 --> 0.2300).  Saving model state dict ...
[36m(_train_fn pid=2812913)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2812913)[0m {'batch_size': 32, 'learning_rate': 0.002681331476927111, 'd_model': 512, 'd_core': 64, 'e_layers': 1, 'dropout': 0.005787899105404264, 'lradj': 'type1', 'd_ff': 1024}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2812913)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2812539)[0m Validation loss decreased (0.2300 --> 0.2298).  Saving model state dict ...
[36m(_train_fn pid=2812913)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2812913)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2812913)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2812913)[0m max_epoch 8[32m [repeated 2x across cluster][0m

Trial trial-53bb4_00005 completed after 8 iterations at 2024-08-26 13:36:30. Total running time: 47s
2024-08-26 13:36:30,996	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:31,087	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:31,987	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:32,346	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:32,998	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:35,192	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2813777)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-36-31/checkpoint_000000)[32m [repeated 9x across cluster][0m
2024-08-26 13:36:35,343	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:35,361	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:37,038	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:37,568	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:38,869	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:39,012	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:39,738	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.65127 â”‚
â”‚ time_total_s                                 5.95769 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22969 â”‚
â”‚ train_loss                                   0.36879 â”‚
â”‚ valid_loss                                   0.22969 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2812539)[0m Validation loss decreased (0.2298 --> 0.2297).  Saving model state dict ...
[36m(_train_fn pid=2812539)[0m Updating learning rate to 8.3862541764899e-07[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2812539)[0m saving checkpoint...[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2812539)[0m Epoch: 8 cost time: 0.5045125484466553[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2812539)[0m Epoch: 8, Steps: 133 | Train Loss: 0.3687880 Vali Loss: 0.2296926 Best vali loss: 0.2296926[32m [repeated 11x across cluster][0m

Trial trial-53bb4_00003 completed after 4 iterations at 2024-08-26 13:36:31. Total running time: 47s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             9.10773 â”‚
â”‚ time_total_s                                 45.9359 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22055 â”‚
â”‚ train_loss                                   0.24506 â”‚
â”‚ valid_loss                                   0.22707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2812637)[0m Validation loss decreased (inf --> 0.2230).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial trial-53bb4_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00333 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00196 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2813735)[0m {'batch_size': 32, 'learning_rate': 0.001963618480113192, 'd_model': 64, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0033338002527237612, 'lradj': 'cosine', 'd_ff': 128}

Trial trial-53bb4_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00128 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00073 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-53bb4_00007 completed after 4 iterations at 2024-08-26 13:36:32. Total running time: 49s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.00882 â”‚
â”‚ time_total_s                                 4.73254 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22575 â”‚
â”‚ train_loss                                   0.20782 â”‚
â”‚ valid_loss                                   0.25053 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2812637)[0m 	iters: 200, epoch: 3 | loss: 0.3344912[32m [repeated 26x across cluster][0m
[36m(_train_fn pid=2812637)[0m 	speed: 0.0032s/iter; left time: 9.5336s[32m [repeated 26x across cluster][0m
[36m(_train_fn pid=2812913)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2812913)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2813777)[0m configuration[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2813777)[0m {'batch_size': 32, 'learning_rate': 0.0007335788889365637, 'd_model': 32, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0012802554597501359, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2813777)[0m Use GPU: cuda:0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2813777)[0m train 8449[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2813777)[0m val 2785[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2812637)[0m Validation loss decreased (0.2230 --> 0.2218).  Saving model state dict ...
[36m(_train_fn pid=2813777)[0m start_epoch 0[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2813777)[0m max_epoch 8[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2813735)[0m Updating learning rate to 0.0018888827017753457[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2813735)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2813735)[0m Epoch: 1 cost time: 2.234088659286499[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2813735)[0m Epoch: 1, Steps: 265 | Train Loss: 0.5137684 Vali Loss: 0.2248191 Best vali loss: 0.2248191[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2813735)[0m Validation loss decreased (inf --> 0.2248).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2812637)[0m 	iters: 500, epoch: 4 | loss: 0.1709913[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2812637)[0m 	speed: 0.0061s/iter; left time: 13.0231s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2813777)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
2024-08-26 13:36:40,737	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2813777)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-36-31/checkpoint_000003)[32m [repeated 8x across cluster][0m
2024-08-26 13:36:41,919	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:42,559	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:42,597	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:43,724	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:45,237	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:36:45,527	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2813735)[0m Validation loss decreased (0.2248 --> 0.2229).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2813777)[0m Updating learning rate to 9.169736111707046e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2813777)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2813777)[0m Epoch: 4 cost time: 1.5908236503601074[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2813777)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4333930 Vali Loss: 0.2220646 Best vali loss: 0.2207892[32m [repeated 6x across cluster][0m

Trial trial-53bb4_00009 completed after 5 iterations at 2024-08-26 13:36:42. Total running time: 58s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.82055 â”‚
â”‚ time_total_s                                 9.90826 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22079 â”‚
â”‚ train_loss                                   0.31555 â”‚
â”‚ valid_loss                                   0.22202 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2813777)[0m Early stopping
[36m(_train_fn pid=2812637)[0m 	iters: 200, epoch: 6 | loss: 0.5542545[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2812637)[0m 	speed: 0.0043s/iter; left time: 5.9785s[32m [repeated 18x across cluster][0m

Trial status: 8 TERMINATED | 2 RUNNING
Current time: 2024-08-26 13:36:43. Total running time: 1min 0s
Logical resource usage: 2.0/32 CPUs, 0.5/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 53bb4_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-53bb4_00006   RUNNING                16       0.00043776           32              1        256            2   0.00435286   type1          5           16.4063        0.329406       0.224541            0.22175  â”‚
â”‚ trial-53bb4_00008   RUNNING                32       0.00196362           64              2        512            4   0.0033338    cosine         5           11.2865        0.236268       0.230179            0.222929 â”‚
â”‚ trial-53bb4_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            9.37033       0.433672       0.232965            0.219535 â”‚
â”‚ trial-53bb4_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           38.2094        0.343032       0.222783            0.215952 â”‚
â”‚ trial-53bb4_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           41.5205        0.359801       0.270699            0.26892  â”‚
â”‚ trial-53bb4_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           45.9359        0.245055       0.22707             0.220549 â”‚
â”‚ trial-53bb4_00004   TERMINATED              8       0.00357919          256              3         64            4   0.00325461   type1          5           28.647         0.433984       0.271551            0.267288 â”‚
â”‚ trial-53bb4_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            5.95769       0.368788       0.229693            0.229693 â”‚
â”‚ trial-53bb4_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          4            4.73254       0.207822       0.250534            0.22575  â”‚
â”‚ trial-53bb4_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            9.90826       0.315546       0.222023            0.220789 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2813735)[0m EarlyStopping counter: 2 out of 3[32m [repeated 6x across cluster][0m

Trial trial-53bb4_00006 completed after 6 iterations at 2024-08-26 13:36:45. Total running time: 1min 1s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             2.63709 â”‚
â”‚ time_total_s                                19.04336 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22175 â”‚
â”‚ train_loss                                   0.36244 â”‚
â”‚ valid_loss                                   0.22489 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-53bb4_00008 completed after 6 iterations at 2024-08-26 13:36:45. Total running time: 1min 1s
2024-08-26 13:36:45,537	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials' in 0.0088s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-53bb4_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.80124 â”‚
â”‚ time_total_s                                13.08779 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22293 â”‚
â”‚ train_loss                                   0.20661 â”‚
â”‚ valid_loss                                   0.24217 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:36:45. Total running time: 1min 1s
Logical resource usage: 1.0/32 CPUs, 0.25/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 53bb4_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-53bb4_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            9.37033       0.433672       0.232965            0.219535 â”‚
â”‚ trial-53bb4_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           38.2094        0.343032       0.222783            0.215952 â”‚
â”‚ trial-53bb4_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           41.5205        0.359801       0.270699            0.26892  â”‚
â”‚ trial-53bb4_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           45.9359        0.245055       0.22707             0.220549 â”‚
â”‚ trial-53bb4_00004   TERMINATED              8       0.00357919          256              3         64            4   0.00325461   type1          5           28.647         0.433984       0.271551            0.267288 â”‚
â”‚ trial-53bb4_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            5.95769       0.368788       0.229693            0.229693 â”‚
â”‚ trial-53bb4_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           19.0434        0.36244        0.224893            0.22175  â”‚
â”‚ trial-53bb4_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          4            4.73254       0.207822       0.250534            0.22575  â”‚
â”‚ trial-53bb4_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6           13.0878        0.206613       0.242167            0.222929 â”‚
â”‚ trial-53bb4_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            9.90826       0.315546       0.222023            0.220789 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2813735)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_4_parallel_trials/trial-53bb4_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-36-30/checkpoint_000005)[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2813735)[0m Updating learning rate to 0.00028756526858096616[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2813735)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2813735)[0m Epoch: 6 cost time: 1.6023683547973633[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2813735)[0m Epoch: 6, Steps: 265 | Train Loss: 0.2066133 Vali Loss: 0.2421666 Best vali loss: 0.2229292[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2813735)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2813735)[0m 	iters: 200, epoch: 6 | loss: 0.2011863[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2813735)[0m 	speed: 0.0059s/iter; left time: 3.5318s[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2813735)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m


Time taken (4 parallel trials): 66 seconds


2024-08-26 13:36:49,335	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:36:49,714	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:36:49,719	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:36:49,726	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_5_parallel_trials   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator                â”‚
â”‚ Scheduler                        FIFOScheduler                        â”‚
â”‚ Number of trials                 10                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-36-48_900130_2815220/artifacts/2024-08-26_13-36-49/ETTh2_96_96_test_5_parallel_trials/driver_artifacts`

Trial status: 5 PENDING
Current time: 2024-08-26 13:36:49. Total running time: 0s
Logical resource usage: 5.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-7b25c_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-7b25c_00001   PENDING               8       6.40637e-05         256              4         64            2   0.00128816   cosine  â”‚
â”‚ trial-7b25c_00002   PENDING              16       0.00351285          512              2         64            4   0.00890605   type1   â”‚
â”‚ trial-7b25c_00003   PENDING               8       0.00015449          128              1        512            4   0.00595831   cosine  â”‚
â”‚ trial-7b25c_00004   PENDING               8       0.00357919          256              3         64            4   0.00325461   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-7b25c_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-7b25c_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00596 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00015 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-7b25c_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00325 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00358 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2817455)[0m configuration
[36m(_train_fn pid=2817455)[0m {'batch_size': 8, 'learning_rate': 0.0035791880881606486, 'd_model': 256, 'd_core': 64, 'e_layers': 4, 'dropout': 0.003254613020686493, 'lradj': 'type1', 'd_ff': 768}
[36m(_train_fn pid=2817455)[0m Use GPU: cuda:0
[36m(_train_fn pid=2817454)[0m {'batch_size': 8, 'learning_rate': 0.00015448958497378217, 'd_model': 128, 'd_core': 512, 'e_layers': 4, 'dropout': 0.005958307406826554, 'lradj': 'cosine', 'd_ff': 128}

Trial trial-7b25c_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00891 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00351 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-7b25c_00001 started with configuration:
2024-08-26 13:36:54,510	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2817451)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00000_0_2024-08-26_13-36-49/checkpoint_000000)
2024-08-26 13:36:56,995	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2817451)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00000_0_2024-08-26_13-36-49/checkpoint_000001)
2024-08-26 13:36:59,495	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2817451)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00000_0_2024-08-26_13-36-49/checkpoint_000002)
2024-08-26 13:37:01,464	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2817453)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-36-49/checkpoint_000000)
2024-08-26 13:37:01,842	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2817451)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00000_0_2024-08-26_13-36-49/checkpoint_000003)
2024-08-26 13:37:04,592	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:05,101	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:05,163	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:05,653	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:06,175	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:06,709	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:07,252	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00129 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2817451)[0m train 8449
[36m(_train_fn pid=2817453)[0m val 2785
[36m(_train_fn pid=2817453)[0m start_epoch 0
[36m(_train_fn pid=2817453)[0m max_epoch 8
[36m(_train_fn pid=2817451)[0m 	iters: 100, epoch: 1 | loss: 0.5344234
[36m(_train_fn pid=2817451)[0m 	speed: 0.0135s/iter; left time: 27.2844s
[36m(_train_fn pid=2817451)[0m Updating learning rate to 0.000288581929876693
[36m(_train_fn pid=2817451)[0m saving checkpoint...
[36m(_train_fn pid=2817451)[0m Validation loss decreased (inf --> 0.2195).  Saving model state dict ...
[36m(_train_fn pid=2817451)[0m Epoch: 1 cost time: 2.319106340408325
[36m(_train_fn pid=2817451)[0m Epoch: 1, Steps: 265 | Train Loss: 0.5247267 Vali Loss: 0.2195345 Best vali loss: 0.2195345
[36m(_train_fn pid=2817452)[0m configuration[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2817453)[0m {'batch_size': 16, 'learning_rate': 0.0035128531928926765, 'd_model': 512, 'd_core': 64, 'e_layers': 4, 'dropout': 0.00890604745419256, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2817452)[0m Use GPU: cuda:0[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2817452)[0m {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2817452)[0m train 8449[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2817452)[0m val 2785[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2817451)[0m Updating learning rate to 0.00025606601717798207
[36m(_train_fn pid=2817451)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2817451)[0m saving checkpoint...
[36m(_train_fn pid=2817451)[0m Epoch: 2 cost time: 2.080615282058716
[36m(_train_fn pid=2817451)[0m Epoch: 2, Steps: 265 | Train Loss: 0.4370105 Vali Loss: 0.2229692 Best vali loss: 0.2195345
[36m(_train_fn pid=2817452)[0m start_epoch 0[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2817452)[0m max_epoch 8[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2817454)[0m 	iters: 400, epoch: 1 | loss: 0.1219476[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2817454)[0m 	speed: 0.0148s/iter; left time: 119.4902s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2817451)[0m Updating learning rate to 0.00020740251485476345
[36m(_train_fn pid=2817451)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2817451)[0m saving checkpoint...
[36m(_train_fn pid=2817451)[0m Epoch: 3 cost time: 2.091038227081299
[36m(_train_fn pid=2817451)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3195181 Vali Loss: 0.2231508 Best vali loss: 0.2195345
[36m(_train_fn pid=2817453)[0m Updating learning rate to 0.0035128531928926765
[36m(_train_fn pid=2817453)[0m saving checkpoint...
[36m(_train_fn pid=2817453)[0m Validation loss decreased (inf --> 0.2859).  Saving model state dict ...
[36m(_train_fn pid=2817453)[0m Epoch: 1 cost time: 8.327068567276001
[36m(_train_fn pid=2817453)[0m Epoch: 1, Steps: 529 | Train Loss: 0.7621403 Vali Loss: 0.2858527 Best vali loss: 0.2858527

Trial trial-7b25c_00000 completed after 4 iterations at 2024-08-26 13:37:01. Total running time: 12s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.34435 â”‚
â”‚ time_total_s                                10.54003 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21953 â”‚
â”‚ train_loss                                   0.43367 â”‚
â”‚ valid_loss                                   0.23297 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2817451)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2817451)[0m Early stopping

Trial trial-7b25c_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00132 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2818476)[0m configuration
[36m(_train_fn pid=2818476)[0m {'batch_size': 64, 'learning_rate': 0.00010734405345907071, 'd_model': 64, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0013179176757202608, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=2818476)[0m Use GPU: cuda:0
[36m(_train_fn pid=2817453)[0m 	iters: 100, epoch: 2 | loss: 0.4133271[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2817453)[0m 	speed: 0.0309s/iter; left time: 111.5301s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2818476)[0m train 8449
[36m(_train_fn pid=2818476)[0m val 2785
[36m(_train_fn pid=2818476)[0m start_epoch 0
[36m(_train_fn pid=2818476)[0m max_epoch 8
[36m(_train_fn pid=2818476)[0m Validation loss decreased (0.2578 --> 0.2387).  Saving model state dict ...
[36m(_train_fn pid=2818476)[0m Validation loss decreased (0.2387 --> 0.2336).  Saving model state dict ...
[36m(_train_fn pid=2818476)[0m Validation loss decreased (0.2336 --> 0.2315).  Saving model state dict ...
[36m(_train_fn pid=2818476)[0m Updating learning rate to 1.341800668238384e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2818476)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2817452)[0m Validation loss decreased (inf --> 0.2160).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2818476)[0m Epoch: 4 cost time: 0.4012320041656494[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2818476)[0m Epoch: 4, Steps: 133 | Train Loss: 0.3616947 Vali Loss: 0.2315005 Best vali loss: 0.2315005[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2818476)[0m Validation loss decreased (0.2315 --> 0.2305).  Saving model state dict ...
[36m(_train_fn pid=2818476)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-37-01/checkpoint_000005)[32m [repeated 7x across cluster][0m
2024-08-26 13:37:07,787	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:08,383	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:08,534	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:08,600	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:08,816	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2817452)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-36-49/checkpoint_000001)[32m [repeated 6x across cluster][0m
2024-08-26 13:37:17,403	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:17,456	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2818476)[0m Validation loss decreased (0.2305 --> 0.2300).  Saving model state dict ...
[36m(_train_fn pid=2818476)[0m Validation loss decreased (0.2300 --> 0.2298).  Saving model state dict ...

Trial trial-7b25c_00005 completed after 8 iterations at 2024-08-26 13:37:08. Total running time: 18s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.59343 â”‚
â”‚ time_total_s                                 4.98222 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22969 â”‚
â”‚ train_loss                                   0.36795 â”‚
â”‚ valid_loss                                   0.22969 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2818476)[0m Validation loss decreased (0.2298 --> 0.2297).  Saving model state dict ...
[36m(_train_fn pid=2818476)[0m 	iters: 100, epoch: 8 | loss: 0.3679461[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2818476)[0m 	speed: 0.0058s/iter; left time: 0.1957s[32m [repeated 21x across cluster][0m

Trial trial-7b25c_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00435 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00044 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2819342)[0m configuration
[36m(_train_fn pid=2819342)[0m {'batch_size': 16, 'learning_rate': 0.0004377603894965504, 'd_model': 32, 'd_core': 256, 'e_layers': 2, 'dropout': 0.00435286364938959, 'lradj': 'type1', 'd_ff': 32}
[36m(_train_fn pid=2819342)[0m Use GPU: cuda:0
[36m(_train_fn pid=2817453)[0m Validation loss decreased (0.2859 --> 0.2783).  Saving model state dict ...
[36m(_train_fn pid=2819342)[0m train 8449
[36m(_train_fn pid=2819342)[0m val 2785
[36m(_train_fn pid=2819342)[0m start_epoch 0
[36m(_train_fn pid=2819342)[0m max_epoch 8
[36m(_train_fn pid=2817453)[0m Updating learning rate to 0.0017564265964463382[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2817453)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2817454)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2817453)[0m Epoch: 2 cost time: 6.331755638122559[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2817453)[0m Epoch: 2, Steps: 529 | Train Loss: 0.4283475 Vali Loss: 0.2783087 Best vali loss: 0.2783087[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2817455)[0m 	iters: 400, epoch: 2 | loss: 0.2784161[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2817455)[0m 	speed: 0.0150s/iter; left time: 104.7470s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2817452)[0m Updating learning rate to 5.4681791396423194e-05
[36m(_train_fn pid=2817452)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2817452)[0m saving checkpoint...
[36m(_train_fn pid=2817452)[0m Epoch: 2 cost time: 9.730833292007446
[36m(_train_fn pid=2817452)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.3143075 Vali Loss: 0.2210888 Best vali loss: 0.2159521
[36m(_train_fn pid=2817453)[0m Validation loss decreased (0.2783 --> 0.2703).  Saving model state dict ...
[36m(_train_fn pid=2819342)[0m Validation loss decreased (inf --> 0.2230).  Saving model state dict ...
[36m(_train_fn pid=2817452)[0m 	iters: 200, epoch: 3 | loss: 0.6524662[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2817452)[0m 	speed: 0.0105s/iter; left time: 64.6193s[32m [repeated 17x across cluster][0m

Trial status: 2 TERMINATED | 5 RUNNING
Current time: 2024-08-26 13:37:19. Total running time: 30s
Logical resource usage: 5.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 7b25c_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2819342)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-37-08/checkpoint_000001)[32m [repeated 3x across cluster][0m
2024-08-26 13:37:25,677	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:25,778	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:26,261	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:29,835	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2817452)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-36-49/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 13:37:30,471	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:35,138	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2817453)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-36-49/checkpoint_000004)[32m [repeated 2x across cluster][0m
2024-08-26 13:37:37,328	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-7b25c_00001   RUNNING                 8       6.40637e-05         256              4         64            2   0.00128816   cosine         2           25.7687        0.314308       0.221089            0.215952 â”‚
â”‚ trial-7b25c_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          3           26.0111        0.647459       0.270347            0.270347 â”‚
â”‚ trial-7b25c_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         1           17.2882        0.409933       0.220549            0.220549 â”‚
â”‚ trial-7b25c_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          1           17.2168        0.548985       0.271137            0.271137 â”‚
â”‚ trial-7b25c_00006   RUNNING                16       0.00043776           32              1        256            2   0.00435286   type1          1            7.50984       0.544316       0.222999            0.222999 â”‚
â”‚ trial-7b25c_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           10.54          0.433672       0.232965            0.219535 â”‚
â”‚ trial-7b25c_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            4.98222       0.367946       0.229691            0.229691 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2819342)[0m Updating learning rate to 0.0004377603894965504[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2819342)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2819342)[0m Epoch: 1 cost time: 5.982268810272217[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2819342)[0m Epoch: 1, Steps: 529 | Train Loss: 0.5443157 Vali Loss: 0.2229993 Best vali loss: 0.2229993[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2819342)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2817453)[0m 	iters: 500, epoch: 4 | loss: 0.3747051[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2817453)[0m 	speed: 0.0127s/iter; left time: 27.2322s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2817455)[0m Validation loss decreased (0.2711 --> 0.2668).  Saving model state dict ...
[36m(_train_fn pid=2817453)[0m Updating learning rate to 0.00043910664911158456[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2817453)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2817453)[0m Epoch: 4 cost time: 7.656909227371216[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2817453)[0m Epoch: 4, Steps: 529 | Train Loss: 0.4098419 Vali Loss: 0.2689202 Best vali loss: 0.2689202[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2817452)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2817453)[0m 	iters: 200, epoch: 5 | loss: 0.2823420[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2817453)[0m 	speed: 0.0138s/iter; left time: 26.4234s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2819342)[0m Validation loss decreased (0.2230 --> 0.2218).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2819342)[0m Updating learning rate to 0.0001094400973741376[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2819342)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2819342)[0m Epoch: 3 cost time: 5.086714506149292[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2819342)[0m Epoch: 3, Steps: 529 | Train Loss: 0.3808414 Vali Loss: 0.2217501 Best vali loss: 0.2217501[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2817452)[0m 	iters: 500, epoch: 4 | loss: 0.2776805[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2817452)[0m 	speed: 0.0092s/iter; left time: 43.9450s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2817453)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2819342)[0m Updating learning rate to 5.47200486870688e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2819342)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2819342)[0m Epoch: 4 cost time: 5.561014890670776[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2819342)[0m Epoch: 4, Steps: 529 | Train Loss: 0.3405288 Vali Loss: 0.2232343 Best vali loss: 0.2217501[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2817455)[0m 	iters: 1000, epoch: 3 | loss: 0.5788560[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2817455)[0m 	speed: 0.0149s/iter; left time: 79.6987s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2819342)[0m EarlyStopping counter: 1 out of 3

Trial trial-7b25c_00001 completed after 4 iterations at 2024-08-26 13:37:42. Total running time: 53s
[36m(_train_fn pid=2817452)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-36-49/checkpoint_000003)[32m [repeated 2x across cluster][0m
2024-08-26 13:37:42,948	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:43,048	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:43,435	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:43,528	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:46,067	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:47,078	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:48,085	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2820658)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00007_7_alpha_d_ff=2,batch_size=32,d_core=64,d_model=512,dropout=0.0058,e_layers=1,learning_rate=0.0027,lradj=type1_2024-08-26_13-37-42/checkpoint_000002)[32m [repeated 7x across cluster][0m
2024-08-26 13:37:49,092	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:49,791	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            12.92595 â”‚
â”‚ time_total_s                                51.36485 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21595 â”‚
â”‚ train_loss                                   0.34303 â”‚
â”‚ valid_loss                                   0.22278 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2817452)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2817452)[0m Early stopping
[36m(_train_fn pid=2817454)[0m Updating learning rate to 0.00010680509480810367[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2817454)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2817454)[0m Epoch: 3 cost time: 15.199033737182617[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2817454)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4148610 Vali Loss: 0.2232911 Best vali loss: 0.2205494[32m [repeated 3x across cluster][0m

Trial trial-7b25c_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00579 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00268 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2820658)[0m configuration
[36m(_train_fn pid=2820658)[0m {'batch_size': 32, 'learning_rate': 0.002681331476927111, 'd_model': 512, 'd_core': 64, 'e_layers': 1, 'dropout': 0.005787899105404264, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2820658)[0m Use GPU: cuda:0
[36m(_train_fn pid=2820658)[0m train 8449
[36m(_train_fn pid=2820658)[0m val 2785
[36m(_train_fn pid=2820658)[0m start_epoch 0
[36m(_train_fn pid=2820658)[0m max_epoch 8
[36m(_train_fn pid=2817455)[0m 	iters: 200, epoch: 4 | loss: 0.4547102[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2817455)[0m 	speed: 0.0122s/iter; left time: 62.1604s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2820658)[0m Validation loss decreased (inf --> 0.2258).  Saving model state dict ...
[36m(_train_fn pid=2820658)[0m EarlyStopping counter: 1 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2820658)[0m Updating learning rate to 0.0006703328692317777[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2820658)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2820658)[0m Epoch: 3 cost time: 0.8320140838623047[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2820658)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3179801 Vali Loss: 0.2430369 Best vali loss: 0.2257504[32m [repeated 5x across cluster][0m

Trial trial-7b25c_00007 completed after 4 iterations at 2024-08-26 13:37:49. Total running time: 59s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                              1.0053 â”‚
â”‚ time_total_s                                 4.73897 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22575 â”‚
â”‚ train_loss                                   0.20782 â”‚
â”‚ valid_loss                                   0.25053 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2820658)[0m Early stopping

Trial trial-7b25c_00006 completed after 6 iterations at 2024-08-26 13:37:49. Total running time: 1min 0s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             6.25994 â”‚
â”‚ time_total_s                                39.83166 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22175 â”‚
â”‚ train_loss                                   0.36244 â”‚
â”‚ valid_loss                                   0.22489 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 5 TERMINATED | 3 RUNNING | 2 PENDING
Current time: 2024-08-26 13:37:49. Total running time: 1min 0s
Logical resource usage: 5.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 7b25c_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
2024-08-26 13:37:50,679	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-7b25c_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          6           52.0248        0.359152       0.270665            0.26892  â”‚
â”‚ trial-7b25c_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         3           51.7308        0.414861       0.223291            0.220549 â”‚
â”‚ trial-7b25c_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          3           51.6224        0.484567       0.271494            0.266773 â”‚
â”‚ trial-7b25c_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           10.54          0.433672       0.232965            0.219535 â”‚
â”‚ trial-7b25c_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           51.3648        0.343032       0.222783            0.215952 â”‚
â”‚ trial-7b25c_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            4.98222       0.367946       0.229691            0.229691 â”‚
â”‚ trial-7b25c_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           39.8317        0.36244        0.224893            0.22175  â”‚
â”‚ trial-7b25c_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          4            4.73897       0.207822       0.250534            0.22575  â”‚
â”‚ trial-7b25c_00008   PENDING                32       0.00196362           64              2        512            4   0.0033338    cosine                                                                                â”‚
â”‚ trial-7b25c_00009   PENDING                32       0.000733579          32              4         64            3   0.00128026   type1                                                                                 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-7b25c_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00333 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00196 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2817454)[0m 	iters: 600, epoch: 4 | loss: 0.2013022[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2817454)[0m 	speed: 0.0104s/iter; left time: 48.9619s[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2821230)[0m configuration
[36m(_train_fn pid=2821230)[0m {'batch_size': 32, 'learning_rate': 0.001963618480113192, 'd_model': 64, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0033338002527237612, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2821230)[0m Use GPU: cuda:0

Trial trial-7b25c_00002 completed after 7 iterations at 2024-08-26 13:37:50. Total running time: 1min 0s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             7.24134 â”‚
â”‚ time_total_s                                59.26612 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26892 â”‚
â”‚ train_loss                                    0.3598 â”‚
â”‚ valid_loss                                    0.2707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2821230)[0m train 8449
[36m(_train_fn pid=2821230)[0m val 2785
[36m(_train_fn pid=2821230)[0m start_epoch 0
[36m(_train_fn pid=2821230)[0m max_epoch 8

Trial trial-7b25c_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00128 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00073 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
2024-08-26 13:37:53,244	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2821293)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-37-49/checkpoint_000000)[32m [repeated 4x across cluster][0m
2024-08-26 13:37:53,915	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:54,526	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:55,172	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:55,315	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:55,846	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:55,973	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:57,085	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:58,028	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:37:58,353	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2821293)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-37-49/checkpoint_000004)[32m [repeated 9x across cluster][0m
2024-08-26 13:38:00,105	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:02,145	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:03,002	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:03,798	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2821230)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials/trial-7b25c_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-37-49/checkpoint_000005)[32m [repeated 4x across cluster][0m
2024-08-26 13:38:03,807	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_5_parallel_trials' in 0.0084s.
[36m(_train_fn pid=2821293)[0m {'batch_size': 32, 'learning_rate': 0.0007335788889365637, 'd_model': 32, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0012802554597501359, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2817453)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2821293)[0m Validation loss decreased (inf --> 0.2216).  Saving model state dict ...
[36m(_train_fn pid=2821293)[0m Updating learning rate to 0.0007335788889365637[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2821293)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2821293)[0m Epoch: 1 cost time: 1.2580432891845703[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2821293)[0m Epoch: 1, Steps: 265 | Train Loss: 0.6565596 Vali Loss: 0.2215726 Best vali loss: 0.2215726[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2821293)[0m Validation loss decreased (0.2216 --> 0.2208).  Saving model state dict ...
[36m(_train_fn pid=2817453)[0m Early stopping[32m [repeated 2x across cluster][0m

Trial trial-7b25c_00003 completed after 4 iterations at 2024-08-26 13:37:55. Total running time: 1min 5s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             12.2641 â”‚
â”‚ time_total_s                                63.99494 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22055 â”‚
â”‚ train_loss                                   0.24506 â”‚
â”‚ valid_loss                                   0.22707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2821293)[0m 	iters: 200, epoch: 3 | loss: 0.2014026[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2821293)[0m 	speed: 0.0040s/iter; left time: 5.6308s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2821293)[0m configuration
[36m(_train_fn pid=2821293)[0m Use GPU: cuda:0
[36m(_train_fn pid=2821293)[0m train 8449
[36m(_train_fn pid=2821293)[0m val 2785
[36m(_train_fn pid=2821293)[0m start_epoch 0
[36m(_train_fn pid=2821293)[0m max_epoch 8

Trial trial-7b25c_00009 completed after 5 iterations at 2024-08-26 13:37:58. Total running time: 1min 8s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.26489 â”‚
â”‚ time_total_s                                 6.98447 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22079 â”‚
â”‚ train_loss                                   0.31516 â”‚
â”‚ valid_loss                                   0.22207 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2821293)[0m EarlyStopping counter: 3 out of 3[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2821230)[0m Validation loss decreased (inf --> 0.2248).  Saving model state dict ...
[36m(_train_fn pid=2821293)[0m Updating learning rate to 4.584868055853523e-05[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2821293)[0m saving checkpoint...[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2821293)[0m Epoch: 5 cost time: 1.0674018859863281[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2821293)[0m Epoch: 5, Steps: 265 | Train Loss: 0.3151644 Vali Loss: 0.2220689 Best vali loss: 0.2207911[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2821230)[0m Validation loss decreased (0.2248 --> 0.2229).  Saving model state dict ...
[36m(_train_fn pid=2821293)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2817455)[0m 	iters: 900, epoch: 5 | loss: 0.2280040[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2817455)[0m 	speed: 0.0066s/iter; left time: 22.0052s[32m [repeated 18x across cluster][0m

Trial trial-7b25c_00004 completed after 5 iterations at 2024-08-26 13:38:02. Total running time: 1min 13s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             7.82658 â”‚
â”‚ time_total_s                                71.67096 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.26677 â”‚
â”‚ train_loss                                   0.43419 â”‚
â”‚ valid_loss                                   0.27142 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-7b25c_00008 completed after 6 iterations at 2024-08-26 13:38:03. Total running time: 1min 14s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-7b25c_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.65052 â”‚
â”‚ time_total_s                                13.12934 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22293 â”‚
â”‚ train_loss                                   0.20661 â”‚
â”‚ valid_loss                                   0.24217 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2821230)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2821230)[0m Updating learning rate to 0.00028756526858096616[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2821230)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2821230)[0m Epoch: 6 cost time: 1.4314661026000977[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2821230)[0m Epoch: 6, Steps: 265 | Train Loss: 0.2066133 Vali Loss: 0.2421666 Best vali loss: 0.2229292[32m [repeated 3x across cluster][0m

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:38:03. Total running time: 1min 14s
Logical resource usage: 1.0/32 CPUs, 0.2/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 7b25c_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-7b25c_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           10.54          0.433672       0.232965            0.219535 â”‚
â”‚ trial-7b25c_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           51.3648        0.343032       0.222783            0.215952 â”‚
â”‚ trial-7b25c_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           59.2661        0.359801       0.270699            0.26892  â”‚
â”‚ trial-7b25c_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           63.9949        0.245055       0.22707             0.220549 â”‚
â”‚ trial-7b25c_00004   TERMINATED              8       0.00357919          256              3         64            4   0.00325461   type1          5           71.671         0.434185       0.271418            0.266773 â”‚
â”‚ trial-7b25c_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            4.98222       0.367946       0.229691            0.229691 â”‚
â”‚ trial-7b25c_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           39.8317        0.36244        0.224893            0.22175  â”‚
â”‚ trial-7b25c_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          4            4.73897       0.207822       0.250534            0.22575  â”‚
â”‚ trial-7b25c_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6           13.1293        0.206613       0.242167            0.222929 â”‚
â”‚ trial-7b25c_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            6.98447       0.315164       0.222069            0.220791 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2821230)[0m Early stopping
[36m(_train_fn pid=2821230)[0m 	iters: 200, epoch: 6 | loss: 0.2011863[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2821230)[0m 	speed: 0.0052s/iter; left time: 3.0780s[32m [repeated 4x across cluster][0m


Time taken (5 parallel trials): 79 seconds


2024-08-26 13:38:07,776	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:38:08,179	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:38:08,184	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:38:08,191	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_6_parallel_trials   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator                â”‚
â”‚ Scheduler                        FIFOScheduler                        â”‚
â”‚ Number of trials                 10                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-38-07_347784_2822453/artifacts/2024-08-26_13-38-08/ETTh2_96_96_test_6_parallel_trials/driver_artifacts`

Trial status: 6 PENDING
Current time: 2024-08-26 13:38:08. Total running time: 0s
Logical resource usage: 6.0/32 CPUs, 0.9600000000000001/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-a9e7d_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-a9e7d_00001   PENDING               8       6.40637e-05         256              4         64            2   0.00128816   cosine  â”‚
â”‚ trial-a9e7d_00002   PENDING              16       0.00351285          512              2         64            4   0.00890605   type1   â”‚
â”‚ trial-a9e7d_00003   PENDING               8       0.00015449          128              1        512            4   0.00595831   cosine  â”‚
â”‚ trial-a9e7d_00004   PENDING               8       0.00357919          256              3         64            4   0.00325461   type1   â”‚
â”‚ trial-a9e7d_00005   PENDING              64       0.000107344          64              1        128            1   0.00131792   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-a9e7d_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00129 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-a9e7d_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00132 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-a9e7d_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00891 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00351 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-a9e7d_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00596 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00015 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-a9e7d_00004 started with configuration:
2024-08-26 13:38:11,593	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2824701)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-38-08/checkpoint_000000)
2024-08-26 13:38:12,530	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2824701)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-38-08/checkpoint_000001)
2024-08-26 13:38:13,488	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2824701)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-38-08/checkpoint_000002)
2024-08-26 13:38:13,584	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:14,437	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:15,395	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:16,345	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:16,480	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:17,319	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2824701)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-38-08/checkpoint_000006)[32m [repeated 6x across cluster][0m
2024-08-26 13:38:18,303	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:19,275	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00325 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00358 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2824699)[0m configuration
[36m(_train_fn pid=2824699)[0m {'batch_size': 8, 'learning_rate': 0.00015448958497378217, 'd_model': 128, 'd_core': 512, 'e_layers': 4, 'dropout': 0.005958307406826554, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2824699)[0m Use GPU: cuda:0
[36m(_train_fn pid=2824698)[0m {'batch_size': 16, 'learning_rate': 0.0035128531928926765, 'd_model': 512, 'd_core': 64, 'e_layers': 4, 'dropout': 0.00890604745419256, 'lradj': 'type1', 'd_ff': 1024}

Trial trial-a9e7d_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2824699)[0m train 8449
[36m(_train_fn pid=2824699)[0m val 2785
[36m(_train_fn pid=2824699)[0m start_epoch 0
[36m(_train_fn pid=2824699)[0m max_epoch 8
[36m(_train_fn pid=2824701)[0m 	iters: 100, epoch: 1 | loss: 0.6779016
[36m(_train_fn pid=2824701)[0m 	speed: 0.0113s/iter; left time: 10.9011s
[36m(_train_fn pid=2824701)[0m Updating learning rate to 0.00010734405345907071
[36m(_train_fn pid=2824701)[0m saving checkpoint...
[36m(_train_fn pid=2824701)[0m Validation loss decreased (inf --> 0.2578).  Saving model state dict ...
[36m(_train_fn pid=2824701)[0m Epoch: 1 cost time: 1.009758710861206
[36m(_train_fn pid=2824701)[0m Epoch: 1, Steps: 133 | Train Loss: 0.6779016 Vali Loss: 0.2577833 Best vali loss: 0.2577833
[36m(_train_fn pid=2824701)[0m Updating learning rate to 5.367202672953536e-05
[36m(_train_fn pid=2824701)[0m saving checkpoint...
[36m(_train_fn pid=2824701)[0m Validation loss decreased (0.2578 --> 0.2387).  Saving model state dict ...
[36m(_train_fn pid=2824701)[0m Epoch: 2 cost time: 0.7244503498077393
[36m(_train_fn pid=2824701)[0m Epoch: 2, Steps: 133 | Train Loss: 0.3805931 Vali Loss: 0.2387081 Best vali loss: 0.2387081
[36m(_train_fn pid=2824701)[0m Updating learning rate to 2.683601336476768e-05
[36m(_train_fn pid=2824701)[0m saving checkpoint...
[36m(_train_fn pid=2824701)[0m Validation loss decreased (0.2387 --> 0.2336).  Saving model state dict ...
[36m(_train_fn pid=2824701)[0m Epoch: 3 cost time: 0.7397620677947998
[36m(_train_fn pid=2824701)[0m Epoch: 3, Steps: 133 | Train Loss: 0.2937359 Vali Loss: 0.2335684 Best vali loss: 0.2335684
[36m(_train_fn pid=2824701)[0m Validation loss decreased (0.2336 --> 0.2315).  Saving model state dict ...
[36m(_train_fn pid=2824696)[0m configuration[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2824696)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2824696)[0m Use GPU: cuda:0[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824701)[0m {'batch_size': 64, 'learning_rate': 0.00010734405345907071, 'd_model': 64, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0013179176757202608, 'lradj': 'type1', 'd_ff': 64}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2824701)[0m train 8449[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824701)[0m val 2785[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824701)[0m start_epoch 0[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824701)[0m max_epoch 8[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824701)[0m Validation loss decreased (0.2315 --> 0.2305).  Saving model state dict ...
[36m(_train_fn pid=2824701)[0m Validation loss decreased (0.2305 --> 0.2300).  Saving model state dict ...
[36m(_train_fn pid=2824698)[0m 	iters: 300, epoch: 1 | loss: 1.3000757[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2824698)[0m 	speed: 0.0178s/iter; left time: 70.0099s[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2824696)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2824696)[0m Updating learning rate to 0.00025606601717798207[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824696)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824696)[0m Validation loss decreased (inf --> 0.2195).  Saving model state dict ...
[36m(_train_fn pid=2824696)[0m Epoch: 2 cost time: 2.4479622840881348[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824696)[0m Epoch: 2, Steps: 265 | Train Loss: 0.4370105 Vali Loss: 0.2229692 Best vali loss: 0.2195345[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824701)[0m Validation loss decreased (0.2300 --> 0.2298).  Saving model state dict ...

Trial trial-a9e7d_00005 completed after 8 iterations at 2024-08-26 13:38:18. Total running time: 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             0.98158 â”‚
â”‚ time_total_s                                  8.4436 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22969 â”‚
â”‚ train_loss                                   0.36879 â”‚
â”‚ valid_loss                                   0.22969 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2824701)[0m Validation loss decreased (0.2298 --> 0.2297).  Saving model state dict ...
[36m(_train_fn pid=2824696)[0m EarlyStopping counter: 2 out of 3

Trial trial-a9e7d_00006 started with configuration:
2024-08-26 13:38:21,077	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:21,662	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:22,476	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2826178)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-38-18/checkpoint_000000)[32m [repeated 5x across cluster][0m
2024-08-26 13:38:24,390	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:24,885	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:25,958	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:26,291	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:27,871	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2826485)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00007_7_alpha_d_ff=2,batch_size=32,d_core=64,d_model=512,dropout=0.0058,e_layers=1,learning_rate=0.0027,lradj=type1_2024-08-26_13-38-21/checkpoint_000001)[32m [repeated 5x across cluster][0m
2024-08-26 13:38:28,288	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:29,586	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:29,736	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:29,850	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:30,002	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:30,337	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:31,511	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:32,250	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:33,643	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2826485)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00007_7_alpha_d_ff=2,batch_size=32,d_core=64,d_model=512,dropout=0.0058,e_layers=1,learning_rate=0.0027,lradj=type1_2024-08-26_13-38-21/checkpoint_000004)[32m [repeated 9x across cluster][0m
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00435 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00044 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2826178)[0m configuration
[36m(_train_fn pid=2826178)[0m Use GPU: cuda:0
[36m(_train_fn pid=2826178)[0m {'batch_size': 16, 'learning_rate': 0.0004377603894965504, 'd_model': 32, 'd_core': 256, 'e_layers': 2, 'dropout': 0.00435286364938959, 'lradj': 'type1', 'd_ff': 32}
[36m(_train_fn pid=2826178)[0m train 8449
[36m(_train_fn pid=2826178)[0m val 2785
[36m(_train_fn pid=2826178)[0m start_epoch 0
[36m(_train_fn pid=2826178)[0m max_epoch 8
[36m(_train_fn pid=2824698)[0m Validation loss decreased (inf --> 0.2859).  Saving model state dict ...
[36m(_train_fn pid=2826178)[0m 	iters: 300, epoch: 1 | loss: 0.3054300[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2826178)[0m 	speed: 0.0031s/iter; left time: 12.1715s[32m [repeated 22x across cluster][0m

Trial trial-a9e7d_00000 completed after 4 iterations at 2024-08-26 13:38:21. Total running time: 13s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.38421 â”‚
â”‚ time_total_s                                11.76033 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21953 â”‚
â”‚ train_loss                                   0.43367 â”‚
â”‚ valid_loss                                   0.23297 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2824696)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2824696)[0m Early stopping
[36m(_train_fn pid=2824696)[0m Updating learning rate to 0.00015[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824696)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824696)[0m Epoch: 4 cost time: 1.9511260986328125[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824696)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4336717 Vali Loss: 0.2329654 Best vali loss: 0.2195345[32m [repeated 5x across cluster][0m

Trial trial-a9e7d_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00579 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00268 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2826485)[0m configuration
[36m(_train_fn pid=2826485)[0m {'batch_size': 32, 'learning_rate': 0.002681331476927111, 'd_model': 512, 'd_core': 64, 'e_layers': 1, 'dropout': 0.005787899105404264, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2826485)[0m Use GPU: cuda:0
[36m(_train_fn pid=2826485)[0m train 8449
[36m(_train_fn pid=2826485)[0m val 2785
[36m(_train_fn pid=2826485)[0m start_epoch 0
[36m(_train_fn pid=2826485)[0m max_epoch 8
[36m(_train_fn pid=2826178)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2826485)[0m Validation loss decreased (inf --> 0.2288).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2826178)[0m Validation loss decreased (0.2230 --> 0.2217).  Saving model state dict ...
[36m(_train_fn pid=2826485)[0m 	iters: 100, epoch: 2 | loss: 0.5055538[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=2826485)[0m 	speed: 0.0147s/iter; left time: 25.7659s[32m [repeated 28x across cluster][0m
[36m(_train_fn pid=2826178)[0m Updating learning rate to 0.0001094400973741376[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2826178)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2826178)[0m Epoch: 3 cost time: 1.6077160835266113[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2826178)[0m Epoch: 3, Steps: 529 | Train Loss: 0.3810136 Vali Loss: 0.2217201 Best vali loss: 0.2217201[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2826178)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2824699)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2824698)[0m Validation loss decreased (0.2859 --> 0.2783).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2824698)[0m 	iters: 100, epoch: 3 | loss: 0.5801491[32m [repeated 29x across cluster][0m
[36m(_train_fn pid=2824698)[0m 	speed: 0.0344s/iter; left time: 105.8078s[32m [repeated 29x across cluster][0m
[36m(_train_fn pid=2826485)[0m Updating learning rate to 0.0003351664346158889[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2826485)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2826485)[0m Epoch: 4 cost time: 1.4717848300933838[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2826485)[0m Epoch: 4, Steps: 265 | Train Loss: 0.2295498 Vali Loss: 0.2308265 Best vali loss: 0.2242911[32m [repeated 8x across cluster][0m

Trial trial-a9e7d_00006 completed after 6 iterations at 2024-08-26 13:38:32. Total running time: 24s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.90997 â”‚
â”‚ time_total_s                                12.37288 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22172 â”‚
â”‚ train_loss                                   0.36204 â”‚
â”‚ valid_loss                                   0.22492 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2826178)[0m Early stopping

Trial trial-a9e7d_00008 started with configuration:
2024-08-26 13:38:35,999	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00333 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00196 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2827634)[0m configuration
[36m(_train_fn pid=2827634)[0m {'batch_size': 32, 'learning_rate': 0.001963618480113192, 'd_model': 64, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0033338002527237612, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2827634)[0m Use GPU: cuda:0
[36m(_train_fn pid=2827634)[0m train 8449
[36m(_train_fn pid=2827634)[0m val 2785
[36m(_train_fn pid=2827634)[0m start_epoch 0
[36m(_train_fn pid=2827634)[0m max_epoch 8
[36m(_train_fn pid=2826485)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m

Trial trial-a9e7d_00007 completed after 6 iterations at 2024-08-26 13:38:36. Total running time: 27s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             2.35403 â”‚
â”‚ time_total_s                                12.75574 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22429 â”‚
â”‚ train_loss                                   0.37774 â”‚
â”‚ valid_loss                                   0.23335 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2824698)[0m 	iters: 400, epoch: 3 | loss: 0.4205400[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2824698)[0m 	speed: 0.0164s/iter; left time: 45.4941s[32m [repeated 19x across cluster][0m

Trial trial-a9e7d_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00128 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00073 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2827861)[0m {'batch_size': 32, 'learning_rate': 0.0007335788889365637, 'd_model': 32, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0012802554597501359, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2826485)[0m Updating learning rate to 8.379160865397222e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2826485)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2826485)[0m Epoch: 6 cost time: 1.7645723819732666[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2826485)[0m Epoch: 6, Steps: 265 | Train Loss: 0.3777368 Vali Loss: 0.2333461 Best vali loss: 0.2242911[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2826485)[0m Early stopping

Trial status: 4 TERMINATED | 6 RUNNING
Current time: 2024-08-26 13:38:38. Total running time: 30s
Logical resource usage: 6.0/32 CPUs, 0.9600000000000001/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: a9e7d_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
2024-08-26 13:38:38,970	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2824697)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-38-08/checkpoint_000001)[32m [repeated 2x across cluster][0m
2024-08-26 13:38:39,115	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:39,537	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:39,920	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:40,910	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:42,278	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:43,247	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:43,558	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:44,795	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2827861)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-38-36/checkpoint_000004)[32m [repeated 8x across cluster][0m
2024-08-26 13:38:47,404	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:48,620	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:48,879	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:49,100	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:51,210	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2827634)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-38-32/checkpoint_000003)[32m [repeated 5x across cluster][0m
2024-08-26 13:38:52,454	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-a9e7d_00001   RUNNING                 8       6.40637e-05         256              4         64            2   0.00128816   cosine         1            15.0514       0.319731       0.215952            0.215952 â”‚
â”‚ trial-a9e7d_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          2            20.1536       0.428347       0.278309            0.278309 â”‚
â”‚ trial-a9e7d_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         1            19.9861       0.409933       0.220549            0.220549 â”‚
â”‚ trial-a9e7d_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          1            19.8627       0.548985       0.271137            0.271137 â”‚
â”‚ trial-a9e7d_00008   RUNNING                32       0.00196362           64              2        512            4   0.0033338    cosine                                                                                â”‚
â”‚ trial-a9e7d_00009   RUNNING                32       0.000733579          32              4         64            3   0.00128026   type1                                                                                 â”‚
â”‚ trial-a9e7d_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            11.7603       0.433672       0.232965            0.219535 â”‚
â”‚ trial-a9e7d_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8             8.4436       0.368788       0.229693            0.229693 â”‚
â”‚ trial-a9e7d_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6            12.3729       0.362042       0.224921            0.22172  â”‚
â”‚ trial-a9e7d_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6            12.7557       0.377737       0.233346            0.224291 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2827861)[0m configuration
[36m(_train_fn pid=2827861)[0m Use GPU: cuda:0
[36m(_train_fn pid=2827634)[0m Validation loss decreased (inf --> 0.2248).  Saving model state dict ...
[36m(_train_fn pid=2827861)[0m train 8449
[36m(_train_fn pid=2827861)[0m val 2785
[36m(_train_fn pid=2827861)[0m start_epoch 0
[36m(_train_fn pid=2827861)[0m max_epoch 8
[36m(_train_fn pid=2824698)[0m Validation loss decreased (0.2783 --> 0.2703).  Saving model state dict ...
[36m(_train_fn pid=2824697)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2824697)[0m 	iters: 300, epoch: 3 | loss: 0.1612788[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2824697)[0m 	speed: 0.0111s/iter; left time: 66.8511s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2827861)[0m Updating learning rate to 0.00018339472223414093[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2827861)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2827861)[0m Epoch: 3 cost time: 1.1546242237091064[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2827861)[0m Epoch: 3, Steps: 265 | Train Loss: 0.2450095 Vali Loss: 0.2231636 Best vali loss: 0.2207911[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2827861)[0m Validation loss decreased (inf --> 0.2216).  Saving model state dict ...

Trial trial-a9e7d_00009 completed after 5 iterations at 2024-08-26 13:38:44. Total running time: 36s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.23434 â”‚
â”‚ time_total_s                                 7.21864 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22079 â”‚
â”‚ train_loss                                   0.31516 â”‚
â”‚ valid_loss                                   0.22207 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2827861)[0m Early stopping
[36m(_train_fn pid=2827861)[0m Validation loss decreased (0.2216 --> 0.2208).  Saving model state dict ...
[36m(_train_fn pid=2827861)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2824698)[0m 	iters: 500, epoch: 4 | loss: 0.3747051[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2824698)[0m 	speed: 0.0134s/iter; left time: 28.6518s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2827634)[0m Validation loss decreased (0.2248 --> 0.2229).  Saving model state dict ...
[36m(_train_fn pid=2827634)[0m Updating learning rate to 0.0013575313699692147[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2827634)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2827634)[0m Epoch: 3 cost time: 3.724865436553955[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2827634)[0m Epoch: 3, Steps: 265 | Train Loss: 0.4804408 Vali Loss: 0.2229292 Best vali loss: 0.2229292[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2824699)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2827634)[0m EarlyStopping counter: 1 out of 3
2024-08-26 13:38:55,379	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:38:58,466	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2824698)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-38-08/checkpoint_000004)[32m [repeated 3x across cluster][0m
2024-08-26 13:38:59,404	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2824697)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-38-08/checkpoint_000003)[32m [repeated 2x across cluster][0m
2024-08-26 13:39:05,332	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:05,469	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:05,619	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2827634)[0m 	iters: 100, epoch: 5 | loss: 0.1700149[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2827634)[0m 	speed: 0.0273s/iter; left time: 26.1971s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2824698)[0m Validation loss decreased (0.2703 --> 0.2689).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2824697)[0m Updating learning rate to 4.428991077132631e-05[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824697)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824697)[0m Epoch: 3 cost time: 10.959511518478394[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2824697)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4252212 Vali Loss: 0.2197178 Best vali loss: 0.2159521[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2827634)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2824700)[0m 	iters: 600, epoch: 3 | loss: 0.3312855[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2824700)[0m 	speed: 0.0150s/iter; left time: 86.0485s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2824698)[0m Updating learning rate to 0.00021955332455579228[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2824698)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2824698)[0m Epoch: 5 cost time: 8.060476064682007[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2824698)[0m Epoch: 5, Steps: 529 | Train Loss: 0.4995547 Vali Loss: 0.2704001 Best vali loss: 0.2689202[32m [repeated 2x across cluster][0m

Trial trial-a9e7d_00008 completed after 6 iterations at 2024-08-26 13:38:59. Total running time: 51s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             4.02208 â”‚
â”‚ time_total_s                                25.55714 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22293 â”‚
â”‚ train_loss                                   0.20661 â”‚
â”‚ valid_loss                                   0.24217 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2827634)[0m Early stopping
[36m(_train_fn pid=2827634)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2824700)[0m 	iters: 1000, epoch: 3 | loss: 0.5788560[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2824700)[0m 	speed: 0.0126s/iter; left time: 67.4101s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2827634)[0m Updating learning rate to 0.00028756526858096616
[36m(_train_fn pid=2827634)[0m saving checkpoint...
[36m(_train_fn pid=2827634)[0m Epoch: 6 cost time: 3.5314464569091797
[36m(_train_fn pid=2827634)[0m Epoch: 6, Steps: 265 | Train Loss: 0.2066133 Vali Loss: 0.2421666 Best vali loss: 0.2229292

Trial trial-a9e7d_00001 completed after 4 iterations at 2024-08-26 13:39:05. Total running time: 56s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            12.58445 â”‚
â”‚ time_total_s                                55.19767 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21595 â”‚
â”‚ train_loss                                   0.34303 â”‚
â”‚ valid_loss                                   0.22278 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2824697)[0m Updating learning rate to 3.203185178516828e-05
[36m(_train_fn pid=2824697)[0m saving checkpoint...
[36m(_train_fn pid=2824697)[0m Epoch: 4 cost time: 10.736329317092896
[36m(_train_fn pid=2824697)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3430316 Vali Loss: 0.2227829 Best vali loss: 0.2159521
[36m(_train_fn pid=2824697)[0m Early stopping
[36m(_train_fn pid=2824698)[0m EarlyStopping counter: 2 out of 3[32m [repeated 4x across cluster][0m

Trial status: 7 TERMINATED | 3 RUNNING
Current time: 2024-08-26 13:39:08. Total running time: 1min 0s
Logical resource usage: 3.0/32 CPUs, 0.48/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: a9e7d_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2824698)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-38-08/checkpoint_000006)[32m [repeated 4x across cluster][0m
2024-08-26 13:39:15,316	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:15,535	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-a9e7d_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          6           55.7442        0.359152       0.270665            0.26892  â”‚
â”‚ trial-a9e7d_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         3           55.5994        0.414861       0.223291            0.220549 â”‚
â”‚ trial-a9e7d_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          3           55.45          0.484567       0.271494            0.266773 â”‚
â”‚ trial-a9e7d_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           11.7603        0.433672       0.232965            0.219535 â”‚
â”‚ trial-a9e7d_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           55.1977        0.343032       0.222783            0.215952 â”‚
â”‚ trial-a9e7d_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            8.4436        0.368788       0.229693            0.229693 â”‚
â”‚ trial-a9e7d_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           12.3729        0.362042       0.224921            0.22172  â”‚
â”‚ trial-a9e7d_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6           12.7557        0.377737       0.233346            0.224291 â”‚
â”‚ trial-a9e7d_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6           25.5571        0.206613       0.242167            0.222929 â”‚
â”‚ trial-a9e7d_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            7.21864       0.315164       0.222069            0.220791 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2824698)[0m Updating learning rate to 0.00010977666227789614[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2824698)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2824698)[0m Epoch: 6 cost time: 6.4445531368255615[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2824698)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3591521 Vali Loss: 0.2706647 Best vali loss: 0.2689202[32m [repeated 3x across cluster][0m

Trial trial-a9e7d_00002 completed after 7 iterations at 2024-08-26 13:39:11. Total running time: 1min 3s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             6.19575 â”‚
â”‚ time_total_s                                61.93991 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26892 â”‚
â”‚ train_loss                                    0.3598 â”‚
â”‚ valid_loss                                    0.2707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2824698)[0m Early stopping
[36m(_train_fn pid=2824698)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2824699)[0m 	iters: 900, epoch: 4 | loss: 0.2850068[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2824699)[0m 	speed: 0.0066s/iter; left time: 28.8125s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2824700)[0m EarlyStopping counter: 2 out of 3

Trial trial-a9e7d_00003 completed after 4 iterations at 2024-08-26 13:39:15. Total running time: 1min 7s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             10.0624 â”‚
â”‚ time_total_s                                65.66183 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22055 â”‚
â”‚ train_loss                                   0.24506 â”‚
â”‚ valid_loss                                   0.22707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2824699)[0m Updating learning rate to 7.724479248689109e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2824699)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2824699)[0m Epoch: 4 cost time: 9.120137453079224[32m [repeated 3x across cluster][0m
                                                                                                                                                                                                                                  2024-08-26 13:39:21,279	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials' in 0.0104s.
[36m(_train_fn pid=2824699)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.2450550 Vali Loss: 0.2270699 Best vali loss: 0.2205494[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2824699)[0m Early stopping
[36m(_train_fn pid=2824700)[0m 	iters: 700, epoch: 5 | loss: 0.3472791[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2824700)[0m 	speed: 0.0049s/iter; left time: 17.3748s[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2824699)[0m EarlyStopping counter: 3 out of 3

Trial trial-a9e7d_00004 completed after 5 iterations at 2024-08-26 13:39:21. Total running time: 1min 13s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-a9e7d_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.94965 â”‚
â”‚ time_total_s                                71.38093 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.26677 â”‚
â”‚ train_loss                                   0.43419 â”‚
â”‚ valid_loss                                   0.27142 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:39:21. Total running time: 1min 13s
Logical resource usage: 1.0/32 CPUs, 0.16/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: a9e7d_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-a9e7d_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           11.7603        0.433672       0.232965            0.219535 â”‚
â”‚ trial-a9e7d_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           55.1977        0.343032       0.222783            0.215952 â”‚
â”‚ trial-a9e7d_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           61.9399        0.359801       0.270699            0.26892  â”‚
â”‚ trial-a9e7d_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           65.6618        0.245055       0.22707             0.220549 â”‚
â”‚ trial-a9e7d_00004   TERMINATED              8       0.00357919          256              3         64            4   0.00325461   type1          5           71.3809        0.434185       0.271418            0.266773 â”‚
â”‚ trial-a9e7d_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            8.4436        0.368788       0.229693            0.229693 â”‚
â”‚ trial-a9e7d_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           12.3729        0.362042       0.224921            0.22172  â”‚
â”‚ trial-a9e7d_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6           12.7557        0.377737       0.233346            0.224291 â”‚
â”‚ trial-a9e7d_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6           25.5571        0.206613       0.242167            0.222929 â”‚
â”‚ trial-a9e7d_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            7.21864       0.315164       0.222069            0.220791 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2824700)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_6_parallel_trials/trial-a9e7d_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-38-08/checkpoint_000004)[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2824700)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2824700)[0m Updating learning rate to 0.00022369925551004054
[36m(_train_fn pid=2824700)[0m saving checkpoint...
[36m(_train_fn pid=2824700)[0m 	iters: 1000, epoch: 5 | loss: 0.3214265[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2824700)[0m 	speed: 0.0050s/iter; left time: 16.0098s[32m [repeated 3x across cluster][0m


Time taken (6 parallel trials): 77 seconds


2024-08-26 13:39:25,113	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:39:25,485	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:39:25,490	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:39:25,497	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_7_parallel_trials   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator                â”‚
â”‚ Scheduler                        FIFOScheduler                        â”‚
â”‚ Number of trials                 10                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-39-24_682894_2829822/artifacts/2024-08-26_13-39-25/ETTh2_96_96_test_7_parallel_trials/driver_artifacts`

Trial status: 7 PENDING
Current time: 2024-08-26 13:39:25. Total running time: 0s
Logical resource usage: 7.0/32 CPUs, 0.9800000000000001/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-d8002_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-d8002_00001   PENDING               8       6.40637e-05         256              4         64            2   0.00128816   cosine  â”‚
â”‚ trial-d8002_00002   PENDING              16       0.00351285          512              2         64            4   0.00890605   type1   â”‚
â”‚ trial-d8002_00003   PENDING               8       0.00015449          128              1        512            4   0.00595831   cosine  â”‚
â”‚ trial-d8002_00004   PENDING               8       0.00357919          256              3         64            4   0.00325461   type1   â”‚
â”‚ trial-d8002_00005   PENDING              64       0.000107344          64              1        128            1   0.00131792   type1   â”‚
â”‚ trial-d8002_00006   PENDING              16       0.00043776           32              1        256            2   0.00435286   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d8002_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00891 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00351 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2832072)[0m configuration
[36m(_train_fn pid=2832072)[0m {'batch_size': 16, 'learning_rate': 0.0035128531928926765, 'd_model': 512, 'd_core': 64, 'e_layers': 4, 'dropout': 0.00890604745419256, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2832072)[0m Use GPU: cuda:0

Trial trial-d8002_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d8002_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00132 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d8002_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00325 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00358 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d8002_00006 started with configuration:
2024-08-26 13:39:28,974	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2832075)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-39-25/checkpoint_000000)
2024-08-26 13:39:30,041	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2832075)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-39-25/checkpoint_000001)
2024-08-26 13:39:31,006	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2832075)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-39-25/checkpoint_000002)
2024-08-26 13:39:31,119	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:31,976	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:32,951	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:33,942	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:34,324	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2832070)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00000_0_2024-08-26_13-39-25/checkpoint_000001)[32m [repeated 5x across cluster][0m
2024-08-26 13:39:34,974	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:35,998	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00435 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00044 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d8002_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00129 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d8002_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00596 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00015 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2832070)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2832070)[0m train 8449
[36m(_train_fn pid=2832074)[0m val 2785
[36m(_train_fn pid=2832074)[0m start_epoch 0
[36m(_train_fn pid=2832074)[0m max_epoch 8
[36m(_train_fn pid=2832075)[0m 	iters: 100, epoch: 1 | loss: 0.6779016
[36m(_train_fn pid=2832075)[0m 	speed: 0.0119s/iter; left time: 11.4754s
[36m(_train_fn pid=2832075)[0m Updating learning rate to 0.00010734405345907071
[36m(_train_fn pid=2832075)[0m saving checkpoint...
[36m(_train_fn pid=2832075)[0m Validation loss decreased (inf --> 0.2578).  Saving model state dict ...
[36m(_train_fn pid=2832075)[0m Epoch: 1 cost time: 1.073683261871338
[36m(_train_fn pid=2832075)[0m Epoch: 1, Steps: 133 | Train Loss: 0.6779016 Vali Loss: 0.2577833 Best vali loss: 0.2577833
[36m(_train_fn pid=2832075)[0m Updating learning rate to 5.367202672953536e-05
[36m(_train_fn pid=2832075)[0m saving checkpoint...
[36m(_train_fn pid=2832075)[0m Validation loss decreased (0.2578 --> 0.2387).  Saving model state dict ...
[36m(_train_fn pid=2832075)[0m Epoch: 2 cost time: 0.8412981033325195
[36m(_train_fn pid=2832075)[0m Epoch: 2, Steps: 133 | Train Loss: 0.3805931 Vali Loss: 0.2387081 Best vali loss: 0.2387081
[36m(_train_fn pid=2832075)[0m Updating learning rate to 2.683601336476768e-05
[36m(_train_fn pid=2832075)[0m saving checkpoint...
[36m(_train_fn pid=2832075)[0m Validation loss decreased (0.2387 --> 0.2336).  Saving model state dict ...
[36m(_train_fn pid=2832075)[0m Epoch: 3 cost time: 0.748220682144165
[36m(_train_fn pid=2832075)[0m Epoch: 3, Steps: 133 | Train Loss: 0.2937359 Vali Loss: 0.2335684 Best vali loss: 0.2335684
[36m(_train_fn pid=2832075)[0m Validation loss decreased (0.2336 --> 0.2315).  Saving model state dict ...
[36m(_train_fn pid=2832073)[0m configuration[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2832076)[0m {'batch_size': 16, 'learning_rate': 0.0004377603894965504, 'd_model': 32, 'd_core': 256, 'e_layers': 2, 'dropout': 0.00435286364938959, 'lradj': 'type1', 'd_ff': 32}[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2832073)[0m Use GPU: cuda:0[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2832073)[0m {'batch_size': 8, 'learning_rate': 0.00015448958497378217, 'd_model': 128, 'd_core': 512, 'e_layers': 4, 'dropout': 0.005958307406826554, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2832073)[0m train 8449[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2832073)[0m val 2785[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2832075)[0m Validation loss decreased (0.2315 --> 0.2305).  Saving model state dict ...
[36m(_train_fn pid=2832073)[0m start_epoch 0[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2832073)[0m max_epoch 8[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2832075)[0m Validation loss decreased (0.2305 --> 0.2300).  Saving model state dict ...
[36m(_train_fn pid=2832071)[0m 	iters: 400, epoch: 1 | loss: 0.1596578[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2832071)[0m 	speed: 0.0139s/iter; left time: 111.7790s[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2832070)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2832070)[0m Updating learning rate to 0.00025606601717798207[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2832070)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2832070)[0m Validation loss decreased (inf --> 0.2195).  Saving model state dict ...
[36m(_train_fn pid=2832070)[0m Epoch: 2 cost time: 2.6995816230773926[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2832070)[0m Epoch: 2, Steps: 265 | Train Loss: 0.4370105 Vali Loss: 0.2229692 Best vali loss: 0.2195345[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2832075)[0m Validation loss decreased (0.2300 --> 0.2298).  Saving model state dict ...

Trial trial-d8002_00005 completed after 8 iterations at 2024-08-26 13:39:35. Total running time: 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.02113 â”‚
â”‚ time_total_s                                 8.83108 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22969 â”‚
â”‚ train_loss                                   0.36879 â”‚
â”‚ valid_loss                                   0.22969 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
2024-08-26 13:39:36,918	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:37,304	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:39,262	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2832072)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-39-25/checkpoint_000000)[32m [repeated 6x across cluster][0m
2024-08-26 13:39:39,681	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:39,951	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:40,299	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:41,329	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:42,382	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:43,920	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:44,417	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2832075)[0m Validation loss decreased (0.2298 --> 0.2297).  Saving model state dict ...
[36m(_train_fn pid=2832076)[0m Validation loss decreased (inf --> 0.2230).  Saving model state dict ...
[36m(_train_fn pid=2832070)[0m EarlyStopping counter: 2 out of 3

Trial trial-d8002_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00579 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00268 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2833712)[0m configuration
[36m(_train_fn pid=2833712)[0m {'batch_size': 32, 'learning_rate': 0.002681331476927111, 'd_model': 512, 'd_core': 64, 'e_layers': 1, 'dropout': 0.005787899105404264, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2833712)[0m Use GPU: cuda:0
[36m(_train_fn pid=2833712)[0m train 8449
[36m(_train_fn pid=2833712)[0m val 2785
[36m(_train_fn pid=2833712)[0m start_epoch 0
[36m(_train_fn pid=2833712)[0m max_epoch 8
[36m(_train_fn pid=2832070)[0m 	iters: 200, epoch: 4 | loss: 0.3015919[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2832070)[0m 	speed: 0.0079s/iter; left time: 8.8791s[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2833712)[0m Updating learning rate to 0.002681331476927111[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2833712)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2833712)[0m Epoch: 1 cost time: 1.0524663925170898[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2833712)[0m Epoch: 1, Steps: 265 | Train Loss: 0.3599869 Vali Loss: 0.2257504 Best vali loss: 0.2257504[32m [repeated 5x across cluster][0m

Trial trial-d8002_00000 completed after 4 iterations at 2024-08-26 13:39:39. Total running time: 14s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             2.64446 â”‚
â”‚ time_total_s                                12.80667 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21953 â”‚
â”‚ train_loss                                   0.43367 â”‚
â”‚ valid_loss                                   0.23297 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2832070)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2832070)[0m Early stopping

Trial trial-d8002_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00333 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00196 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2834147)[0m {'batch_size': 32, 'learning_rate': 0.001963618480113192, 'd_model': 64, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0033338002527237612, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2832072)[0m Validation loss decreased (inf --> 0.2859).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2834147)[0m start_epoch 0
[36m(_train_fn pid=2834147)[0m max_epoch 8

Trial trial-d8002_00007 completed after 4 iterations at 2024-08-26 13:39:42. Total running time: 16s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.04984 â”‚
â”‚ time_total_s                                 4.80876 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22575 â”‚
â”‚ train_loss                                   0.20782 â”‚
â”‚ valid_loss                                   0.25053 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2834147)[0m configuration
[36m(_train_fn pid=2834147)[0m Use GPU: cuda:0
[36m(_train_fn pid=2834147)[0m train 8449
[36m(_train_fn pid=2834147)[0m val 2785

Trial trial-d8002_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00128 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00073 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2834479)[0m configuration
[36m(_train_fn pid=2834479)[0m {'batch_size': 32, 'learning_rate': 0.0007335788889365637, 'd_model': 32, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0012802554597501359, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2834479)[0m Use GPU: cuda:0
[36m(_train_fn pid=2834479)[0m train 8449
[36m(_train_fn pid=2834479)[0m val 2785
[36m(_train_fn pid=2834147)[0m 	iters: 100, epoch: 1 | loss: 0.6080922[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2834147)[0m 	speed: 0.0212s/iter; left time: 42.9390s[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2832076)[0m Updating learning rate to 0.0002188801947482752[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2832076)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2832076)[0m Epoch: 2 cost time: 6.056991815567017[32m [repeated 7x across cluster][0m
2024-08-26 13:39:47,061	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2834147)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-39-39/checkpoint_000000)[32m [repeated 7x across cluster][0m
2024-08-26 13:39:48,829	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:49,992	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:50,001	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:50,308	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:51,777	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:52,678	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2834479)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-39-42/checkpoint_000001)[32m [repeated 6x across cluster][0m
2024-08-26 13:39:52,836	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:56,862	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:39:56,922	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2832076)[0m Epoch: 2, Steps: 529 | Train Loss: 0.3621016 Vali Loss: 0.2247194 Best vali loss: 0.2229993[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2832076)[0m EarlyStopping counter: 1 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2833712)[0m Early stopping
[36m(_train_fn pid=2832071)[0m Validation loss decreased (inf --> 0.2160).  Saving model state dict ...
[36m(_train_fn pid=2834479)[0m start_epoch 0
[36m(_train_fn pid=2834479)[0m max_epoch 8
[36m(_train_fn pid=2834147)[0m Validation loss decreased (inf --> 0.2248).  Saving model state dict ...
[36m(_train_fn pid=2832076)[0m 	iters: 400, epoch: 3 | loss: 0.6287578[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2832076)[0m 	speed: 0.0120s/iter; left time: 33.2335s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2834479)[0m Updating learning rate to 0.0007335788889365637[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2834479)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2834479)[0m Epoch: 1 cost time: 3.9183950424194336[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2834479)[0m Epoch: 1, Steps: 265 | Train Loss: 0.6565566 Vali Loss: 0.2215732 Best vali loss: 0.2215732[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2832072)[0m Validation loss decreased (0.2859 --> 0.2783).  Saving model state dict ...
[36m(_train_fn pid=2834147)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2832073)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2834147)[0m 	iters: 200, epoch: 3 | loss: 0.5251341[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2834147)[0m 	speed: 0.0172s/iter; left time: 23.9535s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2832076)[0m Updating learning rate to 0.0001094400973741376[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2832076)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2832076)[0m Epoch: 3 cost time: 6.769777536392212[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2832076)[0m Epoch: 3, Steps: 529 | Train Loss: 0.3808414 Vali Loss: 0.2217501 Best vali loss: 0.2217501[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2832076)[0m Validation loss decreased (0.2230 --> 0.2218).  Saving model state dict ...[32m [repeated 2x across cluster][0m

Trial status: 3 TERMINATED | 7 RUNNING
Current time: 2024-08-26 13:39:55. Total running time: 30s
Logical resource usage: 7.0/32 CPUs, 0.9800000000000001/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: d8002_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-d8002_00001   RUNNING                 8       6.40637e-05         256              4         64            2   0.00128816   cosine         1           16.7131        0.319731       0.215952            0.215952 â”‚
â”‚ trial-d8002_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          2           23.1908        0.428347       0.278309            0.278309 â”‚
â”‚ trial-d8002_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         1           22.7822        0.409933       0.220549            0.220549 â”‚
â”‚ trial-d8002_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          1           22.8337        0.548985       0.271137            0.271137 â”‚
â”‚ trial-d8002_00006   RUNNING                16       0.00043776           32              1        256            2   0.00435286   type1          3           25.6479        0.380841       0.22175             0.22175  â”‚
â”‚ trial-d8002_00008   RUNNING                32       0.00196362           64              2        512            4   0.0033338    cosine         2           10.2384        0.511868       0.235104            0.224819 â”‚
â”‚ trial-d8002_00009   RUNNING                32       0.000733579          32              4         64            3   0.00128026   type1          2            8.74085       0.276534       0.220789            0.220789 â”‚
â”‚ trial-d8002_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           12.8067        0.433672       0.232965            0.219535 â”‚
â”‚ trial-d8002_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            8.83108       0.368788       0.229693            0.229693 â”‚
â”‚ trial-d8002_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          4            4.80876       0.207822       0.250534            0.22575  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2834479)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2832072)[0m 	iters: 500, epoch: 3 | loss: 0.3148034[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2832072)[0m 	speed: 0.0194s/iter; left time: 51.9726s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2834147)[0m Updating learning rate to 0.0013575313699692147[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2834147)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2834147)[0m Epoch: 3 cost time: 4.619184970855713[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2834147)[0m Epoch: 3, Steps: 265 | Train Loss: 0.4804408 Vali Loss: 0.2229292 Best vali loss: 0.2229292[32m [repeated 2x across cluster][0m
2024-08-26 13:40:00,853	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2834479)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-39-42/checkpoint_000003)[32m [repeated 4x across cluster][0m
2024-08-26 13:40:00,925	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:01,609	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:01,741	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:02,529	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:04,812	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:06,645	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2834147)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-39-39/checkpoint_000004)[32m [repeated 6x across cluster][0m
2024-08-26 13:40:09,843	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:11,151	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:12,647	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2832074)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-39-25/checkpoint_000001)[32m [repeated 3x across cluster][0m
2024-08-26 13:40:12,695	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:12,980	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:15,424	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:16,330	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:21,029	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2832072)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-39-25/checkpoint_000004)[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2834147)[0m Validation loss decreased (0.2248 --> 0.2229).  Saving model state dict ...
[36m(_train_fn pid=2834479)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2832072)[0m Validation loss decreased (0.2783 --> 0.2703).  Saving model state dict ...

Trial trial-d8002_00009 completed after 5 iterations at 2024-08-26 13:40:04. Total running time: 39s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             3.95621 â”‚
â”‚ time_total_s                                20.86692 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22079 â”‚
â”‚ train_loss                                   0.31555 â”‚
â”‚ valid_loss                                   0.22202 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2834479)[0m Early stopping
[36m(_train_fn pid=2832076)[0m 	iters: 300, epoch: 5 | loss: 0.5474342[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2832076)[0m 	speed: 0.0124s/iter; left time: 22.6129s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2834479)[0m Updating learning rate to 4.584868055853523e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2834479)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2834479)[0m Epoch: 5 cost time: 3.4437673091888428[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2834479)[0m Epoch: 5, Steps: 265 | Train Loss: 0.3155461 Vali Loss: 0.2220226 Best vali loss: 0.2207892[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2834479)[0m EarlyStopping counter: 3 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2832071)[0m 	iters: 800, epoch: 3 | loss: 1.2098777[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2832071)[0m 	speed: 0.0114s/iter; left time: 63.0897s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2832076)[0m Updating learning rate to 2.73600243435344e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2832076)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2832076)[0m Epoch: 5 cost time: 6.711322069168091[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2832076)[0m Epoch: 5, Steps: 529 | Train Loss: 0.3294062 Vali Loss: 0.2245405 Best vali loss: 0.2217501[32m [repeated 2x across cluster][0m

Trial trial-d8002_00008 completed after 6 iterations at 2024-08-26 13:40:11. Total running time: 45s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             4.50357 â”‚
â”‚ time_total_s                                29.60164 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22293 â”‚
â”‚ train_loss                                   0.20661 â”‚
â”‚ valid_loss                                   0.24217 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2834147)[0m Early stopping
[36m(_train_fn pid=2834147)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2832074)[0m Validation loss decreased (0.2711 --> 0.2668).  Saving model state dict ...
[36m(_train_fn pid=2832072)[0m 	iters: 200, epoch: 5 | loss: 0.2823420[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2832072)[0m 	speed: 0.0134s/iter; left time: 25.7194s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2832071)[0m Updating learning rate to 4.428991077132631e-05[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2832071)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2832071)[0m Epoch: 3 cost time: 12.060089111328125[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2832071)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4252212 Vali Loss: 0.2197178 Best vali loss: 0.2159521[32m [repeated 5x across cluster][0m

Trial trial-d8002_00006 completed after 6 iterations at 2024-08-26 13:40:16. Total running time: 50s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             6.48427 â”‚
â”‚ time_total_s                                 49.1338 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22175 â”‚
â”‚ train_loss                                   0.36244 â”‚
â”‚ valid_loss                                   0.22489 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2832076)[0m Early stopping
[36m(_train_fn pid=2832076)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2832072)[0m Validation loss decreased (0.2703 --> 0.2689).  Saving model state dict ...
[36m(_train_fn pid=2832071)[0m 	iters: 600, epoch: 4 | loss: 0.3276343[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2832071)[0m 	speed: 0.0078s/iter; left time: 36.7194s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2832076)[0m Updating learning rate to 1.36800121717672e-05
[36m(_train_fn pid=2832076)[0m saving checkpoint...
[36m(_train_fn pid=2832076)[0m Epoch: 6 cost time: 5.2190163135528564
[36m(_train_fn pid=2832076)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3624397 Vali Loss: 0.2248932 Best vali loss: 0.2217501
[36m(_train_fn pid=2832072)[0m Updating learning rate to 0.00021955332455579228
[36m(_train_fn pid=2832072)[0m saving checkpoint...
[36m(_train_fn pid=2832072)[0m Epoch: 5 cost time: 6.974364280700684
[36m(_train_fn pid=2832072)[0m Epoch: 5, Steps: 529 | Train Loss: 0.4995547 Vali Loss: 0.2704001 Best vali loss: 0.2689202
[36m(_train_fn pid=2832072)[0m EarlyStopping counter: 1 out of 3

Trial status: 6 TERMINATED | 4 RUNNING
Current time: 2024-08-26 13:40:25. Total running time: 1min 0s
Logical resource usage: 4.0/32 CPUs, 0.56/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: d8002_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2832071)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-39-25/checkpoint_000003)
2024-08-26 13:40:27,777	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:27,793	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2832074)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-39-25/checkpoint_000002)
2024-08-26 13:40:27,980	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2832072)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-39-25/checkpoint_000006)[32m [repeated 3x across cluster][0m
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-d8002_00001   RUNNING                 8       6.40637e-05         256              4         64            2   0.00128816   cosine         3           48.2012        0.425221       0.219718            0.215952 â”‚
â”‚ trial-d8002_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          5           53.8895        0.499555       0.2704              0.26892  â”‚
â”‚ trial-d8002_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         2           45.4829        0.287397       0.224377            0.220549 â”‚
â”‚ trial-d8002_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          2           45.4789        0.5709         0.266773            0.266773 â”‚
â”‚ trial-d8002_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           12.8067        0.433672       0.232965            0.219535 â”‚
â”‚ trial-d8002_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            8.83108       0.368788       0.229693            0.229693 â”‚
â”‚ trial-d8002_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           49.1338        0.36244        0.224893            0.22175  â”‚
â”‚ trial-d8002_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          4            4.80876       0.207822       0.250534            0.22575  â”‚
â”‚ trial-d8002_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6           29.6016        0.206613       0.242167            0.222929 â”‚
â”‚ trial-d8002_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5           20.8669        0.315546       0.222023            0.220789 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2832072)[0m 	iters: 400, epoch: 6 | loss: 0.3279505[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2832072)[0m 	speed: 0.0120s/iter; left time: 14.2306s[32m [repeated 16x across cluster][0m

Trial trial-d8002_00001 completed after 4 iterations at 2024-08-26 13:40:27. Total running time: 1min 1s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            11.81087 â”‚
â”‚ time_total_s                                60.01208 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21595 â”‚
â”‚ train_loss                                   0.34303 â”‚
â”‚ valid_loss                                   0.22278 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2832071)[0m Updating learning rate to 3.203185178516828e-05
[36m(_train_fn pid=2832071)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2832071)[0m saving checkpoint...
[36m(_train_fn pid=2832071)[0m Epoch: 4 cost time: 9.88997220993042
[36m(_train_fn pid=2832071)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3430316 Vali Loss: 0.2227829 Best vali loss: 0.2159521
[36m(_train_fn pid=2832071)[0m Early stopping
[36m(_train_fn pid=2832074)[0m 	iters: 400, epoch: 4 | loss: 0.1961623[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2832074)[0m 	speed: 0.0101s/iter; left time: 49.3638s[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2832072)[0m Updating learning rate to 0.00010977666227789614[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2832072)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2832072)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2832072)[0m Epoch: 6 cost time: 6.265549421310425[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2832072)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3591521 Vali Loss: 0.2706647 Best vali loss: 0.2689202[32m [repeated 3x across cluster][0m

Trial trial-d8002_00002 completed after 7 iterations at 2024-08-26 13:40:34. Total running time: 1min 8s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             6.22355 â”‚
â”‚ time_total_s                                67.06186 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26892 â”‚
â”‚ train_loss                                    0.3598 â”‚
â”‚ valid_loss                                    0.2707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2832072)[0m Early stopping

2024-08-26 13:40:37,878	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:37,905	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:37,907	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials' in 0.0020s.
Trial trial-d8002_00004 completed after 4 iterations at 2024-08-26 13:40:37. Total running time: 1min 12s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            10.08907 â”‚
â”‚ time_total_s                                70.69461 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.26677 â”‚
â”‚ train_loss                                   0.61468 â”‚
â”‚ valid_loss                                   0.27059 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-d8002_00003 completed after 4 iterations at 2024-08-26 13:40:37. Total running time: 1min 12s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-d8002_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            10.10956 â”‚
â”‚ time_total_s                                70.68758 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22055 â”‚
â”‚ train_loss                                   0.24506 â”‚
â”‚ valid_loss                                   0.22707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:40:37. Total running time: 1min 12s
Logical resource usage: 1.0/32 CPUs, 0.14/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: d8002_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-d8002_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           12.8067        0.433672       0.232965            0.219535 â”‚
â”‚ trial-d8002_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           60.0121        0.343032       0.222783            0.215952 â”‚
â”‚ trial-d8002_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           67.0619        0.359801       0.270699            0.26892  â”‚
â”‚ trial-d8002_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           70.6876        0.245055       0.22707             0.220549 â”‚
â”‚ trial-d8002_00004   TERMINATED              8       0.00357919          256              3         64            4   0.00325461   type1          4           70.6946        0.614685       0.270593            0.266773 â”‚
â”‚ trial-d8002_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            8.83108       0.368788       0.229693            0.229693 â”‚
â”‚ trial-d8002_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           49.1338        0.36244        0.224893            0.22175  â”‚
â”‚ trial-d8002_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          4            4.80876       0.207822       0.250534            0.22575  â”‚
â”‚ trial-d8002_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6           29.6016        0.206613       0.242167            0.222929 â”‚
â”‚ trial-d8002_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5           20.8669        0.315546       0.222023            0.220789 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2832073)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_7_parallel_trials/trial-d8002_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-39-25/checkpoint_000003)[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2832073)[0m 	iters: 1000, epoch: 4 | loss: 0.2337351[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2832073)[0m 	speed: 0.0067s/iter; left time: 28.6340s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2832074)[0m Updating learning rate to 0.00044739851102008107[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2832074)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2832074)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2832072)[0m Epoch: 7 cost time: 5.391329526901245
[36m(_train_fn pid=2832072)[0m Epoch: 7, Steps: 529 | Train Loss: 0.3598005 Vali Loss: 0.2706989 Best vali loss: 0.2689202
[36m(_train_fn pid=2832073)[0m Updating learning rate to 7.724479248689109e-05
[36m(_train_fn pid=2832073)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2832073)[0m saving checkpoint...


Time taken (7 parallel trials): 77 seconds


2024-08-26 13:40:41,762	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:40:42,136	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:40:42,141	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:40:42,148	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_8_parallel_trials   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator                â”‚
â”‚ Scheduler                        FIFOScheduler                        â”‚
â”‚ Number of trials                 10                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-40-41_329347_2836979/artifacts/2024-08-26_13-40-42/ETTh2_96_96_test_8_parallel_trials/driver_artifacts`

Trial status: 8 PENDING
Current time: 2024-08-26 13:40:42. Total running time: 0s
Logical resource usage: 8.0/32 CPUs, 0.96/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-05af8_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-05af8_00001   PENDING               8       6.40637e-05         256              4         64            2   0.00128816   cosine  â”‚
â”‚ trial-05af8_00002   PENDING              16       0.00351285          512              2         64            4   0.00890605   type1   â”‚
â”‚ trial-05af8_00003   PENDING               8       0.00015449          128              1        512            4   0.00595831   cosine  â”‚
â”‚ trial-05af8_00004   PENDING               8       0.00357919          256              3         64            4   0.00325461   type1   â”‚
â”‚ trial-05af8_00005   PENDING              64       0.000107344          64              1        128            1   0.00131792   type1   â”‚
â”‚ trial-05af8_00006   PENDING              16       0.00043776           32              1        256            2   0.00435286   type1   â”‚
â”‚ trial-05af8_00007   PENDING              32       0.00268133          512              2         64            1   0.0057879    type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-05af8_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-05af8_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00325 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00358 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-05af8_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00129 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-05af8_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00435 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00044 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-05af8_00002 started with configuration:
2024-08-26 13:40:45,934	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2839247)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-40-42/checkpoint_000000)
2024-08-26 13:40:47,067	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2839247)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-40-42/checkpoint_000001)
2024-08-26 13:40:47,443	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:48,189	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:48,198	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:49,388	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:50,167	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:50,499	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:51,675	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2839247)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-40-42/checkpoint_000005)[32m [repeated 7x across cluster][0m
2024-08-26 13:40:52,101	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:52,772	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:52,879	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:53,927	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00891 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00351 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-05af8_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00596 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00015 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2839240)[0m configuration
[36m(_train_fn pid=2839240)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2839240)[0m Use GPU: cuda:0
[36m(_train_fn pid=2839242)[0m {'batch_size': 16, 'learning_rate': 0.0035128531928926765, 'd_model': 512, 'd_core': 64, 'e_layers': 4, 'dropout': 0.00890604745419256, 'lradj': 'type1', 'd_ff': 1024}

Trial trial-05af8_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00579 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00268 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-05af8_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00132 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2839240)[0m train 8449
[36m(_train_fn pid=2839240)[0m val 2785
[36m(_train_fn pid=2839240)[0m start_epoch 0
[36m(_train_fn pid=2839240)[0m max_epoch 8
[36m(_train_fn pid=2839247)[0m 	iters: 100, epoch: 1 | loss: 0.6779016
[36m(_train_fn pid=2839247)[0m 	speed: 0.0134s/iter; left time: 12.9748s
[36m(_train_fn pid=2839247)[0m Updating learning rate to 0.00010734405345907071
[36m(_train_fn pid=2839247)[0m saving checkpoint...
[36m(_train_fn pid=2839247)[0m Validation loss decreased (inf --> 0.2578).  Saving model state dict ...
[36m(_train_fn pid=2839247)[0m Epoch: 1 cost time: 1.259964942932129
[36m(_train_fn pid=2839247)[0m Epoch: 1, Steps: 133 | Train Loss: 0.6779016 Vali Loss: 0.2577833 Best vali loss: 0.2577833
[36m(_train_fn pid=2839247)[0m Updating learning rate to 5.367202672953536e-05
[36m(_train_fn pid=2839247)[0m saving checkpoint...
[36m(_train_fn pid=2839247)[0m Validation loss decreased (0.2578 --> 0.2387).  Saving model state dict ...
[36m(_train_fn pid=2839247)[0m Epoch: 2 cost time: 0.8882770538330078
[36m(_train_fn pid=2839247)[0m Epoch: 2, Steps: 133 | Train Loss: 0.3805931 Vali Loss: 0.2387081 Best vali loss: 0.2387081
[36m(_train_fn pid=2839247)[0m Validation loss decreased (0.2387 --> 0.2336).  Saving model state dict ...
[36m(_train_fn pid=2839247)[0m configuration[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2839241)[0m {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2839247)[0m Use GPU: cuda:0[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2839247)[0m {'batch_size': 64, 'learning_rate': 0.00010734405345907071, 'd_model': 64, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0013179176757202608, 'lradj': 'type1', 'd_ff': 64}[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2839247)[0m train 8449[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2839247)[0m val 2785[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2839247)[0m Validation loss decreased (0.2336 --> 0.2315).  Saving model state dict ...
[36m(_train_fn pid=2839246)[0m start_epoch 0[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2839246)[0m max_epoch 8[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2839245)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2839247)[0m Validation loss decreased (0.2315 --> 0.2305).  Saving model state dict ...
[36m(_train_fn pid=2839247)[0m 	iters: 100, epoch: 5 | loss: 0.4641000[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2839247)[0m 	speed: 0.0114s/iter; left time: 4.9554s[32m [repeated 23x across cluster][0m
[36m(_train_fn pid=2839247)[0m Updating learning rate to 6.70900334119192e-06[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2839247)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2839240)[0m Validation loss decreased (inf --> 0.2195).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2839247)[0m Epoch: 5 cost time: 0.8521842956542969[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2839247)[0m Epoch: 5, Steps: 133 | Train Loss: 0.4641000 Vali Loss: 0.2304989 Best vali loss: 0.2304989[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2839247)[0m Validation loss decreased (0.2305 --> 0.2300).  Saving model state dict ...
[36m(_train_fn pid=2839247)[0m Validation loss decreased (0.2300 --> 0.2298).  Saving model state dict ...

Trial trial-05af8_00005 completed after 8 iterations at 2024-08-26 13:40:53. Total running time: 11s
2024-08-26 13:40:55,154	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:55,392	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:55,665	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:57,911	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:57,938	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2839245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00007_7_alpha_d_ff=2,batch_size=32,d_core=64,d_model=512,dropout=0.0058,e_layers=1,learning_rate=0.0027,lradj=type1_2024-08-26_13-40-42/checkpoint_000004)[32m [repeated 8x across cluster][0m
2024-08-26 13:40:59,028	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:59,211	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:40:59,592	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:00,408	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:01,275	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:02,845	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.15336 â”‚
â”‚ time_total_s                                10.02184 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22969 â”‚
â”‚ train_loss                                   0.36879 â”‚
â”‚ valid_loss                                   0.22969 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2839245)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m

Trial trial-05af8_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00333 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00196 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2841219)[0m configuration
[36m(_train_fn pid=2841219)[0m {'batch_size': 32, 'learning_rate': 0.001963618480113192, 'd_model': 64, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0033338002527237612, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2841219)[0m Use GPU: cuda:0
[36m(_train_fn pid=2839242)[0m 	iters: 400, epoch: 1 | loss: 0.8396374[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2839242)[0m 	speed: 0.0235s/iter; left time: 89.9609s[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2841219)[0m train 8449
[36m(_train_fn pid=2841219)[0m val 2785
[36m(_train_fn pid=2841219)[0m start_epoch 0
[36m(_train_fn pid=2841219)[0m max_epoch 8
[36m(_train_fn pid=2839240)[0m Updating learning rate to 0.00020740251485476345[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2839240)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2839246)[0m Validation loss decreased (inf --> 0.2230).  Saving model state dict ...
[36m(_train_fn pid=2839240)[0m Epoch: 3 cost time: 3.0476157665252686[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2839240)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3195181 Vali Loss: 0.2231508 Best vali loss: 0.2195345[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2839247)[0m Validation loss decreased (0.2298 --> 0.2297).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2841219)[0m Validation loss decreased (inf --> 0.2254).  Saving model state dict ...

Trial trial-05af8_00000 completed after 4 iterations at 2024-08-26 13:40:59. Total running time: 16s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             3.36076 â”‚
â”‚ time_total_s                                15.23581 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21953 â”‚
â”‚ train_loss                                   0.43367 â”‚
â”‚ valid_loss                                   0.23297 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2839240)[0m Early stopping

Trial trial-05af8_00007 completed after 6 iterations at 2024-08-26 13:41:00. Total running time: 18s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                              2.4657 â”‚
â”‚ time_total_s                                16.52851 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22429 â”‚
â”‚ train_loss                                   0.37774 â”‚
â”‚ valid_loss                                   0.23335 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2839245)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2839243)[0m 	iters: 700, epoch: 1 | loss: 0.5347154[32m [repeated 26x across cluster][0m
[36m(_train_fn pid=2839243)[0m 	speed: 0.0184s/iter; left time: 142.8186s[32m [repeated 26x across cluster][0m

Trial trial-05af8_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00128 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00073 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2841759)[0m configuration
[36m(_train_fn pid=2841759)[0m {'batch_size': 32, 'learning_rate': 0.0007335788889365637, 'd_model': 32, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0012802554597501359, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2841759)[0m Use GPU: cuda:0
[36m(_train_fn pid=2841759)[0m train 8449
[36m(_train_fn pid=2841759)[0m val 2785
[36m(_train_fn pid=2841759)[0m start_epoch 0
[36m(_train_fn pid=2841759)[0m max_epoch 8
[36m(_train_fn pid=2839245)[0m Updating learning rate to 8.379160865397222e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2839245)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2839245)[0m Epoch: 6 cost time: 1.8518683910369873[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2839245)[0m Epoch: 6, Steps: 265 | Train Loss: 0.3777368 Vali Loss: 0.2333461 Best vali loss: 0.2242911[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2841219)[0m Validation loss decreased (0.2254 --> 0.2247).  Saving model state dict ...
2024-08-26 13:41:02,959	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2839241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-40-42/checkpoint_000000)[32m [repeated 8x across cluster][0m
2024-08-26 13:41:03,241	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:04,532	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:05,209	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:06,211	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:08,578	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2841759)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-40-59/checkpoint_000001)[32m [repeated 5x across cluster][0m
2024-08-26 13:41:08,927	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:09,119	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:09,202	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:10,625	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:12,009	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2839241)[0m Validation loss decreased (inf --> 0.2160).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2839245)[0m Early stopping
[36m(_train_fn pid=2841219)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2839246)[0m 	iters: 200, epoch: 3 | loss: 0.3344912[32m [repeated 22x across cluster][0m
[36m(_train_fn pid=2839246)[0m 	speed: 0.0117s/iter; left time: 34.6643s[32m [repeated 22x across cluster][0m

Trial trial-05af8_00008 completed after 6 iterations at 2024-08-26 13:41:06. Total running time: 24s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.67553 â”‚
â”‚ time_total_s                                10.66977 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22466 â”‚
â”‚ train_loss                                   0.21964 â”‚
â”‚ valid_loss                                   0.23415 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2841219)[0m Early stopping
[36m(_train_fn pid=2841219)[0m Updating learning rate to 0.00028756526858096616[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2841219)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2841219)[0m Epoch: 6 cost time: 1.433443546295166[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2841219)[0m Epoch: 6, Steps: 265 | Train Loss: 0.2196411 Vali Loss: 0.2341471 Best vali loss: 0.2246650[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2841759)[0m Validation loss decreased (inf --> 0.2216).  Saving model state dict ...
[36m(_train_fn pid=2841759)[0m Validation loss decreased (0.2216 --> 0.2208).  Saving model state dict ...
[36m(_train_fn pid=2839244)[0m Validation loss decreased (inf --> 0.2711).  Saving model state dict ...
[36m(_train_fn pid=2841219)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2841759)[0m 	iters: 200, epoch: 3 | loss: 0.2014326[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2841759)[0m 	speed: 0.0113s/iter; left time: 15.7265s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2839246)[0m Updating learning rate to 0.0001094400973741376[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2839246)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2839246)[0m Epoch: 3 cost time: 5.877053260803223[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2839246)[0m Epoch: 3, Steps: 529 | Train Loss: 0.3808414 Vali Loss: 0.2217501 Best vali loss: 0.2217501[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2841759)[0m EarlyStopping counter: 1 out of 3

Trial status: 4 TERMINATED | 6 RUNNING
Current time: 2024-08-26 13:41:12. Total running time: 30s
Logical resource usage: 6.0/32 CPUs, 0.72/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 05af8_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-05af8_00001   RUNNING                 8       6.40637e-05         256              4         64            2   0.00128816   cosine         1            19.1385       0.319731       0.215952            0.215952 â”‚
â”‚ trial-05af8_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          2            25.363        0.428347       0.278309            0.278309 â”‚
â”‚ trial-05af8_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         1            25.2773       0.409933       0.220549            0.220549 â”‚
â”‚ trial-05af8_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          1            25.1319       0.548985       0.271137            0.271137 â”‚
â”‚ trial-05af8_00006   RUNNING                16       0.00043776           32              1        256            2   0.00435286   type1          3            26.7942       0.380841       0.22175             0.22175  â”‚
â”‚ trial-05af8_00009   RUNNING                32       0.000733579          32              4         64            3   0.00128026   type1          3            11.292        0.244997       0.223119            0.220789 â”‚
â”‚ trial-05af8_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            15.2358       0.433672       0.232965            0.219535 â”‚
â”‚ trial-05af8_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            10.0218       0.368788       0.229693            0.229693 â”‚
â”‚ trial-05af8_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6            16.5285       0.377737       0.233346            0.224291 â”‚
â”‚ trial-05af8_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6            10.6698       0.219641       0.234147            0.224665 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
2024-08-26 13:41:15,640	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2841759)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-40-59/checkpoint_000003)[32m [repeated 6x across cluster][0m
2024-08-26 13:41:17,707	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:18,333	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:19,074	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:19,575	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2839246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-40-42/checkpoint_000004)[32m [repeated 5x across cluster][0m
2024-08-26 13:41:27,950	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:28,151	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:28,393	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:30,422	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2839241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-40-42/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 13:41:31,293	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2839242)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-40-42/checkpoint_000004)[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2839246)[0m Validation loss decreased (0.2230 --> 0.2218).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2839243)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...
[36m(_train_fn pid=2841759)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2839243)[0m 	iters: 400, epoch: 2 | loss: 0.3527046[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2839243)[0m 	speed: 0.0165s/iter; left time: 115.6200s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2841759)[0m Updating learning rate to 9.169736111707046e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2841759)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2841759)[0m Epoch: 4 cost time: 3.156057119369507[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2841759)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4333930 Vali Loss: 0.2220646 Best vali loss: 0.2207892[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2839241)[0m EarlyStopping counter: 1 out of 3

Trial trial-05af8_00009 completed after 5 iterations at 2024-08-26 13:41:19. Total running time: 36s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             3.43174 â”‚
â”‚ time_total_s                                18.35272 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22079 â”‚
â”‚ train_loss                                   0.31555 â”‚
â”‚ valid_loss                                   0.22202 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2841759)[0m Early stopping
[36m(_train_fn pid=2839242)[0m Validation loss decreased (0.2783 --> 0.2703).  Saving model state dict ...
[36m(_train_fn pid=2839242)[0m 	iters: 100, epoch: 4 | loss: 0.4897015[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2839242)[0m 	speed: 0.0342s/iter; left time: 86.9644s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2839242)[0m Updating learning rate to 0.0008782132982231691[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2839242)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2839242)[0m Epoch: 3 cost time: 9.000788450241089[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2839242)[0m Epoch: 3, Steps: 529 | Train Loss: 0.6474589 Vali Loss: 0.2703472 Best vali loss: 0.2703472[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2841759)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2839242)[0m 	iters: 500, epoch: 4 | loss: 0.3747051[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2839242)[0m 	speed: 0.0130s/iter; left time: 28.0001s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2839246)[0m Updating learning rate to 2.73600243435344e-05
[36m(_train_fn pid=2839246)[0m saving checkpoint...
[36m(_train_fn pid=2839246)[0m Epoch: 5 cost time: 5.432770252227783
[36m(_train_fn pid=2839246)[0m Epoch: 5, Steps: 529 | Train Loss: 0.3294062 Vali Loss: 0.2245405 Best vali loss: 0.2217501
[36m(_train_fn pid=2839244)[0m Updating learning rate to 0.0017895940440803243
[36m(_train_fn pid=2839244)[0m saving checkpoint...
[36m(_train_fn pid=2839244)[0m Validation loss decreased (0.2711 --> 0.2668).  Saving model state dict ...
[36m(_train_fn pid=2839244)[0m Epoch: 2 cost time: 16.68700861930847
[36m(_train_fn pid=2839244)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.5709005 Vali Loss: 0.2667734 Best vali loss: 0.2667734
[36m(_train_fn pid=2839246)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2839243)[0m EarlyStopping counter: 1 out of 3

Trial trial-05af8_00006 completed after 6 iterations at 2024-08-26 13:41:31. Total running time: 49s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             6.23643 â”‚
â”‚ time_total_s                                47.45456 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22175 â”‚
â”‚ train_loss                                   0.36244 â”‚
â”‚ valid_loss                                   0.22489 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2839246)[0m Early stopping
[36m(_train_fn pid=2839244)[0m 	iters: 300, epoch: 3 | loss: 0.2605102[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2839244)[0m 	speed: 0.0134s/iter; left time: 80.8024s[32m [repeated 14x across cluster][0m
[36m(_train_fn pid=2839246)[0m Updating learning rate to 1.36800121717672e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2839246)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2839242)[0m Validation loss decreased (0.2703 --> 0.2689).  Saving model state dict ...
[36m(_train_fn pid=2839246)[0m Epoch: 6 cost time: 4.970130205154419[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2839246)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3624397 Vali Loss: 0.2248932 Best vali loss: 0.2217501[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2839246)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2839241)[0m 	iters: 700, epoch: 4 | loss: 0.9401140[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2839241)[0m 	speed: 0.0092s/iter; left time: 42.3609s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2839242)[0m Updating learning rate to 0.00021955332455579228
[36m(_train_fn pid=2839242)[0m saving checkpoint...
[36m(_train_fn pid=2839242)[0m Epoch: 5 cost time: 7.021322250366211
[36m(_train_fn pid=2839242)[0m Epoch: 5, Steps: 529 | Train Loss: 0.4995547 Vali Loss: 0.2704001 Best vali loss: 0.2689202
[36m(_train_fn pid=2839242)[0m EarlyStopping counter: 1 out of 3

Trial status: 6 TERMINATED | 4 RUNNING
Current time: 2024-08-26 13:41:42. Total running time: 1min 0s
Logical resource usage: 4.0/32 CPUs, 0.48/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 05af8_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2839241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-40-42/checkpoint_000003)
2024-08-26 13:41:43,321	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2839244)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-40-42/checkpoint_000002)
2024-08-26 13:41:43,456	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:43,596	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2839242)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-40-42/checkpoint_000006)[32m [repeated 3x across cluster][0m
2024-08-26 13:41:53,394	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:41:53,606	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-05af8_00001   RUNNING                 8       6.40637e-05         256              4         64            2   0.00128816   cosine         3            46.5953       0.425221       0.219718            0.215952 â”‚
â”‚ trial-05af8_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          5            52.661        0.499555       0.2704              0.26892  â”‚
â”‚ trial-05af8_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         2            44.307        0.287397       0.224377            0.220549 â”‚
â”‚ trial-05af8_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          2            44.1516       0.5709         0.266773            0.266773 â”‚
â”‚ trial-05af8_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            15.2358       0.433672       0.232965            0.219535 â”‚
â”‚ trial-05af8_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            10.0218       0.368788       0.229693            0.229693 â”‚
â”‚ trial-05af8_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6            47.4546       0.36244        0.224893            0.22175  â”‚
â”‚ trial-05af8_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6            16.5285       0.377737       0.233346            0.224291 â”‚
â”‚ trial-05af8_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6            10.6698       0.219641       0.234147            0.224665 â”‚
â”‚ trial-05af8_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            18.3527       0.315546       0.222023            0.220789 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-05af8_00001 completed after 4 iterations at 2024-08-26 13:41:42. Total running time: 1min 0s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            12.24502 â”‚
â”‚ time_total_s                                58.84035 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21595 â”‚
â”‚ train_loss                                   0.34303 â”‚
â”‚ valid_loss                                   0.22278 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2839242)[0m 	iters: 500, epoch: 6 | loss: 0.3727880[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2839242)[0m 	speed: 0.0101s/iter; left time: 10.9369s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2839241)[0m Updating learning rate to 3.203185178516828e-05
[36m(_train_fn pid=2839241)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2839241)[0m saving checkpoint...
[36m(_train_fn pid=2839241)[0m Epoch: 4 cost time: 10.24335241317749
[36m(_train_fn pid=2839241)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3430316 Vali Loss: 0.2227829 Best vali loss: 0.2159521
[36m(_train_fn pid=2839241)[0m Early stopping
[36m(_train_fn pid=2839244)[0m 	iters: 500, epoch: 4 | loss: 0.2820254[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2839244)[0m 	speed: 0.0104s/iter; left time: 49.5374s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2839242)[0m Updating learning rate to 0.00010977666227789614[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2839242)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2839242)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2839242)[0m Epoch: 6 cost time: 6.362677335739136[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2839242)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3591521 Vali Loss: 0.2706647 Best vali loss: 0.2689202[32m [repeated 3x across cluster][0m

Trial trial-05af8_00002 completed after 7 iterations at 2024-08-26 13:41:49. Total running time: 1min 7s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             6.25537 â”‚
â”‚ time_total_s                                65.98753 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26892 â”‚
â”‚ train_loss                                    0.3598 â”‚
â”‚ valid_loss                                    0.2707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2839242)[0m Early stopping

[36m(_train_fn pid=2839244)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials/trial-05af8_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-40-42/checkpoint_000004)[32m [repeated 3x across cluster][0m
2024-08-26 13:41:58,685	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_8_parallel_trials' in 0.0092s.
Trial trial-05af8_00003 completed after 4 iterations at 2024-08-26 13:41:53. Total running time: 1min 11s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            10.14739 â”‚
â”‚ time_total_s                                69.75605 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22055 â”‚
â”‚ train_loss                                   0.24506 â”‚
â”‚ valid_loss                                   0.22707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2839243)[0m 	iters: 1000, epoch: 4 | loss: 0.2337351[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2839243)[0m 	speed: 0.0067s/iter; left time: 28.8095s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2839243)[0m Updating learning rate to 7.724479248689109e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2839243)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2839243)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2839243)[0m Epoch: 4 cost time: 9.16761565208435[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2839243)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.2450550 Vali Loss: 0.2270699 Best vali loss: 0.2205494[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2839243)[0m Early stopping

Trial trial-05af8_00004 completed after 5 iterations at 2024-08-26 13:41:58. Total running time: 1min 16s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-05af8_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                              5.2787 â”‚
â”‚ time_total_s                                74.86747 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.26677 â”‚
â”‚ train_loss                                   0.43419 â”‚
â”‚ valid_loss                                   0.27142 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2839244)[0m 	iters: 1000, epoch: 5 | loss: 0.3214265[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2839244)[0m 	speed: 0.0043s/iter; left time: 13.7595s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2839244)[0m Updating learning rate to 0.00022369925551004054
[36m(_train_fn pid=2839244)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2839244)[0m saving checkpoint...

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:41:58. Total running time: 1min 16s
Logical resource usage: 1.0/32 CPUs, 0.12/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 05af8_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-05af8_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            15.2358       0.433672       0.232965            0.219535 â”‚
â”‚ trial-05af8_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4            58.8404       0.343032       0.222783            0.215952 â”‚
â”‚ trial-05af8_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7            65.9875       0.359801       0.270699            0.26892  â”‚
â”‚ trial-05af8_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4            69.7561       0.245055       0.22707             0.220549 â”‚
â”‚ trial-05af8_00004   TERMINATED              8       0.00357919          256              3         64            4   0.00325461   type1          5            74.8675       0.434185       0.271418            0.266773 â”‚
â”‚ trial-05af8_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            10.0218       0.368788       0.229693            0.229693 â”‚
â”‚ trial-05af8_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6            47.4546       0.36244        0.224893            0.22175  â”‚
â”‚ trial-05af8_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6            16.5285       0.377737       0.233346            0.224291 â”‚
â”‚ trial-05af8_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6            10.6698       0.219641       0.234147            0.224665 â”‚
â”‚ trial-05af8_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            18.3527       0.315546       0.222023            0.220789 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}


Time taken (8 parallel trials): 80 seconds


2024-08-26 13:42:02,703	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:42:03,058	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:42:03,063	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:42:03,070	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_9_parallel_trials   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator                â”‚
â”‚ Scheduler                        FIFOScheduler                        â”‚
â”‚ Number of trials                 10                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-42-02_272746_2844396/artifacts/2024-08-26_13-42-03/ETTh2_96_96_test_9_parallel_trials/driver_artifacts`

Trial status: 9 PENDING
Current time: 2024-08-26 13:42:03. Total running time: 0s
Logical resource usage: 9.0/32 CPUs, 0.99/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-35eea_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-35eea_00001   PENDING               8       6.40637e-05         256              4         64            2   0.00128816   cosine  â”‚
â”‚ trial-35eea_00002   PENDING              16       0.00351285          512              2         64            4   0.00890605   type1   â”‚
â”‚ trial-35eea_00003   PENDING               8       0.00015449          128              1        512            4   0.00595831   cosine  â”‚
â”‚ trial-35eea_00004   PENDING               8       0.00357919          256              3         64            4   0.00325461   type1   â”‚
â”‚ trial-35eea_00005   PENDING              64       0.000107344          64              1        128            1   0.00131792   type1   â”‚
â”‚ trial-35eea_00006   PENDING              16       0.00043776           32              1        256            2   0.00435286   type1   â”‚
â”‚ trial-35eea_00007   PENDING              32       0.00268133          512              2         64            1   0.0057879    type1   â”‚
â”‚ trial-35eea_00008   PENDING              32       0.00196362           64              2        512            4   0.0033338    cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-35eea_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00129 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-35eea_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00891 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00351 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2846670)[0m configuration
[36m(_train_fn pid=2846670)[0m {'batch_size': 16, 'learning_rate': 0.0035128531928926765, 'd_model': 512, 'd_core': 64, 'e_layers': 4, 'dropout': 0.00890604745419256, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2846669)[0m {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}

Trial trial-35eea_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00435 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00044 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-35eea_00003 started with configuration:
2024-08-26 13:42:07,033	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846673)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-42-03/checkpoint_000000)
2024-08-26 13:42:08,282	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846673)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-42-03/checkpoint_000001)
2024-08-26 13:42:08,880	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:09,499	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:10,091	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:10,776	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00596 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00015 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-35eea_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-35eea_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00333 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00196 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2846674)[0m Use GPU: cuda:0

Trial trial-35eea_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00579 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00268 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-35eea_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00325 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00358 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-35eea_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00132 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2846670)[0m train 8449
[36m(_train_fn pid=2846670)[0m val 2785
[36m(_train_fn pid=2846669)[0m start_epoch 0
[36m(_train_fn pid=2846669)[0m max_epoch 8
[36m(_train_fn pid=2846673)[0m 	iters: 100, epoch: 1 | loss: 0.6779016
[36m(_train_fn pid=2846673)[0m 	speed: 0.0147s/iter; left time: 14.1760s
[36m(_train_fn pid=2846673)[0m Updating learning rate to 0.00010734405345907071
[36m(_train_fn pid=2846673)[0m saving checkpoint...
[36m(_train_fn pid=2846673)[0m Validation loss decreased (inf --> 0.2578).  Saving model state dict ...
[36m(_train_fn pid=2846673)[0m Epoch: 1 cost time: 1.3961212635040283
[36m(_train_fn pid=2846673)[0m Epoch: 1, Steps: 133 | Train Loss: 0.6779016 Vali Loss: 0.2577833 Best vali loss: 0.2577833
[36m(_train_fn pid=2846673)[0m Updating learning rate to 5.367202672953536e-05
[36m(_train_fn pid=2846673)[0m saving checkpoint...
[36m(_train_fn pid=2846673)[0m Validation loss decreased (0.2578 --> 0.2387).  Saving model state dict ...
[36m(_train_fn pid=2846673)[0m Epoch: 2 cost time: 0.9865410327911377
[36m(_train_fn pid=2846673)[0m Epoch: 2, Steps: 133 | Train Loss: 0.3805931 Vali Loss: 0.2387081 Best vali loss: 0.2387081
[36m(_train_fn pid=2846673)[0m Validation loss decreased (0.2387 --> 0.2336).  Saving model state dict ...
[36m(_train_fn pid=2846675)[0m configuration[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2846675)[0m {'batch_size': 32, 'learning_rate': 0.002681331476927111, 'd_model': 512, 'd_core': 64, 'e_layers': 1, 'dropout': 0.005787899105404264, 'lradj': 'type1', 'd_ff': 1024}[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2846676)[0m {'batch_size': 32, 'learning_rate': 0.001963618480113192, 'd_model': 64, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0033338002527237612, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2846675)[0m Use GPU: cuda:0[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2846675)[0m train 8449[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2846675)[0m val 2785[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2846675)[0m start_epoch 0[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2846675)[0m max_epoch 8[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2846673)[0m Validation loss decreased (0.2336 --> 0.2315).  Saving model state dict ...
2024-08-26 13:42:11,962	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:12,014	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846673)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-42-03/checkpoint_000004)[32m [repeated 6x across cluster][0m
2024-08-26 13:42:12,666	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:13,306	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:14,419	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:14,601	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:15,061	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:15,866	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:17,614	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846674)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-42-03/checkpoint_000000)[32m [repeated 7x across cluster][0m
2024-08-26 13:42:17,975	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:18,484	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:19,207	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:19,576	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:20,855	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:21,045	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:22,030	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:22,296	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:22,428	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:23,763	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846675)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00007_7_alpha_d_ff=2,batch_size=32,d_core=64,d_model=512,dropout=0.0058,e_layers=1,learning_rate=0.0027,lradj=type1_2024-08-26_13-42-03/checkpoint_000005)[32m [repeated 10x across cluster][0m
2024-08-26 13:42:23,872	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846668)[0m 	iters: 100, epoch: 2 | loss: 0.5224623[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=2846668)[0m 	speed: 0.0297s/iter; left time: 52.1973s[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=2846675)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2846673)[0m Validation loss decreased (0.2315 --> 0.2305).  Saving model state dict ...
[36m(_train_fn pid=2846673)[0m Updating learning rate to 6.70900334119192e-06[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2846673)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2846668)[0m Validation loss decreased (inf --> 0.2195).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2846673)[0m Epoch: 5 cost time: 0.9815578460693359[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2846673)[0m Epoch: 5, Steps: 133 | Train Loss: 0.4641000 Vali Loss: 0.2304989 Best vali loss: 0.2304989[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2846673)[0m Validation loss decreased (0.2305 --> 0.2300).  Saving model state dict ...
[36m(_train_fn pid=2846673)[0m Validation loss decreased (0.2300 --> 0.2298).  Saving model state dict ...

Trial trial-35eea_00005 completed after 8 iterations at 2024-08-26 13:42:15. Total running time: 12s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.26214 â”‚
â”‚ time_total_s                                11.01457 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22969 â”‚
â”‚ train_loss                                   0.36879 â”‚
â”‚ valid_loss                                   0.22969 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2846670)[0m 	iters: 400, epoch: 1 | loss: 0.8396374[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2846670)[0m 	speed: 0.0276s/iter; left time: 105.9344s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2846668)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2846673)[0m Updating learning rate to 8.3862541764899e-07[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2846673)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2846676)[0m Validation loss decreased (inf --> 0.2248).  Saving model state dict ...
[36m(_train_fn pid=2846673)[0m Epoch: 8 cost time: 0.9812333583831787[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2846673)[0m Epoch: 8, Steps: 133 | Train Loss: 0.3687880 Vali Loss: 0.2296926 Best vali loss: 0.2296926[32m [repeated 6x across cluster][0m

Trial trial-35eea_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00128 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00073 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2848849)[0m configuration
[36m(_train_fn pid=2848849)[0m {'batch_size': 32, 'learning_rate': 0.0007335788889365637, 'd_model': 32, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0012802554597501359, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2848849)[0m Use GPU: cuda:0
[36m(_train_fn pid=2846674)[0m Validation loss decreased (inf --> 0.2230).  Saving model state dict ...
[36m(_train_fn pid=2848849)[0m train 8449
[36m(_train_fn pid=2848849)[0m val 2785
[36m(_train_fn pid=2846675)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2848849)[0m start_epoch 0
[36m(_train_fn pid=2848849)[0m max_epoch 8
[36m(_train_fn pid=2846673)[0m Validation loss decreased (0.2298 --> 0.2297).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2846672)[0m 	iters: 600, epoch: 1 | loss: 0.2172278[32m [repeated 25x across cluster][0m
[36m(_train_fn pid=2846672)[0m 	speed: 0.0237s/iter; left time: 186.0092s[32m [repeated 25x across cluster][0m

Trial trial-35eea_00000 completed after 4 iterations at 2024-08-26 13:42:22. Total running time: 19s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             3.80957 â”‚
â”‚ time_total_s                                17.47474 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21953 â”‚
â”‚ train_loss                                   0.43367 â”‚
â”‚ valid_loss                                   0.23297 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2846668)[0m Early stopping
[36m(_train_fn pid=2846668)[0m Updating learning rate to 0.00015[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2846668)[0m saving checkpoint...[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2846668)[0m Epoch: 4 cost time: 3.1823832988739014[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2846668)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4336717 Vali Loss: 0.2329654 Best vali loss: 0.2195345[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2846670)[0m Validation loss decreased (inf --> 0.2859).  Saving model state dict ...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2848849)[0m EarlyStopping counter: 1 out of 3[32m [repeated 5x across cluster][0m

Trial trial-35eea_00007 completed after 6 iterations at 2024-08-26 13:42:23. Total running time: 20s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             2.90635 â”‚
â”‚ time_total_s                                18.92529 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22429 â”‚
â”‚ train_loss                                   0.37774 â”‚
â”‚ valid_loss                                   0.23335 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
2024-08-26 13:42:24,853	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:25,196	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:26,526	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:26,933	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:29,379	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846676)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-42-03/checkpoint_000003)[32m [repeated 6x across cluster][0m
2024-08-26 13:42:32,884	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:32,906	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:32,966	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:33,553	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:34,510	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846674)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-42-03/checkpoint_000002)[32m [repeated 5x across cluster][0m
2024-08-26 13:42:38,214	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:41,259	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846669)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-42-03/checkpoint_000001)[32m [repeated 2x across cluster][0m
2024-08-26 13:42:41,972	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:42,806	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846674)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-42-03/checkpoint_000004)[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2848849)[0m Validation loss decreased (0.2216 --> 0.2208).  Saving model state dict ...
[36m(_train_fn pid=2846676)[0m Validation loss decreased (0.2248 --> 0.2229).  Saving model state dict ...

Trial trial-35eea_00009 completed after 5 iterations at 2024-08-26 13:42:25. Total running time: 22s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.32159 â”‚
â”‚ time_total_s                                 7.66824 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22079 â”‚
â”‚ train_loss                                   0.31516 â”‚
â”‚ valid_loss                                   0.22207 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2846676)[0m 	iters: 100, epoch: 4 | loss: 0.6417434[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2846676)[0m 	speed: 0.0312s/iter; left time: 38.2347s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2848849)[0m Early stopping[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2846674)[0m Updating learning rate to 0.0002188801947482752[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2846674)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2846674)[0m Epoch: 2 cost time: 7.837486743927002[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2846674)[0m Epoch: 2, Steps: 529 | Train Loss: 0.3621016 Vali Loss: 0.2247194 Best vali loss: 0.2229993[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2846669)[0m Validation loss decreased (inf --> 0.2160).  Saving model state dict ...
[36m(_train_fn pid=2846674)[0m EarlyStopping counter: 1 out of 3[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2846676)[0m 	iters: 200, epoch: 5 | loss: 0.3025218[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2846676)[0m 	speed: 0.0133s/iter; left time: 11.4439s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2846676)[0m Updating learning rate to 0.000981809240056596
[36m(_train_fn pid=2846676)[0m saving checkpoint...
[36m(_train_fn pid=2846676)[0m Epoch: 4 cost time: 4.009237289428711
[36m(_train_fn pid=2846676)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4213992 Vali Loss: 0.2426261 Best vali loss: 0.2229292
[36m(_train_fn pid=2846671)[0m Updating learning rate to 0.00014860967525861138
[36m(_train_fn pid=2846671)[0m saving checkpoint...
[36m(_train_fn pid=2846671)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...
[36m(_train_fn pid=2846671)[0m Epoch: 1 cost time: 24.76735830307007
[36m(_train_fn pid=2846671)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.4099332 Vali Loss: 0.2205494 Best vali loss: 0.2205494
[36m(_train_fn pid=2846670)[0m Validation loss decreased (0.2859 --> 0.2783).  Saving model state dict ...

Trial status: 4 TERMINATED | 6 RUNNING
Current time: 2024-08-26 13:42:33. Total running time: 30s
Logical resource usage: 6.0/32 CPUs, 0.66/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 35eea_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-35eea_00001   RUNNING                 8       6.40637e-05         256              4         64            2   0.00128816   cosine         1           21.8172        0.319731       0.215952            0.215952 â”‚
â”‚ trial-35eea_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          2           28.2488        0.428347       0.278309            0.278309 â”‚
â”‚ trial-35eea_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         1           28.0945        0.409933       0.220549            0.220549 â”‚
â”‚ trial-35eea_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          1           28.0826        0.548985       0.271137            0.271137 â”‚
â”‚ trial-35eea_00006   RUNNING                16       0.00043776           32              1        256            2   0.00435286   type1          2           22.1488        0.362102       0.224719            0.222999 â”‚
â”‚ trial-35eea_00008   RUNNING                32       0.00196362           64              2        512            4   0.0033338    cosine         4           24.5548        0.421399       0.242626            0.222929 â”‚
â”‚ trial-35eea_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           17.4747        0.433672       0.232965            0.219535 â”‚
â”‚ trial-35eea_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8           11.0146        0.368788       0.229693            0.229693 â”‚
â”‚ trial-35eea_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6           18.9253        0.377737       0.233346            0.224291 â”‚
â”‚ trial-35eea_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            7.66824       0.315164       0.222069            0.220791 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2846676)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2846676)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2846669)[0m 	iters: 1000, epoch: 2 | loss: 0.2932285[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2846669)[0m 	speed: 0.0125s/iter; left time: 80.1162s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2846674)[0m Updating learning rate to 0.0001094400973741376[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2846674)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2846672)[0m Validation loss decreased (inf --> 0.2711).  Saving model state dict ...
[36m(_train_fn pid=2846674)[0m Epoch: 3 cost time: 6.05092191696167[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2846674)[0m Epoch: 3, Steps: 529 | Train Loss: 0.3808414 Vali Loss: 0.2217501 Best vali loss: 0.2217501[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2846674)[0m Validation loss decreased (0.2230 --> 0.2218).  Saving model state dict ...

Trial trial-35eea_00008 completed after 6 iterations at 2024-08-26 13:42:38. Total running time: 35s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             4.65768 â”‚
â”‚ time_total_s                                33.38412 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22293 â”‚
â”‚ train_loss                                   0.20661 â”‚
â”‚ valid_loss                                   0.24217 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2846676)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2846676)[0m Early stopping
[36m(_train_fn pid=2846669)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2846670)[0m Validation loss decreased (0.2783 --> 0.2703).  Saving model state dict ...
[36m(_train_fn pid=2846669)[0m 	iters: 200, epoch: 3 | loss: 0.6524662[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2846669)[0m 	speed: 0.0096s/iter; left time: 59.0067s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2846670)[0m Updating learning rate to 0.0008782132982231691[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2846670)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2846670)[0m Epoch: 3 cost time: 8.589209079742432[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2846670)[0m Epoch: 3, Steps: 529 | Train Loss: 0.6474589 Vali Loss: 0.2703472 Best vali loss: 0.2703472[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2846674)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2846669)[0m 	iters: 700, epoch: 3 | loss: 0.1809365[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2846669)[0m 	speed: 0.0103s/iter; left time: 58.2633s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2846674)[0m Updating learning rate to 2.73600243435344e-05
[36m(_train_fn pid=2846674)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2846674)[0m saving checkpoint...
[36m(_train_fn pid=2846674)[0m Epoch: 5 cost time: 5.510176420211792
2024-08-26 13:42:51,407	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:51,425	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:51,740	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:42:54,165	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846669)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-42-03/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 13:42:55,341	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:00,055	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846670)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-42-03/checkpoint_000004)[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2846669)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-42-03/checkpoint_000003)
2024-08-26 13:43:07,013	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:07,039	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846671)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-42-03/checkpoint_000002)
2024-08-26 13:43:07,214	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2846670)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-42-03/checkpoint_000006)[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2846674)[0m Epoch: 5, Steps: 529 | Train Loss: 0.3294062 Vali Loss: 0.2245405 Best vali loss: 0.2217501
[36m(_train_fn pid=2846672)[0m Validation loss decreased (0.2711 --> 0.2668).  Saving model state dict ...
[36m(_train_fn pid=2846674)[0m 	iters: 500, epoch: 6 | loss: 0.1605241[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2846674)[0m 	speed: 0.0109s/iter; left time: 11.8707s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2846669)[0m Updating learning rate to 4.428991077132631e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2846669)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2846669)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2846669)[0m Epoch: 3 cost time: 10.430862426757812[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2846669)[0m Epoch: 3, Steps: 1057 | Train Loss: 0.4252212 Vali Loss: 0.2197178 Best vali loss: 0.2159521[32m [repeated 4x across cluster][0m

Trial trial-35eea_00006 completed after 6 iterations at 2024-08-26 13:42:55. Total running time: 52s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             6.50528 â”‚
â”‚ time_total_s                                50.54585 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22175 â”‚
â”‚ train_loss                                   0.36244 â”‚
â”‚ valid_loss                                   0.22489 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2846674)[0m Early stopping
[36m(_train_fn pid=2846670)[0m Validation loss decreased (0.2703 --> 0.2689).  Saving model state dict ...
[36m(_train_fn pid=2846669)[0m 	iters: 500, epoch: 4 | loss: 0.2776805[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2846669)[0m 	speed: 0.0093s/iter; left time: 44.5742s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2846674)[0m Updating learning rate to 1.36800121717672e-05
[36m(_train_fn pid=2846674)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2846674)[0m saving checkpoint...
[36m(_train_fn pid=2846674)[0m Epoch: 6 cost time: 5.2149248123168945
[36m(_train_fn pid=2846674)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3624397 Vali Loss: 0.2248932 Best vali loss: 0.2217501
[36m(_train_fn pid=2846670)[0m Updating learning rate to 0.00021955332455579228
[36m(_train_fn pid=2846670)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2846670)[0m saving checkpoint...
[36m(_train_fn pid=2846670)[0m Epoch: 5 cost time: 7.191664457321167
[36m(_train_fn pid=2846670)[0m Epoch: 5, Steps: 529 | Train Loss: 0.4995547 Vali Loss: 0.2704001 Best vali loss: 0.2689202

Trial status: 6 TERMINATED | 4 RUNNING
Current time: 2024-08-26 13:43:03. Total running time: 1min 0s
Logical resource usage: 4.0/32 CPUs, 0.44/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 35eea_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-35eea_00001   RUNNING                 8       6.40637e-05         256              4         64            2   0.00128816   cosine         3           49.4489        0.425221       0.219718            0.215952 â”‚
â”‚ trial-35eea_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          5           55.3169        0.499555       0.2704              0.26892  â”‚
â”‚ trial-35eea_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         2           46.6318        0.287397       0.224377            0.220549 â”‚
â”‚ trial-35eea_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          2           46.5805        0.5709         0.266773            0.266773 â”‚
â”‚ trial-35eea_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           17.4747        0.433672       0.232965            0.219535 â”‚
â”‚ trial-35eea_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8           11.0146        0.368788       0.229693            0.229693 â”‚
â”‚ trial-35eea_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           50.5458        0.36244        0.224893            0.22175  â”‚
â”‚ trial-35eea_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6           18.9253        0.377737       0.233346            0.224291 â”‚
â”‚ trial-35eea_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6           33.3841        0.206613       0.242167            0.222929 â”‚
â”‚ trial-35eea_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            7.66824       0.315164       0.222069            0.220791 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2846672)[0m 	iters: 1000, epoch: 3 | loss: 0.5788560[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2846672)[0m 	speed: 0.0129s/iter; left time: 69.1255s[32m [repeated 17x across cluster][0m

Trial trial-35eea_00001 completed after 4 iterations at 2024-08-26 13:43:06. Total running time: 1min 3s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            12.14133 â”‚
â”‚ time_total_s                                61.59019 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21595 â”‚
â”‚ train_loss                                   0.34303 â”‚
â”‚ valid_loss                                   0.22278 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2846669)[0m Updating learning rate to 3.203185178516828e-05
[36m(_train_fn pid=2846669)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2846669)[0m saving checkpoint...
[36m(_train_fn pid=2846669)[0m Epoch: 4 cost time: 10.184232711791992
[36m(_train_fn pid=2846669)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3430316 Vali Loss: 0.2227829 Best vali loss: 0.2159521
[36m(_train_fn pid=2846669)[0m Early stopping
[36m(_train_fn pid=2846671)[0m 	iters: 300, epoch: 4 | loss: 0.2099339[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2846671)[0m 	speed: 0.0103s/iter; left time: 51.1914s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2846670)[0m Updating learning rate to 0.00010977666227789614[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2846670)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2846670)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2846670)[0m Epoch: 6 cost time: 6.4546027183532715[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2846670)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3591521 Vali Loss: 0.2706647 Best vali loss: 0.2689202[32m [repeated 3x across cluster][0m

Trial trial-35eea_00002 completed after 7 iterations at 2024-08-26 13:43:13. Total running time: 1min 10s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             6.37896 â”‚
â”‚ time_total_s                                68.85187 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26892 â”‚
â”‚ train_loss                                    0.3598 â”‚
â”‚ valid_loss                                    0.2707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2846670)[0m Early stopping
2024-08-26 13:43:17,204	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:17,246	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:17,248	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials' in 0.0022s.
[36m(_train_fn pid=2846672)[0m 	iters: 900, epoch: 4 | loss: 1.6161311[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2846672)[0m 	speed: 0.0065s/iter; left time: 28.3914s[32m [repeated 15x across cluster][0m

Trial trial-35eea_00004 completed after 4 iterations at 2024-08-26 13:43:17. Total running time: 1min 14s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            10.18793 â”‚
â”‚ time_total_s                                72.36984 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.26677 â”‚
â”‚ train_loss                                   0.61468 â”‚
â”‚ valid_loss                                   0.27059 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-35eea_00003 completed after 4 iterations at 2024-08-26 13:43:17. Total running time: 1min 14s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-35eea_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            10.20454 â”‚
â”‚ time_total_s                                  72.449 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22055 â”‚
â”‚ train_loss                                   0.24506 â”‚
â”‚ valid_loss                                   0.22707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:43:17. Total running time: 1min 14s
Logical resource usage: 1.0/32 CPUs, 0.11/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 35eea_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-35eea_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4           17.4747        0.433672       0.232965            0.219535 â”‚
â”‚ trial-35eea_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           61.5902        0.343032       0.222783            0.215952 â”‚
â”‚ trial-35eea_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           68.8519        0.359801       0.270699            0.26892  â”‚
â”‚ trial-35eea_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           72.449         0.245055       0.22707             0.220549 â”‚
â”‚ trial-35eea_00004   TERMINATED              8       0.00357919          256              3         64            4   0.00325461   type1          4           72.3698        0.614685       0.270593            0.266773 â”‚
â”‚ trial-35eea_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8           11.0146        0.368788       0.229693            0.229693 â”‚
â”‚ trial-35eea_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           50.5458        0.36244        0.224893            0.22175  â”‚
â”‚ trial-35eea_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6           18.9253        0.377737       0.233346            0.224291 â”‚
â”‚ trial-35eea_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6           33.3841        0.206613       0.242167            0.222929 â”‚
â”‚ trial-35eea_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            7.66824       0.315164       0.222069            0.220791 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2846672)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_9_parallel_trials/trial-35eea_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-42-03/checkpoint_000003)[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2846671)[0m Updating learning rate to 7.724479248689109e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2846671)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2846671)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2846670)[0m Epoch: 7 cost time: 5.527754306793213
[36m(_train_fn pid=2846670)[0m Epoch: 7, Steps: 529 | Train Loss: 0.3598005 Vali Loss: 0.2706989 Best vali loss: 0.2689202
[36m(_train_fn pid=2846671)[0m 	iters: 1000, epoch: 4 | loss: 0.2337351[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2846671)[0m 	speed: 0.0065s/iter; left time: 28.0456s[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2846672)[0m Updating learning rate to 0.00044739851102008107
[36m(_train_fn pid=2846672)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2846672)[0m saving checkpoint...


Time taken (9 parallel trials): 79 seconds


2024-08-26 13:43:21,379	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:43:21,746	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:43:21,751	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:43:21,759	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test_10_parallel_trials   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator                 â”‚
â”‚ Scheduler                        FIFOScheduler                         â”‚
â”‚ Number of trials                 10                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-43-20_945373_2851796/artifacts/2024-08-26_13-43-21/ETTh2_96_96_test_10_parallel_trials/driver_artifacts`

Trial status: 10 PENDING
Current time: 2024-08-26 13:43:21. Total running time: 0s
Logical resource usage: 10.0/32 CPUs, 0.9999999999999999/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-64d31_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-64d31_00001   PENDING               8       6.40637e-05         256              4         64            2   0.00128816   cosine  â”‚
â”‚ trial-64d31_00002   PENDING              16       0.00351285          512              2         64            4   0.00890605   type1   â”‚
â”‚ trial-64d31_00003   PENDING               8       0.00015449          128              1        512            4   0.00595831   cosine  â”‚
â”‚ trial-64d31_00004   PENDING               8       0.00357919          256              3         64            4   0.00325461   type1   â”‚
â”‚ trial-64d31_00005   PENDING              64       0.000107344          64              1        128            1   0.00131792   type1   â”‚
â”‚ trial-64d31_00006   PENDING              16       0.00043776           32              1        256            2   0.00435286   type1   â”‚
â”‚ trial-64d31_00007   PENDING              32       0.00268133          512              2         64            1   0.0057879    type1   â”‚
â”‚ trial-64d31_00008   PENDING              32       0.00196362           64              2        512            4   0.0033338    cosine  â”‚
â”‚ trial-64d31_00009   PENDING              32       0.000733579          32              4         64            3   0.00128026   type1   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-64d31_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00435 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00044 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-64d31_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00325 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00358 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2854086)[0m configuration
[36m(_train_fn pid=2854086)[0m {'batch_size': 16, 'learning_rate': 0.0004377603894965504, 'd_model': 32, 'd_core': 256, 'e_layers': 2, 'dropout': 0.00435286364938959, 'lradj': 'type1', 'd_ff': 32}
[36m(_train_fn pid=2854086)[0m Use GPU: cuda:0

Trial trial-64d31_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00129 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-64d31_00002 started with configuration:
2024-08-26 13:43:25,838	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854087)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-43-21/checkpoint_000000)
2024-08-26 13:43:27,237	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854087)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-43-21/checkpoint_000001)
2024-08-26 13:43:27,915	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00891 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00351 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-64d31_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-64d31_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00596 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00015 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-64d31_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00128 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00073 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-64d31_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00132 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2854086)[0m train 8449
[36m(_train_fn pid=2854086)[0m val 2785
[36m(_train_fn pid=2854085)[0m {'batch_size': 8, 'learning_rate': 0.00015448958497378217, 'd_model': 128, 'd_core': 512, 'e_layers': 4, 'dropout': 0.005958307406826554, 'lradj': 'cosine', 'd_ff': 128}

Trial trial-64d31_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00333 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00196 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-64d31_00007 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00579 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00268 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2854086)[0m start_epoch 0
[36m(_train_fn pid=2854086)[0m max_epoch 8
[36m(_train_fn pid=2854087)[0m 	iters: 100, epoch: 1 | loss: 0.6779016
[36m(_train_fn pid=2854087)[0m 	speed: 0.0147s/iter; left time: 14.1826s
[36m(_train_fn pid=2854087)[0m Updating learning rate to 0.00010734405345907071
[36m(_train_fn pid=2854087)[0m saving checkpoint...
[36m(_train_fn pid=2854087)[0m Validation loss decreased (inf --> 0.2578).  Saving model state dict ...
[36m(_train_fn pid=2854087)[0m Epoch: 1 cost time: 1.4347543716430664
[36m(_train_fn pid=2854087)[0m Epoch: 1, Steps: 133 | Train Loss: 0.6779016 Vali Loss: 0.2577833 Best vali loss: 0.2577833
[36m(_train_fn pid=2854087)[0m Updating learning rate to 5.367202672953536e-05
[36m(_train_fn pid=2854087)[0m saving checkpoint...
[36m(_train_fn pid=2854087)[0m Validation loss decreased (0.2578 --> 0.2387).  Saving model state dict ...
[36m(_train_fn pid=2854087)[0m Epoch: 2 cost time: 1.0993702411651611
[36m(_train_fn pid=2854087)[0m Epoch: 2, Steps: 133 | Train Loss: 0.3805931 Vali Loss: 0.2387081 Best vali loss: 0.2387081
[36m(_train_fn pid=2854089)[0m configuration[32m [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
2024-08-26 13:43:28,513	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:28,880	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:29,878	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:30,487	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:31,178	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:31,219	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854087)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-43-21/checkpoint_000004)[32m [repeated 6x across cluster][0m
2024-08-26 13:43:32,035	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:32,540	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:33,536	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:33,889	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:34,564	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:35,195	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854091)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-43-21/checkpoint_000001)[32m [repeated 8x across cluster][0m
2024-08-26 13:43:36,268	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:36,721	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:37,579	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:37,798	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:39,033	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:40,710	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:41,559	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854091)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-43-21/checkpoint_000002)[32m [repeated 6x across cluster][0m
2024-08-26 13:43:41,895	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:42,598	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:43,678	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:44,955	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:45,885	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854089)[0m {'batch_size': 32, 'learning_rate': 0.002681331476927111, 'd_model': 512, 'd_core': 64, 'e_layers': 1, 'dropout': 0.005787899105404264, 'lradj': 'type1', 'd_ff': 1024}[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2854091)[0m Use GPU: cuda:0[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2854087)[0m Validation loss decreased (0.2387 --> 0.2336).  Saving model state dict ...
[36m(_train_fn pid=2854091)[0m train 8449[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2854082)[0m val 2785[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2854090)[0m {'batch_size': 32, 'learning_rate': 0.001963618480113192, 'd_model': 64, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0033338002527237612, 'lradj': 'cosine', 'd_ff': 128}[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854091)[0m start_epoch 0[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2854091)[0m max_epoch 8[32m [repeated 9x across cluster][0m
[36m(_train_fn pid=2854087)[0m Validation loss decreased (0.2336 --> 0.2315).  Saving model state dict ...
[36m(_train_fn pid=2854082)[0m 	iters: 100, epoch: 2 | loss: 0.5224623[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=2854082)[0m 	speed: 0.0316s/iter; left time: 55.4857s[32m [repeated 24x across cluster][0m
[36m(_train_fn pid=2854091)[0m Updating learning rate to 0.0007335788889365637[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2854091)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2854091)[0m Validation loss decreased (inf --> 0.2216).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854091)[0m Epoch: 1 cost time: 5.626923561096191[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2854091)[0m Epoch: 1, Steps: 265 | Train Loss: 0.6565566 Vali Loss: 0.2215732 Best vali loss: 0.2215732[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2854087)[0m Validation loss decreased (0.2315 --> 0.2305).  Saving model state dict ...
[36m(_train_fn pid=2854089)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2854087)[0m Validation loss decreased (0.2305 --> 0.2300).  Saving model state dict ...
[36m(_train_fn pid=2854087)[0m Validation loss decreased (0.2300 --> 0.2298).  Saving model state dict ...

Trial trial-64d31_00005 completed after 8 iterations at 2024-08-26 13:43:35. Total running time: 13s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                             1.30241 â”‚
â”‚ time_total_s                                11.60343 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22969 â”‚
â”‚ train_loss                                   0.36879 â”‚
â”‚ valid_loss                                   0.22969 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2854089)[0m 	iters: 100, epoch: 4 | loss: 0.2127360[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2854089)[0m 	speed: 0.0230s/iter; left time: 28.2565s[32m [repeated 21x across cluster][0m
[36m(_train_fn pid=2854087)[0m Updating learning rate to 8.3862541764899e-07[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2854087)[0m saving checkpoint...[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2854090)[0m Validation loss decreased (inf --> 0.2248).  Saving model state dict ...
[36m(_train_fn pid=2854087)[0m Epoch: 8 cost time: 1.0247740745544434[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2854087)[0m Epoch: 8, Steps: 133 | Train Loss: 0.3687880 Vali Loss: 0.2296926 Best vali loss: 0.2296926[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2854082)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2854086)[0m Validation loss decreased (inf --> 0.2230).  Saving model state dict ...
[36m(_train_fn pid=2854089)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2854091)[0m Validation loss decreased (0.2216 --> 0.2208).  Saving model state dict ...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854082)[0m 	iters: 200, epoch: 4 | loss: 0.3015919[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2854082)[0m 	speed: 0.0128s/iter; left time: 14.4044s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2854089)[0m Updating learning rate to 0.00016758321730794444[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2854089)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2854089)[0m Epoch: 5 cost time: 2.435117244720459[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2854089)[0m Epoch: 5, Steps: 265 | Train Loss: 0.3267840 Vali Loss: 0.2318944 Best vali loss: 0.2242911[32m [repeated 6x across cluster][0m

Trial trial-64d31_00000 completed after 4 iterations at 2024-08-26 13:43:41. Total running time: 20s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             4.09421 â”‚
â”‚ time_total_s                                18.35862 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21953 â”‚
â”‚ train_loss                                   0.43367 â”‚
â”‚ valid_loss                                   0.23297 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2854082)[0m Early stopping
[36m(_train_fn pid=2854084)[0m Validation loss decreased (inf --> 0.2859).  Saving model state dict ...
[36m(_train_fn pid=2854082)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m

Trial trial-64d31_00007 completed after 6 iterations at 2024-08-26 13:43:43. Total running time: 21s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             2.96484 â”‚
â”‚ time_total_s                                 20.0593 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22429 â”‚
â”‚ train_loss                                   0.37774 â”‚
â”‚ valid_loss                                   0.23335 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2854090)[0m Validation loss decreased (0.2248 --> 0.2229).  Saving model state dict ...
[36m(_train_fn pid=2854085)[0m 	iters: 800, epoch: 1 | loss: 0.3597501[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2854085)[0m 	speed: 0.0188s/iter; left time: 144.1121s[32m [repeated 18x across cluster][0m
2024-08-26 13:43:46,960	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854086)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-43-21/checkpoint_000001)[32m [repeated 6x across cluster][0m
2024-08-26 13:43:47,027	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:49,979	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:49,999	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:53,356	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854088)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-43-21/checkpoint_000000)[32m [repeated 4x across cluster][0m
2024-08-26 13:43:53,719	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:54,109	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:54,129	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:43:54,772	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854091)[0m Updating learning rate to 9.169736111707046e-05[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2854091)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2854091)[0m Epoch: 4 cost time: 3.847182273864746[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2854091)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4333930 Vali Loss: 0.2220646 Best vali loss: 0.2207892[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2854089)[0m Early stopping
[36m(_train_fn pid=2854083)[0m Validation loss decreased (inf --> 0.2160).  Saving model state dict ...
[36m(_train_fn pid=2854086)[0m EarlyStopping counter: 1 out of 3[32m [repeated 3x across cluster][0m

Trial trial-64d31_00009 completed after 5 iterations at 2024-08-26 13:43:50. Total running time: 28s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             4.10953 â”‚
â”‚ time_total_s                                26.41744 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22079 â”‚
â”‚ train_loss                                   0.31555 â”‚
â”‚ valid_loss                                   0.22202 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2854091)[0m Early stopping
[36m(_train_fn pid=2854086)[0m 	iters: 300, epoch: 3 | loss: 0.3420315[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2854086)[0m 	speed: 0.0121s/iter; left time: 34.6455s[32m [repeated 16x across cluster][0m

Trial status: 4 TERMINATED | 6 RUNNING
Current time: 2024-08-26 13:43:51. Total running time: 30s
Logical resource usage: 6.0/32 CPUs, 0.6/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 64d31_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-64d31_00001   RUNNING                 8       6.40637e-05         256              4         64            2   0.00128816   cosine         1            23.5342       0.319731       0.215952            0.215952 â”‚
â”‚ trial-64d31_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          1            19.0997       0.76214        0.285853            0.285853 â”‚
â”‚ trial-64d31_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine                                                                                â”‚
â”‚ trial-64d31_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1                                                                                 â”‚
â”‚ trial-64d31_00006   RUNNING                16       0.00043776           32              1        256            2   0.00435286   type1          2            23.5315       0.362102       0.224719            0.222999 â”‚
â”‚ trial-64d31_00008   RUNNING                32       0.00196362           64              2        512            4   0.0033338    cosine         4            26.3692       0.421399       0.242626            0.222929 â”‚
â”‚ trial-64d31_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            18.3586       0.433672       0.232965            0.219535 â”‚
â”‚ trial-64d31_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            11.6034       0.368788       0.229693            0.229693 â”‚
â”‚ trial-64d31_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6            20.0593       0.377737       0.233346            0.224291 â”‚
â”‚ trial-64d31_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            26.4174       0.315546       0.222023            0.220789 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2854091)[0m Updating learning rate to 4.584868055853523e-05[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2854091)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2854091)[0m Epoch: 5 cost time: 3.604236125946045[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2854091)[0m Epoch: 5, Steps: 265 | Train Loss: 0.3155461 Vali Loss: 0.2220226 Best vali loss: 0.2207892[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2854091)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2854088)[0m Validation loss decreased (inf --> 0.2711).  Saving model state dict ...
[36m(_train_fn pid=2854084)[0m Validation loss decreased (0.2859 --> 0.2783).  Saving model state dict ...
[36m(_train_fn pid=2854086)[0m 	iters: 100, epoch: 4 | loss: 0.4534636[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2854086)[0m 	speed: 0.0294s/iter; left time: 74.7375s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2854086)[0m Updating learning rate to 0.0001094400973741376[32m [repeated 5x across cluster][0m
2024-08-26 13:43:58,720	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854090)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-43-21/checkpoint_000005)[32m [repeated 5x across cluster][0m
2024-08-26 13:44:01,889	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:44:02,150	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:44:03,758	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854086)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-43-21/checkpoint_000004)[32m [repeated 4x across cluster][0m
2024-08-26 13:44:11,711	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:44:12,022	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:44:12,598	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:44:14,530	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854083)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-43-21/checkpoint_000002)[32m [repeated 4x across cluster][0m
2024-08-26 13:44:14,982	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854084)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-43-21/checkpoint_000004)[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2854086)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2854086)[0m Epoch: 3 cost time: 6.470494031906128[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2854086)[0m Epoch: 3, Steps: 529 | Train Loss: 0.3808414 Vali Loss: 0.2217501 Best vali loss: 0.2217501[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2854090)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2854085)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...

Trial trial-64d31_00008 completed after 6 iterations at 2024-08-26 13:43:58. Total running time: 36s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             4.58818 â”‚
â”‚ time_total_s                                35.10453 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22293 â”‚
â”‚ train_loss                                   0.20661 â”‚
â”‚ valid_loss                                   0.24217 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2854090)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2854090)[0m Early stopping
[36m(_train_fn pid=2854086)[0m Validation loss decreased (0.2230 --> 0.2218).  Saving model state dict ...
[36m(_train_fn pid=2854088)[0m 	iters: 500, epoch: 2 | loss: 0.2058588[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2854088)[0m 	speed: 0.0136s/iter; left time: 93.6294s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2854086)[0m Updating learning rate to 5.47200486870688e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854086)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854086)[0m Epoch: 4 cost time: 6.167962551116943[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854086)[0m Epoch: 4, Steps: 529 | Train Loss: 0.3405288 Vali Loss: 0.2232343 Best vali loss: 0.2217501[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854084)[0m Validation loss decreased (0.2783 --> 0.2703).  Saving model state dict ...
[36m(_train_fn pid=2854086)[0m EarlyStopping counter: 1 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2854084)[0m 	iters: 200, epoch: 4 | loss: 0.3712242[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2854084)[0m 	speed: 0.0150s/iter; left time: 36.7934s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2854084)[0m Updating learning rate to 0.0008782132982231691
[36m(_train_fn pid=2854084)[0m saving checkpoint...
[36m(_train_fn pid=2854084)[0m Epoch: 3 cost time: 8.36587142944336
[36m(_train_fn pid=2854084)[0m Epoch: 3, Steps: 529 | Train Loss: 0.6474589 Vali Loss: 0.2703472 Best vali loss: 0.2703472
[36m(_train_fn pid=2854086)[0m Updating learning rate to 2.73600243435344e-05
[36m(_train_fn pid=2854086)[0m saving checkpoint...
[36m(_train_fn pid=2854086)[0m Epoch: 5 cost time: 5.360105276107788
[36m(_train_fn pid=2854086)[0m Epoch: 5, Steps: 529 | Train Loss: 0.3294062 Vali Loss: 0.2245405 Best vali loss: 0.2217501
[36m(_train_fn pid=2854086)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2854088)[0m Validation loss decreased (0.2711 --> 0.2668).  Saving model state dict ...
[36m(_train_fn pid=2854085)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2854083)[0m 	iters: 1000, epoch: 3 | loss: 0.3227447[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2854083)[0m 	speed: 0.0086s/iter; left time: 46.1094s[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2854084)[0m Updating learning rate to 0.00043910664911158456[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854084)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854084)[0m Epoch: 4 cost time: 7.650765657424927[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854084)[0m Epoch: 4, Steps: 529 | Train Loss: 0.4098419 Vali Loss: 0.2689202 Best vali loss: 0.2689202[32m [repeated 3x across cluster][0m

Trial trial-64d31_00006 completed after 6 iterations at 2024-08-26 13:44:14. Total running time: 53s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             6.17326 â”‚
â”‚ time_total_s                                51.54413 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22175 â”‚
â”‚ train_loss                                   0.36244 â”‚
â”‚ valid_loss                                   0.22489 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2854086)[0m Early stopping
[36m(_train_fn pid=2854084)[0m Validation loss decreased (0.2703 --> 0.2689).  Saving model state dict ...
[36m(_train_fn pid=2854086)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2854085)[0m 	iters: 400, epoch: 3 | loss: 0.2015125[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2854085)[0m 	speed: 0.0127s/iter; left time: 75.6235s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2854086)[0m Updating learning rate to 1.36800121717672e-05[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2854086)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2854086)[0m Epoch: 6 cost time: 4.9484453201293945[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2854086)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3624397 Vali Loss: 0.2248932 Best vali loss: 0.2217501[32m [repeated 2x across cluster][0m

Trial status: 6 TERMINATED | 4 RUNNING
Current time: 2024-08-26 13:44:21. Total running time: 1min 0s
Logical resource usage: 4.0/32 CPUs, 0.4/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 64d31_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2854083)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-43-21/checkpoint_000003)
2024-08-26 13:44:26,884	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2854088)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00004_4_alpha_d_ff=3,batch_size=8,d_core=64,d_model=256,dropout=0.0033,e_layers=4,learning_rate=0.0036,lradj=type1_2024-08-26_13-43-21/checkpoint_000002)
2024-08-26 13:44:27,030	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:44:27,412	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-64d31_00001   RUNNING                 8       6.40637e-05         256              4         64            2   0.00128816   cosine         3            51.0304       0.425221       0.219718            0.215952 â”‚
â”‚ trial-64d31_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          5            57.0395       0.499555       0.2704              0.26892  â”‚
â”‚ trial-64d31_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         2            48.4749       0.287397       0.224377            0.220549 â”‚
â”‚ trial-64d31_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          2            48.2791       0.5709         0.266773            0.266773 â”‚
â”‚ trial-64d31_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            18.3586       0.433672       0.232965            0.219535 â”‚
â”‚ trial-64d31_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            11.6034       0.368788       0.229693            0.229693 â”‚
â”‚ trial-64d31_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6            51.5441       0.36244        0.224893            0.22175  â”‚
â”‚ trial-64d31_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6            20.0593       0.377737       0.233346            0.224291 â”‚
â”‚ trial-64d31_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6            35.1045       0.206613       0.242167            0.222929 â”‚
â”‚ trial-64d31_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            26.4174       0.315546       0.222023            0.220789 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2854084)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2854083)[0m 	iters: 900, epoch: 4 | loss: 0.3770967[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2854083)[0m 	speed: 0.0095s/iter; left time: 41.8341s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2854084)[0m Updating learning rate to 0.00021955332455579228
[36m(_train_fn pid=2854084)[0m saving checkpoint...
[36m(_train_fn pid=2854084)[0m Epoch: 5 cost time: 6.9144086837768555
[36m(_train_fn pid=2854084)[0m Epoch: 5, Steps: 529 | Train Loss: 0.4995547 Vali Loss: 0.2704001 Best vali loss: 0.2689202

Trial trial-64d31_00001 completed after 4 iterations at 2024-08-26 13:44:26. Total running time: 1min 4s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            11.84082 â”‚
â”‚ time_total_s                                62.87122 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21595 â”‚
â”‚ train_loss                                   0.34303 â”‚
â”‚ valid_loss                                   0.22278 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2854083)[0m Updating learning rate to 3.203185178516828e-05
[36m(_train_fn pid=2854083)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2854083)[0m saving checkpoint...
[36m(_train_fn pid=2854083)[0m Epoch: 4 cost time: 9.934723377227783
[36m(_train_fn pid=2854083)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.3430316 Vali Loss: 0.2227829 Best vali loss: 0.2159521
[36m(_train_fn pid=2854083)[0m Early stopping
[36m(_train_fn pid=2854085)[0m 	iters: 100, epoch: 4 | loss: 0.2328789[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2854085)[0m 	speed: 0.0336s/iter; left time: 174.1484s[32m [repeated 11x across cluster][0m
[36m(_train_fn pid=2854084)[0m Updating learning rate to 0.00010977666227789614[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854084)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854084)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854084)[0m Epoch: 6 cost time: 6.1065099239349365[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854084)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3591521 Vali Loss: 0.2706647 Best vali loss: 0.2689202[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2854085)[0m 	iters: 600, epoch: 4 | loss: 0.2013022[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2854085)[0m 	speed: 0.0098s/iter; left time: 45.9576s[32m [repeated 15x across cluster][0m

Trial trial-64d31_00002 completed after 7 iterations at 2024-08-26 13:44:33. Total running time: 1min 11s
[36m(_train_fn pid=2854084)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-43-21/checkpoint_000006)[32m [repeated 3x across cluster][0m
2024-08-26 13:44:37,085	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:44:37,185	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:44:37,187	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials' in 0.0019s.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             6.24349 â”‚
â”‚ time_total_s                                70.12233 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26892 â”‚
â”‚ train_loss                                    0.3598 â”‚
â”‚ valid_loss                                    0.2707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-64d31_00004 completed after 4 iterations at 2024-08-26 13:44:37. Total running time: 1min 15s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            10.19771 â”‚
â”‚ time_total_s                                73.64603 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.26677 â”‚
â”‚ train_loss                                   0.61468 â”‚
â”‚ valid_loss                                   0.27059 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2854088)[0m Updating learning rate to 0.00044739851102008107[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2854088)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2854088)[0m saving checkpoint...[32m [repeated 2x across cluster][0m

Trial trial-64d31_00003 completed after 4 iterations at 2024-08-26 13:44:37. Total running time: 1min 15s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-64d31_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                            10.15185 â”‚
â”‚ time_total_s                                 73.6321 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22055 â”‚
â”‚ train_loss                                   0.24506 â”‚
â”‚ valid_loss                                   0.22707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:44:37. Total running time: 1min 15s
Logical resource usage: 1.0/32 CPUs, 0.1/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 64d31_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-64d31_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            18.3586       0.433672       0.232965            0.219535 â”‚
â”‚ trial-64d31_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4            62.8712       0.343032       0.222783            0.215952 â”‚
â”‚ trial-64d31_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7            70.1223       0.359801       0.270699            0.26892  â”‚
â”‚ trial-64d31_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4            73.6321       0.245055       0.22707             0.220549 â”‚
â”‚ trial-64d31_00004   TERMINATED              8       0.00357919          256              3         64            4   0.00325461   type1          4            73.646        0.614685       0.270593            0.266773 â”‚
â”‚ trial-64d31_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            11.6034       0.368788       0.229693            0.229693 â”‚
â”‚ trial-64d31_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6            51.5441       0.36244        0.224893            0.22175  â”‚
â”‚ trial-64d31_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          6            20.0593       0.377737       0.233346            0.224291 â”‚
â”‚ trial-64d31_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6            35.1045       0.206613       0.242167            0.222929 â”‚
â”‚ trial-64d31_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            26.4174       0.315546       0.222023            0.220789 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2854085)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test_10_parallel_trials/trial-64d31_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-43-21/checkpoint_000003)[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2854085)[0m 	iters: 1000, epoch: 4 | loss: 0.2337351[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2854085)[0m 	speed: 0.0066s/iter; left time: 28.1113s[32m [repeated 8x across cluster][0m
[36m(_train_fn pid=2854085)[0m Updating learning rate to 7.724479248689109e-05
[36m(_train_fn pid=2854085)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2854085)[0m saving checkpoint...


Time taken (10 parallel trials): 80 seconds


