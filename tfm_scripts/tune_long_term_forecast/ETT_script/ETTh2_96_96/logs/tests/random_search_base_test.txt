2024-08-26 13:14:24,666	INFO worker.py:1781 -- Started a local Ray instance.
2024-08-26 13:14:25,129	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-08-26 13:14:25,134	WARNING tune.py:821 -- You have passed a `SearchGenerator` instance as the `search_alg`, but `max_concurrent_trials` requires a `Searcher` instance`. `max_concurrent_trials` will be ignored.
2024-08-26 13:14:25,146	WARNING variant_generator.py:324 -- Pre-set value `0.0` is not within valid values of parameter `dropout`: (0.0008, 0.012)
2024-08-26 13:14:29,055	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2777934)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00000_0_2024-08-26_13-14-25/checkpoint_000000)
2024-08-26 13:14:30,488	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2777934)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00000_0_2024-08-26_13-14-25/checkpoint_000001)
2024-08-26 13:14:31,840	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2777934)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00000_0_2024-08-26_13-14-25/checkpoint_000002)
2024-08-26 13:14:32,880	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:14:33,045	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     ETTh2_96_96_test      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator â”‚
â”‚ Scheduler                        FIFOScheduler         â”‚
â”‚ Number of trials                 10                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-08-26_13-14-24_119232_2775707/artifacts/2024-08-26_13-14-25/ETTh2_96_96_test/driver_artifacts`

Trial status: 2 PENDING
Current time: 2024-08-26 13:14:25. Total running time: 0s
Logical resource usage: 2.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status       batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-59986_00000   PENDING              32       0.0003              128              1         64            2   0            cosine  â”‚
â”‚ trial-59986_00001   PENDING               8       6.40637e-05         256              4         64            2   0.00128816   cosine  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-59986_00001 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00001 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00129 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00006 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-59986_00000 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00000 config            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                              1 â”‚
â”‚ batch_size                             32 â”‚
â”‚ d_core                                 64 â”‚
â”‚ d_model                               128 â”‚
â”‚ dropout                                0. â”‚
â”‚ e_layers                                2 â”‚
â”‚ learning_rate                      0.0003 â”‚
â”‚ lradj                              cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2777935)[0m configuration
[36m(_train_fn pid=2777935)[0m {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2777935)[0m Use GPU: cuda:0
[36m(_train_fn pid=2777935)[0m train 8449
[36m(_train_fn pid=2777935)[0m val 2785
[36m(_train_fn pid=2777935)[0m start_epoch 0
[36m(_train_fn pid=2777935)[0m max_epoch 8
[36m(_train_fn pid=2777935)[0m 	iters: 100, epoch: 1 | loss: 0.1373764
[36m(_train_fn pid=2777935)[0m 	speed: 0.0129s/iter; left time: 107.9588s
[36m(_train_fn pid=2777934)[0m Updating learning rate to 0.000288581929876693
[36m(_train_fn pid=2777934)[0m saving checkpoint...
[36m(_train_fn pid=2777934)[0m Validation loss decreased (inf --> 0.2195).  Saving model state dict ...
[36m(_train_fn pid=2777934)[0m Epoch: 1 cost time: 1.4950528144836426
[36m(_train_fn pid=2777934)[0m Epoch: 1, Steps: 265 | Train Loss: 0.5247267 Vali Loss: 0.2195345 Best vali loss: 0.2195345
[36m(_train_fn pid=2777934)[0m Updating learning rate to 0.00025606601717798207
[36m(_train_fn pid=2777934)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2777934)[0m saving checkpoint...
[36m(_train_fn pid=2777934)[0m Epoch: 2 cost time: 1.2092218399047852
[36m(_train_fn pid=2777934)[0m Epoch: 2, Steps: 265 | Train Loss: 0.4370105 Vali Loss: 0.2229692 Best vali loss: 0.2195345
[36m(_train_fn pid=2777934)[0m configuration
[36m(_train_fn pid=2777934)[0m {'batch_size': 32, 'learning_rate': 0.0003, 'd_model': 128, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2777934)[0m Use GPU: cuda:0
[36m(_train_fn pid=2777934)[0m Updating learning rate to 0.00020740251485476345
[36m(_train_fn pid=2777934)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2777934)[0m saving checkpoint...
[36m(_train_fn pid=2777934)[0m Epoch: 3 cost time: 1.1324164867401123
[36m(_train_fn pid=2777934)[0m Epoch: 3, Steps: 265 | Train Loss: 0.3195181 Vali Loss: 0.2231508 Best vali loss: 0.2195345
[36m(_train_fn pid=2777934)[0m train 8449
[36m(_train_fn pid=2777934)[0m val 2785
[36m(_train_fn pid=2777934)[0m start_epoch 0
[36m(_train_fn pid=2777934)[0m max_epoch 8

Trial trial-59986_00000 completed after 4 iterations at 2024-08-26 13:14:33. Total running time: 7s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00000 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.20094 â”‚
â”‚ time_total_s                                 6.30272 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21953 â”‚
â”‚ train_loss                                   0.43367 â”‚
â”‚ valid_loss                                   0.23297 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
2024-08-26 13:14:36,556	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2777935)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-14-25/checkpoint_000001)[32m [repeated 3x across cluster][0m
2024-08-26 13:14:39,584	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:14:40,196	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:14:43,737	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2777935)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00001_1_alpha_d_ff=4,batch_size=8,d_core=64,d_model=256,dropout=0.0013,e_layers=2,learning_rate=0.0001,lradj=cosine_2024-08-26_13-14-25/checkpoint_000003)[32m [repeated 3x across cluster][0m
2024-08-26 13:14:43,852	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:14:48,090	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:14:51,480	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2777934)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2777934)[0m Early stopping
[36m(_train_fn pid=2777935)[0m 	iters: 100, epoch: 2 | loss: 0.1362936[32m [repeated 18x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(_train_fn pid=2777935)[0m 	speed: 0.0144s/iter; left time: 105.2337s[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2777934)[0m Updating learning rate to 0.00015[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2777934)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2777935)[0m Validation loss decreased (inf --> 0.2160).  Saving model state dict ...
[36m(_train_fn pid=2777934)[0m Epoch: 4 cost time: 0.9877278804779053[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2777934)[0m Epoch: 4, Steps: 265 | Train Loss: 0.4336717 Vali Loss: 0.2329654 Best vali loss: 0.2195345[32m [repeated 2x across cluster][0m

Trial trial-59986_00002 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00002 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00891 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00351 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2778725)[0m configuration
[36m(_train_fn pid=2778725)[0m {'batch_size': 16, 'learning_rate': 0.0035128531928926765, 'd_model': 512, 'd_core': 64, 'e_layers': 4, 'dropout': 0.00890604745419256, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2778725)[0m Use GPU: cuda:0
[36m(_train_fn pid=2778725)[0m train 8449
[36m(_train_fn pid=2778725)[0m val 2785
[36m(_train_fn pid=2778725)[0m start_epoch 0
[36m(_train_fn pid=2778725)[0m max_epoch 8
[36m(_train_fn pid=2777935)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2777935)[0m 	iters: 600, epoch: 3 | loss: 1.0170871[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2777935)[0m 	speed: 0.0028s/iter; left time: 16.2848s[32m [repeated 19x across cluster][0m
[36m(_train_fn pid=2777935)[0m Updating learning rate to 5.4681791396423194e-05
[36m(_train_fn pid=2777935)[0m saving checkpoint...
[36m(_train_fn pid=2777935)[0m Epoch: 2 cost time: 3.1765100955963135
[36m(_train_fn pid=2777935)[0m Epoch: 2, Steps: 1057 | Train Loss: 0.3143075 Vali Loss: 0.2210888 Best vali loss: 0.2159521
[36m(_train_fn pid=2778725)[0m Updating learning rate to 0.0035128531928926765
[36m(_train_fn pid=2778725)[0m saving checkpoint...
[36m(_train_fn pid=2778725)[0m Validation loss decreased (inf --> 0.2884).  Saving model state dict ...
[36m(_train_fn pid=2778725)[0m Epoch: 1 cost time: 4.104032754898071
[36m(_train_fn pid=2778725)[0m Epoch: 1, Steps: 529 | Train Loss: 0.7850501 Vali Loss: 0.2883907 Best vali loss: 0.2883907
[36m(_train_fn pid=2777935)[0m EarlyStopping counter: 2 out of 3

Trial trial-59986_00001 completed after 4 iterations at 2024-08-26 13:14:43. Total running time: 18s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00001 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             3.53793 â”‚
â”‚ time_total_s                                16.99569 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.21595 â”‚
â”‚ train_loss                                   0.34303 â”‚
â”‚ valid_loss                                   0.22278 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2777935)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2777935)[0m Early stopping
[36m(_train_fn pid=2778725)[0m 	iters: 500, epoch: 2 | loss: 0.3073497[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2778725)[0m 	speed: 0.0071s/iter; left time: 22.7487s[32m [repeated 20x across cluster][0m
[36m(_train_fn pid=2778725)[0m Validation loss decreased (0.2884 --> 0.2775).  Saving model state dict ...
[36m(_train_fn pid=2778725)[0m Updating learning rate to 0.0017564265964463382[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2778725)[0m saving checkpoint...[32m [repeated 3x across cluster][0m

Trial trial-59986_00003 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00003 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                128 â”‚
â”‚ dropout                            0.00596 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00015 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2779165)[0m configuration
[36m(_train_fn pid=2779165)[0m {'batch_size': 8, 'learning_rate': 0.00015448958497378217, 'd_model': 128, 'd_core': 512, 'e_layers': 4, 'dropout': 0.005958307406826554, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2779165)[0m Use GPU: cuda:0
[36m(_train_fn pid=2778725)[0m Epoch: 2 cost time: 3.8084511756896973[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2778725)[0m Epoch: 2, Steps: 529 | Train Loss: 0.4281371 Vali Loss: 0.2774843 Best vali loss: 0.2774843[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2779165)[0m train 8449
[36m(_train_fn pid=2779165)[0m val 2785
[36m(_train_fn pid=2779165)[0m start_epoch 0
[36m(_train_fn pid=2779165)[0m max_epoch 8
[36m(_train_fn pid=2778725)[0m Validation loss decreased (0.2775 --> 0.2703).  Saving model state dict ...
[36m(_train_fn pid=2778725)[0m 	iters: 100, epoch: 4 | loss: 0.4900947[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2778725)[0m 	speed: 0.0140s/iter; left time: 35.6047s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2778725)[0m Updating learning rate to 0.0008782132982231691
[36m(_train_fn pid=2778725)[0m saving checkpoint...
[36m(_train_fn pid=2778725)[0m Epoch: 3 cost time: 3.7905144691467285
[36m(_train_fn pid=2778725)[0m Epoch: 3, Steps: 529 | Train Loss: 0.6475151 Vali Loss: 0.2702828 Best vali loss: 0.2702828
[36m(_train_fn pid=2779165)[0m Updating learning rate to 0.00014860967525861138
[36m(_train_fn pid=2779165)[0m saving checkpoint...
[36m(_train_fn pid=2779165)[0m Validation loss decreased (inf --> 0.2205).  Saving model state dict ...
[36m(_train_fn pid=2779165)[0m Epoch: 1 cost time: 5.195730686187744
[36m(_train_fn pid=2779165)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-14-43/checkpoint_000000)[32m [repeated 3x across cluster][0m
2024-08-26 13:14:52,332	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:14:56,562	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2778725)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00002_2_alpha_d_ff=2,batch_size=16,d_core=64,d_model=512,dropout=0.0089,e_layers=4,learning_rate=0.0035,lradj=type1_2024-08-26_13-14-33/checkpoint_000004)[32m [repeated 2x across cluster][0m
2024-08-26 13:14:56,998	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:00,794	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:02,315	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2779165)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-14-43/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 13:15:05,004	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2779165)[0m Epoch: 1, Steps: 1057 | Train Loss: 0.4099332 Vali Loss: 0.2205494 Best vali loss: 0.2205494
[36m(_train_fn pid=2778725)[0m Validation loss decreased (0.2703 --> 0.2689).  Saving model state dict ...
[36m(_train_fn pid=2779165)[0m 	iters: 600, epoch: 2 | loss: 0.4358251[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2779165)[0m 	speed: 0.0045s/iter; left time: 30.8367s[32m [repeated 16x across cluster][0m

Trial status: 2 TERMINATED | 2 RUNNING
Current time: 2024-08-26 13:14:55. Total running time: 30s
Logical resource usage: 2.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 59986_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-59986_00002   RUNNING                16       0.00351285          512              2         64            4   0.00890605   type1          4           17.776         0.409946       0.268922            0.268922 â”‚
â”‚ trial-59986_00003   RUNNING                 8       0.00015449          128              1        512            4   0.00595831   cosine         1            6.27157       0.409933       0.220549            0.220549 â”‚
â”‚ trial-59986_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            6.30272       0.433672       0.232965            0.219535 â”‚
â”‚ trial-59986_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           16.9957        0.343032       0.222783            0.215952 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2778725)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2778725)[0m Updating learning rate to 0.00021955332455579228[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2778725)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2778725)[0m Epoch: 5 cost time: 3.8052303791046143[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2778725)[0m Epoch: 5, Steps: 529 | Train Loss: 0.4993637 Vali Loss: 0.2703723 Best vali loss: 0.2689222[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2779165)[0m 	iters: 500, epoch: 3 | loss: 0.3416516[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2779165)[0m 	speed: 0.0043s/iter; left time: 25.2450s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2778725)[0m EarlyStopping counter: 2 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2778725)[0m Updating learning rate to 0.00010977666227789614[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2778725)[0m saving checkpoint...[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2778725)[0m Epoch: 6 cost time: 3.824176788330078[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2778725)[0m Epoch: 6, Steps: 529 | Train Loss: 0.3591892 Vali Loss: 0.2706392 Best vali loss: 0.2689222[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2778725)[0m 	iters: 500, epoch: 7 | loss: 0.4792185[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2778725)[0m 	speed: 0.0071s/iter; left time: 3.9726s[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2778725)[0m Early stopping

Trial trial-59986_00002 completed after 7 iterations at 2024-08-26 13:15:05. Total running time: 39s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00002 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000006 â”‚
â”‚ time_this_iter_s                             4.20754 â”‚
â”‚ time_total_s                                30.43421 â”‚
â”‚ training_iteration                                 7 â”‚
â”‚ best_valid_loss                              0.26892 â”‚
â”‚ train_loss                                   0.35979 â”‚
â”‚ valid_loss                                   0.27069 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-59986_00004 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00004 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               3 â”‚
â”‚ batch_size                               8 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                256 â”‚
â”‚ dropout                            0.00325 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00358 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2779937)[0m configuration
[36m(_train_fn pid=2779937)[0m {'batch_size': 8, 'learning_rate': 0.0035791880881606486, 'd_model': 256, 'd_core': 64, 'e_layers': 4, 'dropout': 0.003254613020686493, 'lradj': 'type1', 'd_ff': 768}
[36m(_train_fn pid=2779937)[0m Use GPU: cuda:0
[36m(_train_fn pid=2779937)[0m train 8449
[36m(_train_fn pid=2779937)[0m val 2785
[36m(_train_fn pid=2779937)[0m start_epoch 0
[36m(_train_fn pid=2779937)[0m max_epoch 8
[36m(_train_fn pid=2779165)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00003_3_alpha_d_ff=1,batch_size=8,d_core=512,d_model=128,dropout=0.0060,e_layers=4,learning_rate=0.0002,lradj=cosine_2024-08-26_13-14-43/checkpoint_000003)[32m [repeated 2x across cluster][0m
2024-08-26 13:15:07,844	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:10,444	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:10,960	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:11,435	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:11,913	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:12,393	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:12,885	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2780094)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00005_5_alpha_d_ff=1,batch_size=64,d_core=128,d_model=64,dropout=0.0013,e_layers=1,learning_rate=0.0001,lradj=type1_2024-08-26_13-15-07/checkpoint_000005)[32m [repeated 6x across cluster][0m
2024-08-26 13:15:13,079	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:13,386	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:13,875	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:18,826	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2780845)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-15-13/checkpoint_000000)[32m [repeated 4x across cluster][0m
2024-08-26 13:15:19,565	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2779165)[0m EarlyStopping counter: 3 out of 3[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2779165)[0m Updating learning rate to 7.724479248689109e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2779165)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2778725)[0m Epoch: 7 cost time: 3.809218406677246[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2778725)[0m Epoch: 7, Steps: 529 | Train Loss: 0.3597851 Vali Loss: 0.2706914 Best vali loss: 0.2689222[32m [repeated 2x across cluster][0m

Trial trial-59986_00003 completed after 4 iterations at 2024-08-26 13:15:07. Total running time: 42s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00003 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             5.52597 â”‚
â”‚ time_total_s                                22.62664 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22055 â”‚
â”‚ train_loss                                   0.24506 â”‚
â”‚ valid_loss                                   0.22707 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial trial-59986_00005 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00005 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              64 â”‚
â”‚ d_core                                 128 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00132 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00011 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2779937)[0m 	iters: 400, epoch: 1 | loss: 0.1746203[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2779937)[0m 	speed: 0.0049s/iter; left time: 39.3444s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2779165)[0m Early stopping
[36m(_train_fn pid=2780094)[0m Validation loss decreased (inf --> 0.2578).  Saving model state dict ...
[36m(_train_fn pid=2780094)[0m Validation loss decreased (0.2578 --> 0.2387).  Saving model state dict ...
[36m(_train_fn pid=2780094)[0m Validation loss decreased (0.2387 --> 0.2336).  Saving model state dict ...
[36m(_train_fn pid=2780094)[0m configuration
[36m(_train_fn pid=2780094)[0m {'batch_size': 64, 'learning_rate': 0.00010734405345907071, 'd_model': 64, 'd_core': 128, 'e_layers': 1, 'dropout': 0.0013179176757202608, 'lradj': 'type1', 'd_ff': 64}
[36m(_train_fn pid=2780094)[0m Use GPU: cuda:0
[36m(_train_fn pid=2780094)[0m train 8449
[36m(_train_fn pid=2780094)[0m val 2785
[36m(_train_fn pid=2780094)[0m Validation loss decreased (0.2336 --> 0.2315).  Saving model state dict ...
[36m(_train_fn pid=2780094)[0m start_epoch 0
[36m(_train_fn pid=2780094)[0m max_epoch 8
[36m(_train_fn pid=2780094)[0m Validation loss decreased (0.2315 --> 0.2305).  Saving model state dict ...
[36m(_train_fn pid=2780094)[0m Validation loss decreased (0.2305 --> 0.2300).  Saving model state dict ...
[36m(_train_fn pid=2780094)[0m Updating learning rate to 3.35450167059596e-06[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2780094)[0m saving checkpoint...[32m [repeated 6x across cluster][0m
[36m(_train_fn pid=2780094)[0m Epoch: 6 cost time: 0.37070751190185547[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2780094)[0m Epoch: 6, Steps: 133 | Train Loss: 0.3472632 Vali Loss: 0.2300409 Best vali loss: 0.2300409[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2780094)[0m Validation loss decreased (0.2300 --> 0.2298).  Saving model state dict ...

Trial trial-59986_00005 completed after 8 iterations at 2024-08-26 13:15:13. Total running time: 48s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00005 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000007 â”‚
â”‚ time_this_iter_s                              0.4865 â”‚
â”‚ time_total_s                                 4.55289 â”‚
â”‚ training_iteration                                 8 â”‚
â”‚ best_valid_loss                              0.22969 â”‚
â”‚ train_loss                                   0.36795 â”‚
â”‚ valid_loss                                   0.22969 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2780094)[0m Validation loss decreased (0.2298 --> 0.2297).  Saving model state dict ...
[36m(_train_fn pid=2779937)[0m 	iters: 300, epoch: 2 | loss: 0.9320508[32m [repeated 17x across cluster][0m
[36m(_train_fn pid=2779937)[0m 	speed: 0.0047s/iter; left time: 33.2260s[32m [repeated 17x across cluster][0m

Trial trial-59986_00006 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00006 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               1 â”‚
â”‚ batch_size                              16 â”‚
â”‚ d_core                                 256 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00435 â”‚
â”‚ e_layers                                 2 â”‚
â”‚ learning_rate                      0.00044 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2780845)[0m configuration
[36m(_train_fn pid=2780845)[0m {'batch_size': 16, 'learning_rate': 0.0004377603894965504, 'd_model': 32, 'd_core': 256, 'e_layers': 2, 'dropout': 0.00435286364938959, 'lradj': 'type1', 'd_ff': 32}
[36m(_train_fn pid=2780845)[0m Use GPU: cuda:0
[36m(_train_fn pid=2780845)[0m train 8449
[36m(_train_fn pid=2780845)[0m val 2785
[36m(_train_fn pid=2780845)[0m start_epoch 0
[36m(_train_fn pid=2780845)[0m max_epoch 8
[36m(_train_fn pid=2779937)[0m Validation loss decreased (inf --> 0.2711).  Saving model state dict ...
[36m(_train_fn pid=2780094)[0m Updating learning rate to 8.3862541764899e-07[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2780094)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2780094)[0m Epoch: 8 cost time: 0.36223936080932617[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2780094)[0m Epoch: 8, Steps: 133 | Train Loss: 0.3679461 Vali Loss: 0.2296906 Best vali loss: 0.2296906[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2780845)[0m Validation loss decreased (inf --> 0.2230).  Saving model state dict ...
2024-08-26 13:15:21,553	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:24,407	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2780845)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-15-13/checkpoint_000002)[32m [repeated 3x across cluster][0m
2024-08-26 13:15:26,914	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:26,984	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:29,810	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2780845)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00006_6_alpha_d_ff=1,batch_size=16,d_core=256,d_model=32,dropout=0.0044,e_layers=2,learning_rate=0.0004,lradj=type1_2024-08-26_13-15-13/checkpoint_000004)[32m [repeated 3x across cluster][0m
2024-08-26 13:15:32,630	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:33,872	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2779937)[0m Validation loss decreased (0.2711 --> 0.2668).  Saving model state dict ...
[36m(_train_fn pid=2780845)[0m 	iters: 100, epoch: 2 | loss: 0.3049569[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2780845)[0m 	speed: 0.0101s/iter; left time: 36.3761s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2780845)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2780845)[0m Updating learning rate to 0.0002188801947482752[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2780845)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2780845)[0m Epoch: 2 cost time: 2.2080607414245605[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2780845)[0m Epoch: 2, Steps: 529 | Train Loss: 0.3621016 Vali Loss: 0.2247194 Best vali loss: 0.2229993[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2780845)[0m Validation loss decreased (0.2230 --> 0.2218).  Saving model state dict ...
[36m(_train_fn pid=2780845)[0m 	iters: 100, epoch: 4 | loss: 0.4534636[32m [repeated 18x across cluster][0m
[36m(_train_fn pid=2780845)[0m 	speed: 0.0110s/iter; left time: 28.0530s[32m [repeated 18x across cluster][0m

Trial status: 5 TERMINATED | 2 RUNNING
Current time: 2024-08-26 13:15:25. Total running time: 1min 0s
Logical resource usage: 2.0/32 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 59986_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-59986_00004   RUNNING                 8       0.00357919          256              3         64            4   0.00325461   type1          2           13.0955        0.5709         0.266773            0.266773 â”‚
â”‚ trial-59986_00006   RUNNING                16       0.00043776           32              1        256            2   0.00435286   type1          3            9.0851        0.380841       0.22175             0.22175  â”‚
â”‚ trial-59986_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            6.30272       0.433672       0.232965            0.219535 â”‚
â”‚ trial-59986_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           16.9957        0.343032       0.222783            0.215952 â”‚
â”‚ trial-59986_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           30.4342        0.359785       0.270691            0.268922 â”‚
â”‚ trial-59986_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           22.6266        0.245055       0.22707             0.220549 â”‚
â”‚ trial-59986_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            4.55289       0.367946       0.229691            0.229691 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2779937)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2780845)[0m EarlyStopping counter: 1 out of 3
[36m(_train_fn pid=2780845)[0m Updating learning rate to 5.47200486870688e-05[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2780845)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2780845)[0m Epoch: 4 cost time: 2.139101028442383[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2780845)[0m Epoch: 4, Steps: 529 | Train Loss: 0.3405288 Vali Loss: 0.2232343 Best vali loss: 0.2217501[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2780845)[0m EarlyStopping counter: 2 out of 3
[36m(_train_fn pid=2779937)[0m 	iters: 500, epoch: 4 | loss: 0.2820254[32m [repeated 16x across cluster][0m
[36m(_train_fn pid=2779937)[0m 	speed: 0.0057s/iter; left time: 27.0514s[32m [repeated 16x across cluster][0m

Trial trial-59986_00006 completed after 6 iterations at 2024-08-26 13:15:32. Total running time: 1min 7s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00006 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             2.81762 â”‚
â”‚ time_total_s                                17.30003 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22175 â”‚
â”‚ train_loss                                   0.36244 â”‚
â”‚ valid_loss                                   0.22489 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2780845)[0m EarlyStopping counter: 3 out of 3
[36m(_train_fn pid=2780845)[0m Early stopping
[36m(_train_fn pid=2779937)[0m Updating learning rate to 0.00044739851102008107[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2779937)[0m saving checkpoint...[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2779937)[0m Epoch: 4 cost time: 6.292524099349976[32m [repeated 3x across cluster][0m
[36m(_train_fn pid=2779937)[0m Epoch: 4, Steps: 1057 | Train Loss: 0.6146849 Vali Loss: 0.2705928 Best vali loss: 0.2667734[32m [repeated 3x across cluster][0m

Trial trial-59986_00007 started with configuration:
2024-08-26 13:15:35,791	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2781566)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00007_7_alpha_d_ff=2,batch_size=32,d_core=64,d_model=512,dropout=0.0058,e_layers=1,learning_rate=0.0027,lradj=type1_2024-08-26_13-15-32/checkpoint_000000)[32m [repeated 3x across cluster][0m
2024-08-26 13:15:36,780	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:37,775	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:38,781	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:39,340	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:42,351	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2782063)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-15-38/checkpoint_000000)[32m [repeated 5x across cluster][0m
2024-08-26 13:15:42,711	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:43,808	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:44,006	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:45,265	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00007 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                512 â”‚
â”‚ dropout                            0.00579 â”‚
â”‚ e_layers                                 1 â”‚
â”‚ learning_rate                      0.00268 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2781566)[0m configuration
[36m(_train_fn pid=2781566)[0m {'batch_size': 32, 'learning_rate': 0.002681331476927111, 'd_model': 512, 'd_core': 64, 'e_layers': 1, 'dropout': 0.005787899105404264, 'lradj': 'type1', 'd_ff': 1024}
[36m(_train_fn pid=2781566)[0m Use GPU: cuda:0
[36m(_train_fn pid=2781566)[0m train 8449
[36m(_train_fn pid=2781566)[0m val 2785
[36m(_train_fn pid=2781566)[0m start_epoch 0
[36m(_train_fn pid=2781566)[0m max_epoch 8
[36m(_train_fn pid=2781566)[0m 	iters: 100, epoch: 1 | loss: 0.3522490[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2781566)[0m 	speed: 0.0084s/iter; left time: 16.9902s[32m [repeated 13x across cluster][0m
[36m(_train_fn pid=2781566)[0m Validation loss decreased (inf --> 0.2258).  Saving model state dict ...
[36m(_train_fn pid=2781566)[0m EarlyStopping counter: 2 out of 3[32m [repeated 3x across cluster][0m

Trial trial-59986_00007 completed after 4 iterations at 2024-08-26 13:15:38. Total running time: 1min 13s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00007 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000003 â”‚
â”‚ time_this_iter_s                             1.00446 â”‚
â”‚ time_total_s                                 4.66576 â”‚
â”‚ training_iteration                                 4 â”‚
â”‚ best_valid_loss                              0.22575 â”‚
â”‚ train_loss                                   0.20782 â”‚
â”‚ valid_loss                                   0.25053 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2781566)[0m Early stopping

Trial trial-59986_00004 completed after 5 iterations at 2024-08-26 13:15:39. Total running time: 1min 14s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00004 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             5.46564 â”‚
â”‚ time_total_s                                32.86111 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.26677 â”‚
â”‚ train_loss                                   0.43419 â”‚
â”‚ valid_loss                                   0.27142 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2779937)[0m Updating learning rate to 0.00022369925551004054[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2779937)[0m saving checkpoint...[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2779937)[0m Epoch: 5 cost time: 4.827175617218018[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2779937)[0m Epoch: 5, Steps: 1057 | Train Loss: 0.4341850 Vali Loss: 0.2714180 Best vali loss: 0.2667734[32m [repeated 5x across cluster][0m

Trial trial-59986_00008 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00008 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               2 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                 512 â”‚
â”‚ d_model                                 64 â”‚
â”‚ dropout                            0.00333 â”‚
â”‚ e_layers                                 4 â”‚
â”‚ learning_rate                      0.00196 â”‚
â”‚ lradj                               cosine â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2782063)[0m configuration
[36m(_train_fn pid=2782063)[0m {'batch_size': 32, 'learning_rate': 0.001963618480113192, 'd_model': 64, 'd_core': 512, 'e_layers': 4, 'dropout': 0.0033338002527237612, 'lradj': 'cosine', 'd_ff': 128}
[36m(_train_fn pid=2782063)[0m Use GPU: cuda:0
[36m(_train_fn pid=2779937)[0m 	iters: 1000, epoch: 5 | loss: 0.3214265[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2779937)[0m 	speed: 0.0045s/iter; left time: 14.6612s[32m [repeated 15x across cluster][0m
[36m(_train_fn pid=2782063)[0m train 8449
[36m(_train_fn pid=2782063)[0m val 2785
[36m(_train_fn pid=2782063)[0m start_epoch 0
[36m(_train_fn pid=2782063)[0m max_epoch 8

Trial trial-59986_00009 started with configuration:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00009 config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ alpha_d_ff                               4 â”‚
â”‚ batch_size                              32 â”‚
â”‚ d_core                                  64 â”‚
â”‚ d_model                                 32 â”‚
â”‚ dropout                            0.00128 â”‚
â”‚ e_layers                                 3 â”‚
â”‚ learning_rate                      0.00073 â”‚
â”‚ lradj                                type1 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2782127)[0m {'batch_size': 32, 'learning_rate': 0.0007335788889365637, 'd_model': 32, 'd_core': 64, 'e_layers': 3, 'dropout': 0.0012802554597501359, 'lradj': 'type1', 'd_ff': 128}
[36m(_train_fn pid=2782063)[0m Validation loss decreased (inf --> 0.2248).  Saving model state dict ...
[36m(_train_fn pid=2779937)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2779937)[0m Early stopping
[36m(_train_fn pid=2782127)[0m Validation loss decreased (0.2216 --> 0.2208).  Saving model state dict ...
[36m(_train_fn pid=2782127)[0m Updating learning rate to 0.00036678944446828186[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2782127)[0m saving checkpoint...[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2782127)[0m Epoch: 2 cost time: 1.0870914459228516[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2782127)[0m Epoch: 2, Steps: 265 | Train Loss: 0.2765278 Vali Loss: 0.2207911 Best vali loss: 0.2207911[32m [repeated 4x across cluster][0m
[36m(_train_fn pid=2782127)[0m configuration
2024-08-26 13:15:45,332	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:46,535	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:46,859	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:47,864	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(_train_fn pid=2782127)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00009_9_alpha_d_ff=4,batch_size=32,d_core=64,d_model=32,dropout=0.0013,e_layers=3,learning_rate=0.0007,lradj=type1_2024-08-26_13-15-39/checkpoint_000004)[32m [repeated 8x across cluster][0m
2024-08-26 13:15:48,406	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:50,014	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2024-08-26 13:15:50,023	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test' in 0.0083s.
[36m(_train_fn pid=2782127)[0m Use GPU: cuda:0
[36m(_train_fn pid=2782127)[0m 	iters: 200, epoch: 3 | loss: 0.2014026[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2782127)[0m 	speed: 0.0038s/iter; left time: 5.2593s[32m [repeated 12x across cluster][0m
[36m(_train_fn pid=2782127)[0m train 8449
[36m(_train_fn pid=2782127)[0m val 2785
[36m(_train_fn pid=2782127)[0m start_epoch 0
[36m(_train_fn pid=2782127)[0m max_epoch 8
[36m(_train_fn pid=2782127)[0m Validation loss decreased (inf --> 0.2216).  Saving model state dict ...

Trial trial-59986_00009 completed after 5 iterations at 2024-08-26 13:15:47. Total running time: 1min 22s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00009 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000004 â”‚
â”‚ time_this_iter_s                             1.32613 â”‚
â”‚ time_total_s                                 7.05875 â”‚
â”‚ training_iteration                                 5 â”‚
â”‚ best_valid_loss                              0.22079 â”‚
â”‚ train_loss                                   0.31516 â”‚
â”‚ valid_loss                                   0.22207 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[36m(_train_fn pid=2782127)[0m Early stopping
[36m(_train_fn pid=2782127)[0m EarlyStopping counter: 3 out of 3[32m [repeated 5x across cluster][0m
[36m(_train_fn pid=2782063)[0m Validation loss decreased (0.2248 --> 0.2229).  Saving model state dict ...

Trial trial-59986_00008 completed after 6 iterations at 2024-08-26 13:15:50. Total running time: 1min 24s
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial trial-59986_00008 result                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkpoint_dir_name                checkpoint_000005 â”‚
â”‚ time_this_iter_s                             1.60583 â”‚
â”‚ time_total_s                                  9.7709 â”‚
â”‚ training_iteration                                 6 â”‚
â”‚ best_valid_loss                              0.22293 â”‚
â”‚ train_loss                                   0.20661 â”‚
â”‚ valid_loss                                   0.24217 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Trial status: 10 TERMINATED
Current time: 2024-08-26 13:15:50. Total running time: 1min 24s
Logical resource usage: 1.0/32 CPUs, 0.5/2 GPUs (0.0/1.0 accelerator_type:G)
Current best trial: 59986_00001 with best_valid_loss=0.21595208034374128 and params={'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Trial name          status         batch_size     learning_rate     d_model     alpha_d_ff     d_core     e_layers      dropout   lradj       iter     total time (s)     train_loss     valid_loss     best_valid_loss â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trial-59986_00000   TERMINATED             32       0.0003              128              1         64            2   0            cosine         4            6.30272       0.433672       0.232965            0.219535 â”‚
â”‚ trial-59986_00001   TERMINATED              8       6.40637e-05         256              4         64            2   0.00128816   cosine         4           16.9957        0.343032       0.222783            0.215952 â”‚
â”‚ trial-59986_00002   TERMINATED             16       0.00351285          512              2         64            4   0.00890605   type1          7           30.4342        0.359785       0.270691            0.268922 â”‚
â”‚ trial-59986_00003   TERMINATED              8       0.00015449          128              1        512            4   0.00595831   cosine         4           22.6266        0.245055       0.22707             0.220549 â”‚
â”‚ trial-59986_00004   TERMINATED              8       0.00357919          256              3         64            4   0.00325461   type1          5           32.8611        0.434185       0.271418            0.266773 â”‚
â”‚ trial-59986_00005   TERMINATED             64       0.000107344          64              1        128            1   0.00131792   type1          8            4.55289       0.367946       0.229691            0.229691 â”‚
â”‚ trial-59986_00006   TERMINATED             16       0.00043776           32              1        256            2   0.00435286   type1          6           17.3           0.36244        0.224893            0.22175  â”‚
â”‚ trial-59986_00007   TERMINATED             32       0.00268133          512              2         64            1   0.0057879    type1          4            4.66576       0.207822       0.250534            0.22575  â”‚
â”‚ trial-59986_00008   TERMINATED             32       0.00196362           64              2        512            4   0.0033338    cosine         6            9.7709        0.206613       0.242167            0.222929 â”‚
â”‚ trial-59986_00009   TERMINATED             32       0.000733579          32              4         64            3   0.00128026   type1          5            7.05875       0.315164       0.222069            0.220791 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Best hyperparameters found were:  {'batch_size': 8, 'learning_rate': 6.406370357033657e-05, 'd_model': 256, 'd_core': 64, 'e_layers': 2, 'dropout': 0.0012881646578979025, 'lradj': 'cosine', 'd_ff': 1024}
[36m(_train_fn pid=2782063)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/antonio/Documents/tfm/SOFTS/NEW/checkpoints/hptunning/random_search/ETTh2_96_96_test/trial-59986_00008_8_alpha_d_ff=2,batch_size=32,d_core=512,d_model=64,dropout=0.0033,e_layers=4,learning_rate=0.0020,lradj=cosine_2024-08-26_13-15-38/checkpoint_000005)[32m [repeated 2x across cluster][0m
[36m(_train_fn pid=2782063)[0m Updating learning rate to 0.00028756526858096616[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2782063)[0m saving checkpoint...[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2782063)[0m Epoch: 6 cost time: 1.3962953090667725[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2782063)[0m Epoch: 6, Steps: 265 | Train Loss: 0.2066133 Vali Loss: 0.2421666 Best vali loss: 0.2229292[32m [repeated 7x across cluster][0m
[36m(_train_fn pid=2782063)[0m 	iters: 200, epoch: 6 | loss: 0.2011863[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2782063)[0m 	speed: 0.0051s/iter; left time: 3.0678s[32m [repeated 10x across cluster][0m
[36m(_train_fn pid=2782063)[0m Early stopping
[36m(_train_fn pid=2782063)[0m EarlyStopping counter: 3 out of 3[32m [repeated 2x across cluster][0m


Time taken (2 parallel trials): 92 seconds


